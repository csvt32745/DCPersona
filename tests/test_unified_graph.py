"""
æ¸¬è©¦ agent_core/graph.py çµ±ä¸€ LangGraph å¯¦ä½œ
"""

import pytest
from unittest.mock import patch, MagicMock, Mock
from schemas.agent_types import OverallState, MsgNode, AgentPlan, ToolPlan
from agent_core.graph import UnifiedAgent, create_unified_agent, create_agent_graph
from schemas.config_types import AppConfig, AgentConfig, ToolConfig, AgentBehaviorConfig, LLMConfig, LLMModelConfig, SystemConfig, DiscordConfig


def get_test_config(agent_config=None):
    """å‰µå»ºæ¸¬è©¦ç”¨çš„é…ç½®"""
    default_agent_config = {
        "tools": {
            "google_search": {"enabled": True, "priority": 1},
            # "citation": {"enabled": False, "priority": 2} # Removed citation tool
        },
        "behavior": {
            "max_tool_rounds": 0,
            "enable_reflection": True
        }
    }
    
    if agent_config:
        # æ·±åº¦åˆä½µé…ç½®
        for key, value in agent_config.items():
            if key in default_agent_config and isinstance(value, dict):
                default_agent_config[key].update(value)
            else:
                default_agent_config[key] = value
    
    # è½‰æ›å·¥å…·é…ç½®ç‚º ToolConfig å¯¦ä¾‹
    tools_dict = {}
    for tool_name, tool_config in default_agent_config["tools"].items():
        tools_dict[tool_name] = ToolConfig(**tool_config)
    
    return AppConfig(
        system=SystemConfig(log_level="INFO", timezone="Asia/Taipei"),
        discord=DiscordConfig(bot_token="test_token", enable_conversation_history=True),
        llm=LLMConfig(
            models={
                "tool_analysis": LLMModelConfig(model="gemini-2.0-flash", temperature=0.1),
                "final_answer": LLMModelConfig(model="gemini-2.0-flash", temperature=0.7),
                "reflection": LLMModelConfig(model="gemini-2.0-flash-exp", temperature=0.3)
            }
        ),
        agent=AgentConfig(
            tools=tools_dict,
            behavior=AgentBehaviorConfig(**default_agent_config["behavior"])
        )
    )


class TestUnifiedAgent:
    """æ¸¬è©¦çµ±ä¸€ Agent é¡åˆ¥"""
    
    def test_agent_creation(self):
        """æ¸¬è©¦ Agent å‰µå»º"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            assert agent.config == config
            assert agent.agent_config == config.agent
            assert agent.tools_config == config.agent.tools
            assert agent.behavior_config == config.agent.behavior
    
    def test_agent_creation_with_default_config(self):
        """æ¸¬è©¦ä½¿ç”¨é è¨­é…ç½®å‰µå»º Agent"""
        with patch('agent_core.graph.load_typed_config') as mock_load_config, \
             patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            
            mock_load_config.return_value = get_test_config()
            agent = UnifiedAgent()
            
            assert agent.config is not None
    
    def test_build_graph_pure_conversation_mode(self):
        """æ¸¬è©¦ç´”å°è©±æ¨¡å¼çš„åœ–æ§‹å»º"""
        config = get_test_config({
            "behavior": {"max_tool_rounds": 0}
        })
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            graph = agent.build_graph()
            
            assert graph is not None
            # æª¢æŸ¥æ ¸å¿ƒç¯€é»å­˜åœ¨
            expected_nodes = ["generate_query_or_plan", "reflection", "finalize_answer", "execute_tools"]
            for node in expected_nodes:
                assert node in graph.nodes
            
            # åœ¨ç´”å°è©±æ¨¡å¼ä¸‹ï¼Œå³ä½¿ç¯€é»å­˜åœ¨ï¼ŒåŸ·è¡Œè·¯å¾‘ä¹Ÿä¸æ‡‰ç¶“éå®ƒã€‚
            # é€™éƒ¨åˆ†çš„é‚è¼¯ç”± route_after_planning å‡½æ•¸æ§åˆ¶ï¼Œè€Œä¸æ˜¯ç¯€é»æ˜¯å¦å­˜åœ¨ã€‚
    
    def test_build_graph_tool_mode(self):
        """æ¸¬è©¦å·¥å…·æ¨¡å¼çš„åœ–æ§‹å»º"""
        config = get_test_config({
            "behavior": {"max_tool_rounds": 2}
        })
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            graph = agent.build_graph()
            
            assert graph is not None
    
    @pytest.mark.asyncio
    async def test_generate_query_or_plan_no_messages(self):
        """æ¸¬è©¦æ²’æœ‰è¨Šæ¯çš„æŸ¥è©¢ç”Ÿæˆ"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState(messages=[])
            
            # é€™æ‡‰è©²æœƒå¼•ç™¼éŒ¯èª¤ï¼Œå› ç‚ºæ²’æœ‰è¨Šæ¯
            result = await agent.generate_query_or_plan(state)
            assert "finished" in result or "agent_plan" in result
    
    @pytest.mark.asyncio
    async def test_generate_query_or_plan_with_user_message(self):
        """æ¸¬è©¦æœ‰ç”¨æˆ¶è¨Šæ¯çš„æŸ¥è©¢ç”Ÿæˆ"""
        config = get_test_config({
            "tools": {"google_search": {"enabled": True}},
            "behavior": {"max_tool_rounds": 1}
        })
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState(
                messages=[MsgNode(role="user", content="æœå°‹æœ€æ–°çš„ AI æ–°è")]
            )
            
            result = await agent.generate_query_or_plan(state)
            
            # ç”±æ–¼ LLM æ˜¯ mock å°è±¡ï¼Œå¯èƒ½æœƒå¤±æ•—ä¸¦è¿”å› finished: True
            if "finished" in result:
                assert result["finished"] is True
                assert "agent_plan" in result
            else:
                assert "tool_round" in result
                assert "agent_plan" in result
                assert "research_topic" in result
            
            # æª¢æŸ¥ agent_plan çµæ§‹
            agent_plan = result["agent_plan"]
            assert isinstance(agent_plan, AgentPlan)
    
    def test_analyze_tool_necessity(self):
        """æ¸¬è©¦å·¥å…·éœ€æ±‚åˆ†æ"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            # ç›´æ¥æ¸¬è©¦ fallback æ–¹æ³•ä»¥ç¢ºä¿é—œéµå­—æª¢æ¸¬é‚è¼¯æ­£ç¢º
            # åŒ…å«å·¥å…·é—œéµå­—çš„å…§å®¹
            assert agent._analyze_tool_necessity_fallback([MsgNode(role="user", content="è«‹æœå°‹æœ€æ–°è³‡è¨Š")]) is True
            assert agent._analyze_tool_necessity_fallback([MsgNode(role="user", content="æŸ¥è©¢ä»Šå¤©çš„æ–°è")]) is True
            
            # ä¸åŒ…å«å·¥å…·é—œéµå­—çš„å…§å®¹  
            assert agent._analyze_tool_necessity_fallback([MsgNode(role="user", content="ä½ å¥½å—ï¼Ÿ")]) is False
            assert agent._analyze_tool_necessity_fallback([MsgNode(role="user", content="è¬è¬ä½ çš„å¹«åŠ©")]) is False

    def test_route_and_dispatch_tools(self):
        """æ¸¬è©¦å·¥å…·è·¯ç”±å’Œåˆ†ç™¼é‚è¼¯"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            # æ¸¬è©¦ä¸éœ€è¦å·¥å…·çš„æƒ…æ³
            state_no_tools = OverallState(
                agent_plan=AgentPlan(needs_tools=False)
            )
            route = agent.route_and_dispatch_tools(state_no_tools)
            assert route == "direct_answer"
            
            # æ¸¬è©¦éœ€è¦å·¥å…·çš„æƒ…æ³
            state_with_tools = OverallState(
                agent_plan=AgentPlan(needs_tools=True),
                metadata={"pending_tool_calls": [{"name": "test"}]}
            )
            route = agent.route_and_dispatch_tools(state_with_tools)
            assert route == "execute_tools"

    @pytest.mark.asyncio
    async def test_reflection_enabled(self):
        """æ¸¬è©¦å•Ÿç”¨åæ€çš„æƒ…æ³"""
        config = get_test_config({
            "behavior": {"enable_reflection": True}
        })
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState(
                aggregated_tool_results=["è¶³å¤ é•·çš„æ¸¬è©¦çµæœå…§å®¹ä¾†é€šéå……åˆ†æ€§æª¢æŸ¥"],
                research_topic="æ¸¬è©¦ä¸»é¡Œ"
            )
            
            result = await agent.reflection(state)
        
        assert "is_sufficient" in result
        assert "reflection_complete" in result
        assert result["reflection_complete"] is True
    
    @pytest.mark.asyncio
    async def test_reflection_disabled(self):
        """æ¸¬è©¦ç¦ç”¨åæ€çš„æƒ…æ³"""
        config = get_test_config({
            "behavior": {"enable_reflection": False}
        })
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState()
            result = await agent.reflection(state)
            
            assert result["is_sufficient"] is True

    def test_decide_next_step_sufficient(self):
        """æ¸¬è©¦çµæœå……åˆ†æ™‚çš„æ±ºç­–"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState(
                is_sufficient=True,
                tool_round=1
            )
            
            result = agent.decide_next_step(state)
            assert result == "finish"
    
    def test_decide_next_step_max_rounds_reached(self):
        """æ¸¬è©¦é”åˆ°æœ€å¤§è¼ªæ¬¡æ™‚çš„æ±ºç­–"""
        config = get_test_config({
            "behavior": {"max_tool_rounds": 2}
        })
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState(
                is_sufficient=False,
                tool_round=2
            )
            
            result = agent.decide_next_step(state)
            assert result == "finish"
    
    def test_decide_next_step_continue(self):
        """æ¸¬è©¦ç¹¼çºŒç ”ç©¶çš„æƒ…æ³"""
        config = get_test_config({
            "behavior": {"max_tool_rounds": 3}
        })
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState(
                is_sufficient=False,
                tool_round=1
            )
            
            result = agent.decide_next_step(state)
            assert result == "continue"
    
    @pytest.mark.asyncio
    async def test_finalize_answer_no_messages(self):
        """æ¸¬è©¦æ²’æœ‰è¨Šæ¯çš„æœ€çµ‚ç­”æ¡ˆç”Ÿæˆ"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            state = OverallState(messages=[])
            result = await agent.finalize_answer(state)
            
            assert "final_answer" in result
            assert "finished" in result
    
    @pytest.mark.asyncio
    async def test_finalize_answer_with_context(self):
        """æ¸¬è©¦æœ‰ä¸Šä¸‹æ–‡çš„æœ€çµ‚ç­”æ¡ˆç”Ÿæˆ"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI') as mock_llm_class, \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            # è¨­ç½®æ¨¡æ“¬çš„ LLM å›æ‡‰
            mock_llm_instance = MagicMock()
            
            # æ¨¡æ“¬ astream æ–¹æ³•
            async def mock_astream_with_content(messages):
                chunks = ["é—œæ–¼ä½ å•çš„ã€Œä»€éº¼æ˜¯ AIï¼Ÿã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€äº›æœ‰ç”¨çš„è³‡è¨Šå‘¢ï¼âœ¨\n\n",
                          "AI æ˜¯äººå·¥æ™ºæ…§çš„ç¸®å¯«ï¼ŒAI æŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•\n\n",
                          "å¸Œæœ›é€™äº›å°ä½ æœ‰å¹«åŠ©ï¼é‚„æœ‰ä»€éº¼æƒ³äº†è§£çš„å—ï¼ŸğŸ˜Š"]
                for chunk in chunks:
                    yield Mock(content=chunk)
                yield Mock(content="") # çµæŸæ¨™è¨˜
            
            mock_llm_instance.astream = mock_astream_with_content
            mock_llm_class.return_value = mock_llm_instance
            
            agent = UnifiedAgent(config)
            # æ‰‹å‹•è¨­ç½® final_answer_llm
            agent.final_answer_llm = mock_llm_instance
            
            state = OverallState(
                messages=[MsgNode(role="user", content="ä»€éº¼æ˜¯ AIï¼Ÿ")],
                aggregated_tool_results=["AI æ˜¯äººå·¥æ™ºæ…§çš„ç¸®å¯«", "AI æŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•"]
            )
            
            result = await agent.finalize_answer(state)
            
            assert "final_answer" in result
            assert "AI" in result["final_answer"]
            assert "äººå·¥æ™ºæ…§" in result["final_answer"]
            assert "æŠ€è¡“" in result["final_answer"]
    
    @pytest.mark.asyncio
    async def test_finalize_answer_without_context(self):
        """æ¸¬è©¦ç„¡ä¸Šä¸‹æ–‡çš„æœ€çµ‚ç­”æ¡ˆç”Ÿæˆ"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI') as mock_llm_class, \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            # è¨­ç½®æ¨¡æ“¬çš„ LLM å›æ‡‰
            mock_llm_instance = MagicMock()
            
            # æ¨¡æ“¬ astream æ–¹æ³•
            async def mock_astream_without_content(messages):
                chunks = ["å—¨ï¼é—œæ–¼ã€Œä½ å¥½ã€ï¼Œæˆ‘å¾ˆæ¨‚æ„å’Œä½ èŠèŠï½",
                          "é›–ç„¶æˆ‘ç¾åœ¨æ²’æœ‰é¡å¤–çš„æœå°‹è³‡è¨Šï¼Œä½†æˆ‘æœƒç›¡æˆ‘æ‰€çŸ¥ä¾†å›ç­”ä½ ï¼ğŸ˜Š ",
                          "æœ‰ä»€éº¼ç‰¹åˆ¥æƒ³èŠçš„å—ï¼Ÿ"]
                for chunk in chunks:
                    yield Mock(content=chunk)
                yield Mock(content="") # çµæŸæ¨™è¨˜
            
            mock_llm_instance.astream = mock_astream_without_content
            mock_llm_class.return_value = mock_llm_instance
            
            agent = UnifiedAgent(config)
            # æ‰‹å‹•è¨­ç½® final_answer_llm
            agent.final_answer_llm = mock_llm_instance
            
            state = OverallState(
                messages=[MsgNode(role="user", content="ä½ å¥½")],
                aggregated_tool_results=[]
            )
            
            result = await agent.finalize_answer(state)
            
            assert "final_answer" in result
            assert "ä½ å¥½" in result["final_answer"]
            assert "æ¨‚æ„" in result["final_answer"]
            assert "èŠèŠ" in result["final_answer"]
            assert "è³‡è¨Š" in result["final_answer"]
    
    @pytest.mark.asyncio
    async def test_evaluate_results_sufficiency(self):
        """æ¸¬è©¦çµæœå……åˆ†æ€§è©•ä¼°"""
        config = get_test_config()
        
        with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
             patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
            agent = UnifiedAgent(config)
            
            # æ¸¬è©¦å……åˆ†çš„çµæœ
            sufficient_results = ["é€™æ˜¯ä¸€å€‹è¶³å¤ é•·çš„çµæœä¾†é€šéå……åˆ†æ€§æª¢æŸ¥"]
            assert agent._evaluate_results_sufficiency(sufficient_results, "æ¸¬è©¦ä¸»é¡Œ") is True
            
            # æ¸¬è©¦ä¸å……åˆ†çš„çµæœ
            insufficient_results = ["çŸ­"]
            assert agent._evaluate_results_sufficiency(insufficient_results, "æ¸¬è©¦ä¸»é¡Œ") is True  # ç¾åœ¨ç¸½æ˜¯è¿”å› True
            
            # æ¸¬è©¦ç©ºçµæœ
            empty_results = []
            assert agent._evaluate_results_sufficiency(empty_results, "æ¸¬è©¦ä¸»é¡Œ") is True


def test_create_unified_agent():
    """æ¸¬è©¦å‰µå»ºçµ±ä¸€ Agent å‡½æ•¸"""
    config = get_test_config()
    
    with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
         patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
        agent = create_unified_agent(config)
        
        assert isinstance(agent, UnifiedAgent)
        assert agent.config == config


def test_create_agent_graph():
    """æ¸¬è©¦å‰µå»º Agent åœ–å‡½æ•¸"""
    config = get_test_config()
    
    with patch('agent_core.graph.ChatGoogleGenerativeAI'), \
         patch.dict('os.environ', {'GEMINI_API_KEY': 'test_key'}):
        graph = create_agent_graph(config)
        
        assert graph is not None


if __name__ == "__main__":
    pytest.main([__file__, "-v"]) 