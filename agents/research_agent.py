"""
LangGraph ç ”ç©¶ä»£ç†

åŸºæ–¼ gemini-fullstack-langgraph-quickstart çš„åœ–çµæ§‹ï¼Œ
é©é…åˆ° Discord ç’°å¢ƒä¸¦æ·»åŠ å³æ™‚é€²åº¦å›é¥‹å’ŒéŒ¯èª¤è™•ç†ã€‚
"""

import os
import asyncio
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime

import discord
from langchain_core.messages import AIMessage, HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.types import Send
from langgraph.graph import StateGraph, START, END
from langgraph.graph import add_messages
from langchain_core.runnables import RunnableConfig
from google.genai import Client

from .state import (
    OverallState, 
    ReflectionState, 
    QueryGenerationState, 
    WebSearchState,
    DiscordContext,
    ResearchProgress,
    create_initial_state
)
from .configuration import AgentConfiguration
from .prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
    get_progress_message
)
from .tools_and_schemas import (
    SearchQueryList, 
    Reflection,
    DiscordTools,
    ErrorHandler,
    DataValidator
)
from .utils import (
    get_citations,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
    clean_and_truncate_text,
    format_time_elapsed
)


class ResearchAgent:
    """LangGraph ç ”ç©¶ä»£ç†ä¸»é¡åˆ¥"""
    
    def __init__(self, config: AgentConfiguration):
        """
        åˆå§‹åŒ–ç ”ç©¶ä»£ç†
        
        Args:
            config: Agent é…ç½®
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # é©—è­‰å¿…è¦çš„ API é‡‘é‘°
        is_valid, message = config.validate_api_keys()
        if not is_valid:
            self.logger.warning(f"API é‡‘é‘°é©—è­‰å¤±æ•—: {message}")
        
        # åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯
        if config.gemini_api_key:
            os.environ["GEMINI_API_KEY"] = config.gemini_api_key
            self.genai_client = Client(api_key=config.gemini_api_key)
        else:
            self.genai_client = None
            self.logger.warning("Gemini API é‡‘é‘°æœªè¨­ç½®ï¼Œç„¡æ³•ä½¿ç”¨ç¶²è·¯æœå°‹åŠŸèƒ½")
        
        # å»ºç«‹ LangGraph
        self.graph = self._build_graph()
        
        # é€²åº¦è¿½è¹¤
        self.current_progress = {}
    
    def _build_graph(self) -> StateGraph:
        """å»ºç«‹ LangGraph åœ–çµæ§‹"""
        builder = StateGraph(OverallState, config_schema=AgentConfiguration)
        
        # å®šç¾©ç¯€é»
        builder.add_node("generate_query", self._generate_query)
        builder.add_node("web_research", self._web_research)
        builder.add_node("reflection", self._reflection)
        builder.add_node("finalize_answer", self._finalize_answer)
        
        # è¨­ç½®å…¥å£é»
        builder.add_edge(START, "generate_query")
        
        # æ·»åŠ æ¢ä»¶é‚Šç·£
        builder.add_conditional_edges(
            "generate_query", 
            self._continue_to_web_research, 
            ["web_research"]
        )
        
        builder.add_edge("web_research", "reflection")
        
        builder.add_conditional_edges(
            "reflection", 
            self._evaluate_research, 
            ["web_research", "finalize_answer"]
        )
        
        builder.add_edge("finalize_answer", END)
        
        return builder.compile(name="discord-research-agent")
    
    async def process_research_request(
        self,
        discord_ctx: DiscordContext,
        user_message: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        è™•ç†ç ”ç©¶è«‹æ±‚
        
        Args:
            discord_ctx: Discord ä¸Šä¸‹æ–‡
            user_message: ä½¿ç”¨è€…è¨Šæ¯
            progress_callback: é€²åº¦å›èª¿å‡½å¼
            
        Returns:
            Dict: ç ”ç©¶çµæœ
        """
        start_time = datetime.now()
        session_id = discord_ctx.session_id
        
        try:
            # é©—è­‰æŸ¥è©¢
            is_valid, error_msg = DataValidator.validate_search_query(user_message)
            if not is_valid:
                return self._create_error_result("invalid_query", error_msg)
            
            # å‰µå»ºåˆå§‹ç‹€æ…‹
            initial_state = create_initial_state(discord_ctx, user_message, self.config)
            
            # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
            progress = ResearchProgress(stage="generate_query")
            self.current_progress[session_id] = progress
            
            # ç™¼é€åˆå§‹é€²åº¦è¨Šæ¯
            if progress_callback:
                await progress_callback(progress)
            
            # å‰µå»ºé…ç½®
            config = RunnableConfig(
                configurable=self.config.model_dump(exclude_none=True)
            )
            
            # åŸ·è¡Œç ”ç©¶æµç¨‹ï¼ˆå¸¶è¶…æ™‚ï¼‰
            result = await asyncio.wait_for(
                self._run_research_graph(initial_state, config, session_id, progress_callback),
                timeout=self.config.timeout_seconds
            )
            
            # è¨ˆç®—åŸ·è¡Œæ™‚é–“
            execution_time = format_time_elapsed(start_time)
            
            # å®Œæˆé€²åº¦
            progress.stage = "completed"
            self.current_progress[session_id] = progress
            if progress_callback:
                await progress_callback(progress)
            
            return {
                "success": True,
                "result": result,
                "execution_time": execution_time,
                "session_id": session_id,
                "sources_count": len(result.get("sources_gathered", [])),
            }
            
        except asyncio.TimeoutError:
            self.logger.warning(f"ç ”ç©¶è«‹æ±‚è¶…æ™‚: {session_id}")
            progress.stage = "timeout"
            if progress_callback:
                await progress_callback(progress)
            
            return self._create_error_result("timeout", "ç ”ç©¶è«‹æ±‚è¶…æ™‚")
            
        except Exception as e:
            self.logger.error(f"ç ”ç©¶éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            progress.stage = "error"
            if progress_callback:
                await progress_callback(progress)
            
            return self._create_error_result("unknown", str(e))
        
        finally:
            # æ¸…ç†é€²åº¦è¿½è¹¤
            self.current_progress.pop(session_id, None)
            
            # æ¸…ç†é »é“çš„é€²åº¦æ¶ˆæ¯è¨˜éŒ„ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            if discord_ctx and hasattr(discord_ctx, 'channel_id'):
                DiscordTools.cleanup_progress_messages(discord_ctx.channel_id)
    
    async def _run_research_graph(
        self,
        initial_state: OverallState,
        config: RunnableConfig,
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """åŸ·è¡Œç ”ç©¶åœ–æµç¨‹"""
        # é€™è£¡éœ€è¦å¯¦éš›åŸ·è¡Œ LangGraph
        # ç”±æ–¼ LangGraph çš„ invoke æ–¹æ³•é€šå¸¸æ˜¯åŒæ­¥çš„ï¼Œæˆ‘å€‘éœ€è¦åœ¨åŸ·è¡Œå™¨ä¸­é‹è¡Œå®ƒ
        loop = asyncio.get_event_loop()
        
        def run_sync():
            return self.graph.invoke(initial_state, config)
        
        # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡ŒåŒæ­¥çš„åœ–èª¿ç”¨
        result = await loop.run_in_executor(None, run_sync)
        return result
    
    def _generate_query(self, state: OverallState, config: RunnableConfig) -> QueryGenerationState:
        """LangGraph ç¯€é»ï¼šç”Ÿæˆæœå°‹æŸ¥è©¢"""
        try:
            configurable = AgentConfiguration.from_runnable_config(config)
            
            # æª¢æŸ¥è‡ªå®šç¾©åˆå§‹æœå°‹æŸ¥è©¢æ•¸é‡
            if state.get("initial_search_query_count") is None:
                state["initial_search_query_count"] = configurable.number_of_initial_queries
            
            # åˆå§‹åŒ– Gemini æ¨¡å‹
            llm = ChatGoogleGenerativeAI(
                model=configurable.query_generator_model,
                temperature=1.0,
                max_retries=2,
                api_key=self.config.gemini_api_key,
            )
            structured_llm = llm.with_structured_output(SearchQueryList)
            
            # æ ¼å¼åŒ–æç¤º
            current_date = get_current_date()
            formatted_prompt = query_writer_instructions.format(
                current_date=current_date,
                research_topic=get_research_topic(state["messages"]),
                number_queries=state["initial_search_query_count"],
            )
            
            # ç”Ÿæˆæœå°‹æŸ¥è©¢
            result = structured_llm.invoke(formatted_prompt)
            
            # æ›´æ–°é€²åº¦
            session_id = state.get("session_id")
            if session_id and session_id in self.current_progress:
                progress = self.current_progress[session_id]
                progress.stage = "web_research"
                progress.total_queries = len(result.query)
            
            return {"query_list": [{"query": q, "rationale": result.rationale} for q in result.query]}
            
        except Exception as e:
            self.logger.error(f"æŸ¥è©¢ç”Ÿæˆå¤±æ•—: {str(e)}")
            raise
    
    def _continue_to_web_research(self, state: QueryGenerationState):
        """LangGraph ç¯€é»ï¼šå°‡æœå°‹æŸ¥è©¢ç™¼é€åˆ°ç¶²è·¯ç ”ç©¶ç¯€é»"""
        return [
            Send("web_research", {"search_query": query_info["query"], "id": int(idx)})
            for idx, query_info in enumerate(state["query_list"])
        ]
    
    def _web_research(self, state: WebSearchState, config: RunnableConfig) -> OverallState:
        """LangGraph ç¯€é»ï¼šåŸ·è¡Œç¶²è·¯ç ”ç©¶"""
        try:
            if not self.genai_client:
                raise ValueError("Gemini å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
            
            configurable = AgentConfiguration.from_runnable_config(config)
            formatted_prompt = web_searcher_instructions.format(
                current_date=get_current_date(),
                research_topic=state["search_query"],
            )
            
            # ä½¿ç”¨ Google genai å®¢æˆ¶ç«¯ï¼ˆå› ç‚º langchain å®¢æˆ¶ç«¯ä¸è¿”å› grounding metadataï¼‰
            response = self.genai_client.models.generate_content(
                model=configurable.query_generator_model,
                contents=formatted_prompt,
                config={
                    "tools": [{"google_search": {}}],
                    "temperature": 0,
                },
            )
            
            # è§£æ URL ç‚ºçŸ­ URL ä»¥ç¯€çœ token å’Œæ™‚é–“
            resolved_urls = resolve_urls(
                response.candidates[0].grounding_metadata.grounding_chunks, state["id"]
            )
            
            # ç²å–å¼•ç”¨ä¸¦æ·»åŠ åˆ°ç”Ÿæˆçš„æ–‡å­—ä¸­
            citations = get_citations(response, resolved_urls)
            modified_text = insert_citation_markers(response.text, citations)
            sources_gathered = [item for citation in citations for item in citation["segments"]]
            
            # æ›´æ–°é€²åº¦
            session_id = state.get("session_id")
            if session_id and session_id in self.current_progress:
                progress = self.current_progress[session_id]
                progress.completed_queries += 1
                progress.sources_found += len(sources_gathered)
            
            return {
                "sources_gathered": sources_gathered,
                "search_query": [state["search_query"]],
                "web_research_result": [modified_text],
            }
            
        except Exception as e:
            self.logger.error(f"ç¶²è·¯ç ”ç©¶å¤±æ•—: {str(e)}")
            # è¿”å›ç©ºçµæœè€Œä¸æ˜¯æ‹‹å‡ºç•°å¸¸ï¼Œè®“æµç¨‹ç¹¼çºŒ
            return {
                "sources_gathered": [],
                "search_query": [state["search_query"]],
                "web_research_result": [f"æœå°‹ '{state['search_query']}' æ™‚ç™¼ç”ŸéŒ¯èª¤"],
            }
    
    def _reflection(self, state: OverallState, config: RunnableConfig) -> ReflectionState:
        """LangGraph ç¯€é»ï¼šåæ€å’Œè©•ä¼°"""
        try:
            configurable = AgentConfiguration.from_runnable_config(config)
            
            # å¢åŠ ç ”ç©¶å¾ªç’°è¨ˆæ•¸ä¸¦ç²å–æ¨ç†æ¨¡å‹
            state["research_loop_count"] = state.get("research_loop_count", 0) + 1
            reasoning_model = state.get("reasoning_model") or configurable.reflection_model
            
            # æ ¼å¼åŒ–æç¤º
            current_date = get_current_date()
            formatted_prompt = reflection_instructions.format(
                current_date=current_date,
                research_topic=get_research_topic(state["messages"]),
                summaries="\n\n---\n\n".join(state["web_research_result"]),
            )
            
            # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
            llm = ChatGoogleGenerativeAI(
                model=reasoning_model,
                temperature=1.0,
                max_retries=2,
                api_key=self.config.gemini_api_key,
            )
            result = llm.with_structured_output(Reflection).invoke(formatted_prompt)
            
            # æ›´æ–°é€²åº¦
            session_id = state.get("session_id")
            if session_id and session_id in self.current_progress:
                progress = self.current_progress[session_id]
                progress.stage = "reflection" if not result.is_sufficient else "finalize_answer"
                progress.loop_count = state["research_loop_count"]
            
            return {
                "is_sufficient": result.is_sufficient,
                "knowledge_gap": result.knowledge_gap,
                "follow_up_queries": result.follow_up_queries,
                "research_loop_count": state["research_loop_count"],
                "number_of_ran_queries": len(state["search_query"]),
            }
            
        except Exception as e:
            self.logger.error(f"åæ€éç¨‹å¤±æ•—: {str(e)}")
            # å¦‚æœåæ€å¤±æ•—ï¼Œå‡è¨­è³‡è¨Šå……åˆ†ä»¥ç¹¼çºŒæµç¨‹
            return {
                "is_sufficient": True,
                "knowledge_gap": "",
                "follow_up_queries": [],
                "research_loop_count": state.get("research_loop_count", 0) + 1,
                "number_of_ran_queries": len(state.get("search_query", [])),
            }
    
    def _evaluate_research(self, state: ReflectionState, config: RunnableConfig):
        """LangGraph è·¯ç”±å‡½å¼ï¼šæ±ºå®šä¸‹ä¸€æ­¥"""
        configurable = AgentConfiguration.from_runnable_config(config)
        max_research_loops = (
            state.get("max_research_loops")
            if state.get("max_research_loops") is not None
            else configurable.max_research_loops
        )
        
        if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
            return "finalize_answer"
        else:
            return [
                Send(
                    "web_research",
                    {
                        "search_query": follow_up_query,
                        "id": state["number_of_ran_queries"] + int(idx),
                    },
                )
                for idx, follow_up_query in enumerate(state["follow_up_queries"])
            ]
    
    def _finalize_answer(self, state: OverallState, config: RunnableConfig):
        """LangGraph ç¯€é»ï¼šæœ€çµ‚åŒ–ç­”æ¡ˆ"""
        try:
            configurable = AgentConfiguration.from_runnable_config(config)
            reasoning_model = state.get("reasoning_model") or configurable.answer_model
            
            # æ ¼å¼åŒ–æç¤º
            current_date = get_current_date()
            formatted_prompt = answer_instructions.format(
                current_date=current_date,
                research_topic=get_research_topic(state["messages"]),
                summaries="\n---\n\n".join(state["web_research_result"]),
            )
            
            # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
            llm = ChatGoogleGenerativeAI(
                model=reasoning_model,
                temperature=0,
                max_retries=2,
                api_key=self.config.gemini_api_key,
            )
            result = llm.invoke(formatted_prompt)
            
            # æ›¿æ›çŸ­ URL ç‚ºåŸå§‹ URL ä¸¦æ·»åŠ æ‰€æœ‰ä½¿ç”¨çš„ URL åˆ° sources_gathered
            unique_sources = []
            for source in state["sources_gathered"]:
                if source["short_url"] in result.content:
                    result.content = result.content.replace(
                        source["short_url"], source["value"]
                    )
                    unique_sources.append(source)
            
            # æ¸…ç†å’Œæˆªæ–·æ–‡å­—ä»¥é©æ‡‰ Discord
            final_content = clean_and_truncate_text(result.content)
            
            # æ·»åŠ ä¾†æºè³‡è¨Šåˆ°æœ€çµ‚ç­”æ¡ˆ
            if unique_sources:
                sources_text = "\n\n**ğŸ“š åƒè€ƒä¾†æºï¼š**\n"
                for i, source in enumerate(unique_sources[:3], 1):
                    source_title = source.get('label', source.get('title', 'ä¾†æº'))
                    source_url = source.get('value', source.get('url', '#'))
                    sources_text += f"{i}. [{source_title}]({source_url})\n"
                
                # ç¢ºä¿æœ€çµ‚å…§å®¹ä¸æœƒè¶…é Discord é™åˆ¶
                if len(final_content + sources_text) <= 1900:
                    final_content += sources_text
            
            # æ›´æ–°é€²åº¦ - å°‡æœ€çµ‚ç­”æ¡ˆæ¨™è¨˜ç‚ºå®Œæˆ
            session_id = state.get("session_id")
            if session_id and session_id in self.current_progress:
                progress = self.current_progress[session_id]
                progress.stage = "completed"
                progress.final_answer = final_content  # ä¿å­˜æœ€çµ‚ç­”æ¡ˆåˆ°é€²åº¦å°è±¡
            
            return {
                "messages": [AIMessage(content=final_content)],
                "sources_gathered": unique_sources,
                "final_answer": final_content,  # æ·»åŠ æœ€çµ‚ç­”æ¡ˆåˆ°è¿”å›çµæœ
            }
            
        except Exception as e:
            self.logger.error(f"æœ€çµ‚åŒ–ç­”æ¡ˆå¤±æ•—: {str(e)}")
            # æä¾›é™ç´šå›æ‡‰
            fallback_response = "æŠ±æ­‰ï¼Œåœ¨æ•´ç†æœ€çµ‚ç­”æ¡ˆæ™‚é‡åˆ°äº†å•é¡Œ ğŸ˜… ä¸éæ ¹æ“šæˆ‘çš„ç ”ç©¶ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€äº›ç›¸é—œè³‡è¨Š..."
            
            # æ›´æ–°é€²åº¦ç‚ºéŒ¯èª¤ç‹€æ…‹
            session_id = state.get("session_id")
            if session_id and session_id in self.current_progress:
                progress = self.current_progress[session_id]
                progress.stage = "error"
                progress.final_answer = fallback_response
            
            return {
                "messages": [AIMessage(content=fallback_response)],
                "sources_gathered": state.get("sources_gathered", []),
                "final_answer": fallback_response,
            }
    
    def _create_error_result(self, error_type: str, message: str) -> Dict[str, Any]:
        """å‰µå»ºéŒ¯èª¤çµæœ"""
        error = ErrorHandler.create_user_friendly_error(error_type, message)
        
        return {
            "success": False,
            "error": error,
            "fallback_available": error.fallback_available,
        }
    
    def get_progress(self, session_id: str) -> Optional[ResearchProgress]:
        """ç²å–æœƒè©±çš„é€²åº¦"""
        return self.current_progress.get(session_id)
    
    async def create_progress_callback(self, message: discord.Message):
        """å‰µå»ºé€²åº¦å›èª¿å‡½å¼"""
        channel_id = message.channel.id
        
        # è¨­ç½®ç•¶å‰è™•ç†çš„åŸå§‹æ¶ˆæ¯ID
        DiscordTools.set_current_original_message_id(message.id)
        
        async def progress_callback(progress: ResearchProgress):
            try:
                # ç¢ºä¿æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½å­˜åœ¨ä¸”å‹åˆ¥æ­£ç¢º
                stage = progress.stage if progress.stage else "processing"
                progress_message = progress.get_progress_message()
                
                # é©—è­‰é€²åº¦è¨Šæ¯ä¸ç‚ºç©º
                if not progress_message:
                    progress_message = f"ğŸ”„ è™•ç†ä¸­... ({stage})"
                
                # å»ºæ§‹é€²åº¦æ›´æ–°è³‡æ–™
                progress_update = {
                    "stage": stage,
                    "message": progress_message,
                    "progress_percentage": None,
                    "eta_seconds": None,
                }
                
                # è¨ˆç®—é€²åº¦ç™¾åˆ†æ¯”
                if progress.total_queries > 0 and progress.completed_queries <= progress.total_queries:
                    progress_percentage = int(
                        (progress.completed_queries / progress.total_queries) * 100
                    )
                    # ç¢ºä¿ç™¾åˆ†æ¯”åœ¨æœ‰æ•ˆç¯„åœå…§
                    progress_update["progress_percentage"] = max(0, min(100, progress_percentage))
                
                # ä¼°è¨ˆå‰©é¤˜æ™‚é–“ï¼ˆç°¡å–®å¯¦ç¾ï¼‰
                if (progress.total_queries > 0 and
                    progress.completed_queries > 0 and
                    progress.completed_queries < progress.total_queries):
                    
                    # å‡è¨­æ¯å€‹æŸ¥è©¢å¹³å‡éœ€è¦ 3-5 ç§’
                    remaining_queries = progress.total_queries - progress.completed_queries
                    estimated_seconds = remaining_queries * 4  # å¹³å‡ 4 ç§’æ¯æŸ¥è©¢
                    progress_update["eta_seconds"] = estimated_seconds
                
                # æ·»åŠ èª¿è©¦æ—¥èªŒ
                self.logger.debug(f"å‰µå»º DiscordProgressUpdate: {progress_update}")
                
                from .tools_and_schemas import DiscordProgressUpdate
                discord_progress = DiscordProgressUpdate(**progress_update)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æœ€çµ‚ç­”æ¡ˆéœ€è¦æ•´åˆ
                final_answer = getattr(progress, 'final_answer', None)
                
                # ç™¼é€æˆ–æ›´æ–°é€²åº¦æ¶ˆæ¯
                if stage == "completed" and final_answer:
                    # å¦‚æœæ˜¯å®Œæˆç‹€æ…‹ä¸”æœ‰æœ€çµ‚ç­”æ¡ˆï¼Œä½¿ç”¨æ•´åˆåŠŸèƒ½
                    progress_msg = await DiscordTools.send_progress_update(
                        message, discord_progress, edit_previous=True, final_answer=final_answer
                    )
                else:
                    # æ­£å¸¸çš„é€²åº¦æ›´æ–°
                    progress_msg = await DiscordTools.send_progress_update(
                        message, discord_progress, edit_previous=True
                    )
                
                # å¦‚æœç ”ç©¶å®Œæˆæˆ–å‡ºéŒ¯ï¼Œå®‰æ’æ¸…ç†ä»»å‹™
                if stage in ["completed", "error", "timeout"]:
                    # å»¶é²æ¸…ç†ï¼Œçµ¦ç”¨æˆ¶æ™‚é–“çœ‹åˆ°æœ€çµ‚ç‹€æ…‹
                    asyncio.create_task(self._delayed_cleanup(channel_id, delay=30))  # å¢åŠ å»¶é²æ™‚é–“
                
            except Exception as e:
                self.logger.error(f"ç™¼é€é€²åº¦æ›´æ–°å¤±æ•—: {str(e)}", exc_info=True)
                # å˜—è©¦ç™¼é€ç°¡åŒ–çš„é€²åº¦è¨Šæ¯
                try:
                    simple_message = f"ğŸ”„ {stage}" if stage != "processing" else "ğŸ”„ è™•ç†ä¸­..."
                    await message.channel.send(simple_message)
                except Exception as fallback_error:
                    self.logger.error(f"é™ç´šé€²åº¦è¨Šæ¯ä¹Ÿå¤±æ•—: {str(fallback_error)}")
        
        return progress_callback
    
    async def _delayed_cleanup(self, channel_id: int, delay: int = 10):
        """å»¶é²æ¸…ç†é€²åº¦æ¶ˆæ¯è¨˜éŒ„"""
        try:
            await asyncio.sleep(delay)
            DiscordTools.cleanup_progress_messages(channel_id)
            self.logger.debug(f"å·²æ¸…ç†é »é“ {channel_id} çš„é€²åº¦æ¶ˆæ¯è¨˜éŒ„")
        except Exception as e:
            self.logger.error(f"æ¸…ç†é€²åº¦æ¶ˆæ¯è¨˜éŒ„å¤±æ•—: {str(e)}")


# ä¾¿åˆ©å‡½å¼
async def create_research_agent_from_config(llmcord_config: Dict[str, Any]) -> ResearchAgent:
    """å¾ llmcord é…ç½®å‰µå»ºç ”ç©¶ä»£ç†"""
    agent_config = AgentConfiguration.from_llmcord_config(llmcord_config)
    return ResearchAgent(agent_config)


def should_use_research_mode(message_content: str, config: AgentConfiguration) -> bool:
    """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ä½¿ç”¨ç ”ç©¶æ¨¡å¼"""
    return config.should_use_research_mode(message_content)