"""
çµ±ä¸€çš„ LangGraph åœ–å¯¦ä½œ

æ ¹æ“šé…ç½®å‹•æ…‹æ§‹å»º LangGraph çš„ StateGraphï¼Œå®šç¾©æ‰€æœ‰ç¯€é»žçš„å¯¦ç¾ï¼Œ
åŒ…æ‹¬å·¥å…·é‚è¼¯çš„ç›´æŽ¥å…§åµŒã€‚åš´æ ¼åƒè€ƒ reference_arch.md ä¸­å®šç¾©çš„ Agent æ ¸å¿ƒæµç¨‹ã€‚
"""

import asyncio
import logging
import json
from typing import Dict, Any, List, Optional, Literal
from datetime import datetime

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langchain_core.runnables import RunnableConfig
from google.genai import Client

from schemas.agent_types import OverallState, MsgNode
from utils.config_loader import load_typed_config
from prompt_system.prompts import web_searcher_instructions, get_current_date
from .agent_utils import resolve_urls, get_citations, insert_citation_markers
from .progress_mixin import ProgressMixin
from schemas.config_types import AppConfig


class UnifiedAgent(ProgressMixin):
    """çµ±ä¸€çš„ Agent å¯¦ä½œï¼Œæ ¹æ“šé…ç½®å‹•æ…‹èª¿æ•´è¡Œç‚º"""
    
    def __init__(self, config: Optional[AppConfig] = None):
        """
        åˆå§‹åŒ–çµ±ä¸€ Agent
        
        Args:
            config: åž‹åˆ¥å®‰å…¨çš„é…ç½®å¯¦ä¾‹ï¼Œå¦‚æžœç‚º None å‰‡è¼‰å…¥é è¨­é…ç½®
        """
        # é¦–å…ˆåˆå§‹åŒ– ProgressMixin
        super().__init__()
        
        self.config = config or load_typed_config()
        self.logger = logging.getLogger(__name__)
        
        # å¾žé…ç½®ç²å– agent è¨­å®š - ä½¿ç”¨åž‹åˆ¥å®‰å…¨å­˜å–
        self.agent_config = self.config.agent
        self.tools_config = self.config.agent.tools
        self.behavior_config = self.config.agent.behavior
        
        # åˆå§‹åŒ–å¤šå€‹ LLM å¯¦ä¾‹
        self.llm_instances = self._initialize_llm_instances()
        
        # åˆå§‹åŒ– Google å®¢æˆ¶ç«¯ï¼ˆå¦‚æžœå·¥å…·å•Ÿç”¨ï¼‰
        self.google_client = None
        if self.config.is_tool_enabled("google_search"):
            api_key = self.config.gemini_api_key
            if api_key:
                self.google_client = Client(api_key=api_key)
    
    def _initialize_llm_instances(self) -> Dict[str, Optional[ChatGoogleGenerativeAI]]:
        """åˆå§‹åŒ–ä¸åŒç”¨é€”çš„ LLM å¯¦ä¾‹"""
        api_key = self.config.gemini_api_key
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is required")
        
        # ä½¿ç”¨åž‹åˆ¥å®‰å…¨çš„é…ç½®å­˜å–
        llm_configs_source = self.config.llm.models
        
        llm_instances = {}
        for purpose, llm_config in llm_configs_source.items():
            try:
                # llm_config ç¾åœ¨ç¸½æ˜¯ LLMModelConfig å¯¦ä¾‹
                model_name = llm_config.model
                temperature = llm_config.temperature
                
                llm_instances[purpose] = ChatGoogleGenerativeAI(
                    model=model_name,
                    temperature=temperature,
                    api_key=api_key
                )
                self.logger.info(f"åˆå§‹åŒ– {purpose} LLM: {model_name}")
            except Exception as e:
                self.logger.warning(f"åˆå§‹åŒ– {purpose} LLM å¤±æ•—: {e}")
                llm_instances[purpose] = None
        
        self.tool_analysis_llm: ChatGoogleGenerativeAI = llm_instances.get("tool_analysis")
        self.final_answer_llm: ChatGoogleGenerativeAI = llm_instances.get("final_answer")
        
        return llm_instances
    
    def build_graph(self) -> StateGraph:
        """å»ºç«‹ä¸¦ç·¨è­¯ LangGraph"""
        builder = StateGraph(OverallState)
        
        # æ·»åŠ æ‰€æœ‰ç¯€é»ž
        builder.add_node("generate_query_or_plan", self.generate_query_or_plan)
        builder.add_node("tool_selection", self.tool_selection)
        builder.add_node("execute_tool", self.execute_tool)
        builder.add_node("reflection", self.reflection)
        builder.add_node("evaluate_research", self.evaluate_research)
        builder.add_node("finalize_answer", self.finalize_answer)
        
        # è¨­ç½®æµç¨‹é‚Šç·£
        builder.add_edge(START, "generate_query_or_plan")
        
        # æ ¹æ“šé…ç½®æ±ºå®šæµç¨‹ - ä½¿ç”¨åž‹åˆ¥å®‰å…¨å­˜å–
        max_tool_rounds = self.behavior_config.max_tool_rounds
        
        if max_tool_rounds == 0:
            # ç´”å°è©±æ¨¡å¼ï¼šç›´æŽ¥åˆ°æœ€çµ‚ç­”æ¡ˆ
            builder.add_edge("generate_query_or_plan", "finalize_answer")
        else:
            # å·¥å…·æ¨¡å¼ï¼šå®Œæ•´æµç¨‹
            builder.add_edge("generate_query_or_plan", "tool_selection")
            builder.add_edge("tool_selection", "execute_tool")
            builder.add_edge("execute_tool", "reflection")
            
            builder.add_conditional_edges(
                "reflection",
                self.decide_next_step,
                {
                    "continue": "tool_selection",
                    "finish": "finalize_answer"
                }
            )
        
        builder.add_edge("finalize_answer", END)
        
        return builder.compile()
    
    async def generate_query_or_plan(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šç”ŸæˆæŸ¥è©¢æˆ–åˆæ­¥è¦åŠƒ
        
        åˆ†æžç”¨æˆ¶è«‹æ±‚ä¸¦æ±ºå®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œç”Ÿæˆåˆæ­¥çš„è¡Œå‹•è¨ˆåŠƒã€‚
        """
        try:
            self.logger.info("generate_query_or_plan: é–‹å§‹åˆ†æžç”¨æˆ¶è«‹æ±‚")
            
            # é€šçŸ¥é–‹å§‹éšŽæ®µ
            await self._notify_progress(
                stage="generate_query", 
                message="ðŸ¤” æ­£åœ¨åˆ†æžæ‚¨çš„å•é¡Œ..."
            )
            
            user_content = state.messages[-1].content
            max_tool_rounds = self.behavior_config.max_tool_rounds
            
            # æ±ºå®šæ˜¯å¦éœ€è¦å·¥å…·
            needs_tools = False
            if max_tool_rounds > 0:
                needs_tools = self._analyze_tool_necessity(state.messages)
            
            # ç”ŸæˆæŸ¥è©¢æˆ–è¨ˆåŠƒ
            queries = []
            if needs_tools and self.google_client:
                queries = self._generate_search_queries(state.messages)
            
            self.logger.info(f"generate_query_or_plan: éœ€è¦å·¥å…·={needs_tools}, æŸ¥è©¢æ•¸é‡={len(queries)}")
            
            return {
                "tool_round": 0,
                "needs_tools": needs_tools,
                "search_queries": queries,
                "research_topic": user_content[:200],  # æˆªå–å‰200å­—ç¬¦ä½œç‚ºç ”ç©¶ä¸»é¡Œ
            }
            
        except Exception as e:
            self.logger.error(f"generate_query_or_plan å¤±æ•—: {e}")
            await self._notify_progress(
                stage="error",
                message="âŒ åˆ†æžå•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤",
                progress_percentage=0
            )
            return {"finished": True}
    
    def tool_selection(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šå·¥å…·é¸æ“‡
        
        æ ¹æ“šç•¶å‰ç‹€æ…‹å’Œé…ç½®æ±ºå®šä½¿ç”¨å“ªäº›å·¥å…·ã€‚
        """
        try:
            self.logger.info("tool_selection: é¸æ“‡é©ç•¶çš„å·¥å…·")
            
            available_tools = []
            
            # æ ¹æ“šå„ªå…ˆç´šæŽ’åºå·¥å…· - ä½¿ç”¨åž‹åˆ¥å®‰å…¨å­˜å–
            tools_by_priority = sorted(
                self.tools_config.items(),
                key=lambda x: x[1].priority
            )
            
            for tool_name, tool_config in tools_by_priority:
                if tool_config.enabled:
                    # æª¢æŸ¥å·¥å…·æ˜¯å¦çœŸçš„å¯ç”¨
                    if tool_name == "google_search" and self.google_client:
                        available_tools.append(tool_name)
                    elif tool_name != "google_search":
                        available_tools.append(tool_name)
            
            # æ±ºå®šä½¿ç”¨çš„å·¥å…·
            selected_tool = None
            if state.needs_tools and available_tools:
                # ç°¡å–®ç­–ç•¥ï¼šé¸æ“‡ç¬¬ä¸€å€‹å¯ç”¨çš„å·¥å…·
                selected_tool = available_tools[0]
            
            self.logger.info(f"tool_selection: å¯ç”¨å·¥å…·={available_tools}, é¸æ“‡å·¥å…·={selected_tool}")
            
            return {
                "available_tools": available_tools,
                "selected_tool": selected_tool
            }
            
        except Exception as e:
            self.logger.error(f"tool_selection å¤±æ•—: {e}")
            return {"selected_tool": None}
    
    async def execute_tool(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šåŸ·è¡Œå·¥å…·
        
        åŸ·è¡Œé¸å®šçš„å·¥å…·ï¼Œå…§åµŒå·¥å…·é‚è¼¯å¦‚ Google Searchã€‚
        """
        try:
            selected_tool = state.selected_tool
            
            if not selected_tool:
                self.logger.info("execute_tool: æ²’æœ‰é¸æ“‡å·¥å…·ï¼Œè·³éŽ")
                return {
                    "tool_results": [], 
                    "tool_round": state.tool_round + 1
                }
            
            self.logger.info(f"execute_tool: åŸ·è¡Œå·¥å…· {selected_tool}")
            
            # è¨ˆç®—é€²åº¦ç™¾åˆ†æ¯”
            current_round = state.tool_round + 1
            max_rounds = self.behavior_config.max_tool_rounds
            progress_percentage = int((current_round / max_rounds) * 50)  # å·¥å…·åŸ·è¡Œä½”ç¸½é€²åº¦çš„50%
            
            # é€šçŸ¥å·¥å…·åŸ·è¡Œé€²åº¦
            tool_names = {
                "google_search": "Google æœå°‹",
                "citation": "å¼•ç”¨è™•ç†"
            }
            tool_display_name = tool_names.get(selected_tool, selected_tool)
            
            await self._notify_progress(
                stage="execute_tool",
                message=f"ðŸ” æ­£åœ¨ä½¿ç”¨ {tool_display_name} æœå°‹è³‡æ–™... (ç¬¬ {current_round}/{max_rounds} è¼ª)",
                progress_percentage=progress_percentage,
                current_round=current_round,
                max_rounds=max_rounds,
                tool_name=selected_tool
            )
            
            tool_results = []
            
            if selected_tool == "google_search":
                # å…§åµŒ Google Search é‚è¼¯
                tool_results = self._execute_google_search(state)
            elif selected_tool == "citation":
                # å…§åµŒå¼•ç”¨è™•ç†é‚è¼¯
                tool_results = self._execute_citation_tool(state)
            
            # æ›´æ–°å·¥å…·ä½¿ç”¨è¼ªæ¬¡
            current_round = state.tool_round + 1
            
            self.logger.info(f"execute_tool: å®Œæˆï¼Œçµæžœæ•¸é‡={len(tool_results)}, è¼ªæ¬¡={current_round}")
            # self.logger.info(f"execute_tool: çµæžœ={tool_results}")
            
            
            return {
                "tool_results": tool_results,
                "tool_round": current_round
            }
            
        except Exception as e:
            self.logger.error(f"execute_tool å¤±æ•—: {e}")
            return {"tool_results": [], "tool_round": state.tool_round + 1}
    
    async def reflection(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šåæ€
        
        è©•ä¼°å·¥å…·çµæžœçš„è³ªé‡å’Œå®Œæ•´æ€§ã€‚
        """
        try:
            if not self.behavior_config.enable_reflection:
                # å¦‚æžœç¦ç”¨åæ€ï¼Œç›´æŽ¥èªç‚ºçµæžœå……åˆ†
                return {"is_sufficient": True}
            
            self.logger.info("reflection: é–‹å§‹åæ€å·¥å…·çµæžœ")
            
            # é€šçŸ¥åæ€éšŽæ®µ
            await self._notify_progress(
                stage="reflection",
                message="ðŸ¤” æ­£åœ¨è©•ä¼°æœå°‹çµæžœçš„å“è³ª...",
                progress_percentage=75
            )
            
            tool_results = state.tool_results
            research_topic = state.research_topic
            
            # ä½¿ç”¨å°ˆç”¨çš„åæ€ LLM æˆ–ç°¡åŒ–é‚è¼¯
            is_sufficient = self._evaluate_results_sufficiency(tool_results, research_topic)
            
            self.logger.info(f"reflection: çµæžœå……åˆ†åº¦={is_sufficient}")
            
            return {
                "is_sufficient": is_sufficient,
                "reflection_complete": True
            }
            
        except Exception as e:
            self.logger.error(f"reflection å¤±æ•—: {e}")
            return {"is_sufficient": True}  # å¤±æ•—æ™‚å‡è¨­å……åˆ†
    
    def evaluate_research(self, state: OverallState) -> str:
        """
        LangGraph è·¯ç”±ç¯€é»žï¼šè©•ä¼°ç ”ç©¶é€²åº¦
        
        æ±ºå®šæ˜¯å¦ç¹¼çºŒç ”ç©¶æˆ–é€²å…¥æœ€çµ‚ç­”æ¡ˆç”Ÿæˆã€‚
        """
        try:
            current_round = state.tool_round
            max_rounds = self.behavior_config.max_tool_rounds
            is_sufficient = state.is_sufficient
            
            # æ±ºç­–é‚è¼¯
            if is_sufficient or current_round >= max_rounds:
                self.logger.info(f"evaluate_research: å®Œæˆç ”ç©¶ (è¼ªæ¬¡={current_round}, å……åˆ†={is_sufficient})")
                return "finish"
            else:
                self.logger.info(f"evaluate_research: ç¹¼çºŒç ”ç©¶ (è¼ªæ¬¡={current_round}/{max_rounds})")
                return "continue"
                
        except Exception as e:
            self.logger.error(f"evaluate_research å¤±æ•—: {e}")
            return "finish"
    
    def decide_next_step(self, state: OverallState) -> Literal["continue", "finish"]:
        """æ±ºå®šä¸‹ä¸€æ­¥çš„è·¯ç”±å‡½æ•¸"""
        return self.evaluate_research(state)
    
    async def finalize_answer(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
        
        åŸºæ–¼æ‰€æœ‰å¯ç”¨çš„ä¿¡æ¯ç”Ÿæˆæœ€çµ‚å›žç­”ã€‚
        """
        try:
            self.logger.info("finalize_answer: ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
            
            # é€šçŸ¥ç­”æ¡ˆç”ŸæˆéšŽæ®µ
            await self._notify_progress(
                stage="finalize_answer",
                message="âœï¸ æ­£åœ¨æ•´ç†ç­”æ¡ˆ...",
                progress_percentage=90
            )
            
            messages = state.messages
            tool_results = state.tool_results or []
            
            # æ§‹å»ºä¸Šä¸‹æ–‡
            context = ""
            if tool_results:
                context = "\n".join([f"æœå°‹çµæžœ: {result}" for result in tool_results])
            
            # ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
            try:
                final_answer = self._generate_final_answer(messages, context)
            except Exception as e:
                self.logger.warning(f"LLM ç­”æ¡ˆç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨åŸºæœ¬å›žè¦†: {e}")
                final_answer = self._generate_basic_fallback_answer(messages, context)
            
            self.logger.info("finalize_answer: ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            
            # é€šçŸ¥å®Œæˆ
            await self._notify_progress(
                stage="completed",
                message="âœ… å›žç­”å®Œæˆï¼",
                progress_percentage=100
            )
            
            return {
                "final_answer": final_answer,
                "finished": True,
                "sources": self._extract_sources_from_results(tool_results)
            }
            
        except Exception as e:
            self.logger.error(f"finalize_answer å¤±æ•—: {e}")
            
            # é€šçŸ¥éŒ¯èª¤
            await self._notify_error(e)
            
            return {
                "final_answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›žç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚", 
                "finished": True
            }
    
    # å…§éƒ¨è¼”åŠ©æ–¹æ³•
    
    def _analyze_tool_necessity(self, messages: List[MsgNode]) -> bool:
        """åˆ†æžæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼ˆä½¿ç”¨å°ˆç”¨ LLM æ™ºèƒ½åˆ¤æ–·ï¼‰"""
        if not self.tool_analysis_llm:
            return self._analyze_tool_necessity_fallback(messages)
        
        try:
            # æ§‹å»ºåŒ…å«æ‰€æœ‰æ­·å²å°è©±çš„æç¤º
            messages_for_llm = [SystemMessage(content="""
è«‹åˆ†æžä»¥ä¸‹å°è©±æ­·å²ï¼Œåˆ¤æ–·æœ€æ–°ä¸€æ¢ç”¨æˆ¶è«‹æ±‚æ˜¯å¦éœ€è¦ä½¿ç”¨æœå°‹å·¥å…·ä¾†ç²å–æœ€æ–°è³‡è¨Šï¼š

åˆ¤æ–·æ¨™æº–ï¼š
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚éœ€è¦æœ€æ–°è³‡è¨Šã€å³æ™‚æ•¸æ“šã€æ–°èžäº‹ä»¶
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚éœ€è¦æŸ¥æ‰¾ç‰¹å®šäº‹å¯¦ã€æ•¸æ“šã€çµ±è¨ˆ
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚æ¶‰åŠç•¶å‰ç‹€æ³ã€æœ€æ–°ç™¼å±•
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚éœ€è¦é©—è­‰æˆ–å¼•ç”¨å¤–éƒ¨è³‡æº
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚æ˜¯ä¹‹å‰å•é¡Œçš„å»¶ä¼¸æˆ–è£œå……ï¼Œä¸”éœ€è¦æœå°‹ä¾†å›žç­”ã€‚

è«‹åªå›žç­”ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ï¼Œä¸éœ€è¦è§£é‡‹ã€‚
""")]
            
            for msg in messages:
                if msg.role == "user":
                    messages_for_llm.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    messages_for_llm.append(AIMessage(content=msg.content))

            response = self.tool_analysis_llm.invoke(messages_for_llm)
            result = response.content.strip().lower()
            
            needs_tools = "æ˜¯" in result or "yes" in result or "éœ€è¦" in result
            
            self.logger.info(f"LLM å·¥å…·éœ€æ±‚åˆ¤æ–· (åŸºæ–¼æ­·å²å°è©±): '{messages[-1].content[:50]}...' -> {needs_tools}")
            return needs_tools
            
        except Exception as e:
            self.logger.warning(f"LLM å·¥å…·éœ€æ±‚åˆ¤æ–·å¤±æ•—ï¼Œå›žé€€åˆ°é—œéµå­—æª¢æ¸¬: {e}")
            return self._analyze_tool_necessity_fallback(messages)

    def _analyze_tool_necessity_fallback(self, messages: List[MsgNode]) -> bool:
        """é—œéµå­—æª¢æ¸¬å›žé€€æ–¹æ¡ˆ (ä»åŸºæ–¼æœ€æ–°è¨Šæ¯)"""
        tool_keywords = [
            "æœå°‹", "æŸ¥è©¢", "æœ€æ–°", "ç¾åœ¨", "ä»Šå¤©", "ä»Šå¹´", "æŸ¥æ‰¾",
            "è³‡è¨Š", "è³‡æ–™", "æ–°èž", "æ¶ˆæ¯", "æ›´æ–°", "ç‹€æ³", "search",
            "find", "latest", "current", "recent", "what is", "å‘Šè¨´æˆ‘"
        ]
        
        content_lower = messages[-1].content.lower()
        return any(keyword in content_lower for keyword in tool_keywords)
    
    def _generate_search_queries(self, messages: List[MsgNode]) -> List[str]:
        """ç”Ÿæˆæœå°‹æŸ¥è©¢ï¼ˆåŸºæ–¼å®Œæ•´å°è©±æ­·å²ï¼‰"""
        try:
            self.logger.info("ç”Ÿæˆæœå°‹æŸ¥è©¢ï¼ˆåŸºæ–¼å®Œæ•´å°è©±æ­·å²ï¼‰")
            
            # System instruction for the LLM
            system_instruction = f"""
            ä»Šæ—¥æ˜¯ {get_current_date(self.config.system.timezone)}
                ä½ æ˜¯ä¸€å€‹ç¶²è·¯æœå°‹æŸ¥è©¢ç”Ÿæˆå™¨ã€‚
                ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„å°è©±æ­·å²å’Œæœ€æ–°ä¸€æ¢ç”¨æˆ¶è«‹æ±‚ï¼Œç”Ÿæˆ 1-2 å€‹ç²¾ç¢ºçš„ç¶²è·¯æœå°‹æŸ¥è©¢ã€‚
                è«‹ç¢ºä¿æŸ¥è©¢èƒ½æ¶µè“‹ç”¨æˆ¶è«‹æ±‚çš„æœ€æ–°è³‡è¨Šéœ€æ±‚ã€‚

                è¼¸å‡ºæ ¼å¼ï¼š
                è«‹ç›´æŽ¥æä¾› JSON æ ¼å¼çš„æœå°‹æŸ¥è©¢åˆ—è¡¨ã€‚å¦‚æžœæ²’æœ‰éœ€è¦æœå°‹çš„æŸ¥è©¢ï¼Œè«‹å›žå‚³ä¸€å€‹ç©ºçš„ JSON åˆ—è¡¨ `[]`ã€‚

                ç¯„ä¾‹ï¼š
                [ "æŸ¥è©¢ä¸€", "æŸ¥è©¢äºŒ" ]
                æˆ–è€…ï¼Œå¦‚æžœä¸éœ€è¦æŸ¥è©¢ï¼š
                []
                """
            
            messages_for_llm = [SystemMessage(content=system_instruction)]

            # Add conversation history as individual messages
            # Iterate in chronological order as LLM expects messages in order
            for msg in messages:
                print(msg.role, msg.content[:100])
                if msg.role == "user":
                    messages_for_llm.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    messages_for_llm.append(AIMessage(content=msg.content))
            
            response = self.tool_analysis_llm.invoke(messages_for_llm)
            raw_content = response.content.strip()
            
            parsed_json = None
            if "```json" in raw_content:
                # Try to extract JSON from markdown code block
                start_marker = "```json"
                end_marker = "```"
                start_index = raw_content.find(start_marker)
                end_index = raw_content.rfind(end_marker)

                if start_index != -1 and end_index != -1 and end_index > start_index:
                    json_str = raw_content[start_index + len(start_marker):end_index].strip()
                    try:
                        if json_str:
                            parsed_json = json.loads(json_str)
                        else:
                            parsed_json = []
                    except json.JSONDecodeError:
                        self.logger.warning(f"ç„¡æ³•è§£æž Markdown ä¸­çš„ JSONã€‚å›žæ‡‰: {raw_content}")
            
            if parsed_json is None and raw_content.startswith("["):
                # Try to parse as direct JSON array
                try:
                    parsed_json = json.loads(raw_content)
                except json.JSONDecodeError:
                    self.logger.warning(f"ç„¡æ³•è§£æžç‚º JSON åˆ—è¡¨ã€‚å›žæ‡‰: {raw_content}")

            if parsed_json is None or not isinstance(parsed_json, list):
                self.logger.warning(f"LLM ç”Ÿæˆçš„æœå°‹æŸ¥è©¢å…§å®¹ç‚ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¢ºï¼Œå›žé€€åˆ°ç°¡åŒ–æ¨¡å¼ã€‚å›žæ‡‰: {raw_content}")
                return [messages[-1].content[:100]]
            
            queries = parsed_json

            self.logger.info(f"LLM ç”Ÿæˆæœå°‹æŸ¥è©¢: {queries}")
            return queries
            
        except Exception as e:
            self.logger.warning(f"LLM ç”Ÿæˆæœå°‹æŸ¥è©¢å¤±æ•—ï¼Œå›žé€€åˆ°ç°¡åŒ–æ¨¡å¼: {e}")
            return [messages[-1].content[:100]]
    
    def _execute_google_search(self, state: OverallState) -> List[str]:
        """åŸ·è¡Œ Google æœå°‹ï¼ˆå…§åµŒå¯¦ä½œï¼‰"""
        if not self.google_client:
            raise ValueError("Google å®¢æˆ¶ç«¯æœªé…ç½®ï¼Œç„¡æ³•åŸ·è¡Œ Google æœå°‹ã€‚")
        
        search_queries = state.search_queries
        self.logger.debug(f"DEBUG: _execute_google_search: search_queries type: {type(search_queries)}, value: {search_queries}")
        results = []
        
        try:
            for query in search_queries[:2]:  # é™åˆ¶æœ€å¤š2å€‹æŸ¥è©¢
                current_date = get_current_date(timezone_str=self.config.system.timezone)
                
                # æº–å‚™å‚³éžçµ¦ Gemini æ¨¡åž‹çš„æç¤º
                formatted_prompt = web_searcher_instructions.format(
                    research_topic=query,
                    current_date=current_date
                )
                
                try:
                    model_name = self.tool_analysis_llm.model
                    # èª¿ç”¨ Gemini API ä¸¦å•Ÿç”¨ google_search å·¥å…·
                    response = self.google_client.models.generate_content(
                        model=model_name,
                        contents=formatted_prompt,
                        config={
                            "tools": [{"google_search": {}}],
                            "temperature": 0
                        }
                    )
                    if response.text:
                        # ç”Ÿæˆä¸€å€‹å”¯ä¸€çš„ ID çµ¦æ¯å€‹æŸ¥è©¢çµæžœï¼Œç”¨æ–¼ resolve_urls
                        query_id = state.tool_round * 100 + search_queries.index(query) # ç¢ºä¿å”¯ä¸€æ€§
                        
                        # åˆå§‹åŒ–å®‰å…¨é è¨­å€¼
                        grounding_chunks = []
                        resolved_urls = {} # ç¢ºä¿åˆå§‹åŒ–ç‚ºç©ºå­—å…¸
                        citations = []
                        modified_text = response.text

                        # å˜—è©¦æå– grounding_chunks
                        if response.candidates and len(response.candidates) > 0:
                            candidate_0 = response.candidates[0]
                            if candidate_0 and hasattr(candidate_0, 'grounding_metadata') and candidate_0.grounding_metadata:
                                if hasattr(candidate_0.grounding_metadata, 'grounding_chunks') and candidate_0.grounding_metadata.grounding_chunks:
                                    grounding_chunks = candidate_0.grounding_metadata.grounding_chunks

                        # è™•ç† URL è§£æžå’Œå¼•ç”¨
                        try:
                            resolved_urls = resolve_urls(grounding_chunks, query_id)
                            # ç¢ºèªé€™è£¡æ²’æœ‰å°åˆ—è¡¨é¡žåž‹çš„æª¢æŸ¥ï¼Œå› ç‚º resolved_urls æ‡‰ç‚ºå­—å…¸
                        except Exception as url_e:
                            self.logger.warning(f"è§£æž URL å¤±æ•—: {url_e}")
                            resolved_urls = {} # ç¢ºä¿é‡è¨­ç‚ºç©ºå­—å…¸

                        try:
                            citations = get_citations(response, resolved_urls)
                            # ç¢ºèªé€™è£¡ä¸å†æœ‰å°åˆ—è¡¨é¡žåž‹çš„æª¢æŸ¥ï¼Œå› ç‚º get_citations å…§éƒ¨æœƒè™•ç†
                        except Exception as cite_e:
                            self.logger.warning(f"ç²å–å¼•ç”¨å¤±æ•—: {cite_e}")
                            import traceback
                            print(traceback.format_exc())
                            citations = []

                        # æ’å…¥å¼•ç”¨æ¨™è¨˜
                        modified_text = insert_citation_markers(response.text, citations)
                        
                        results.append(modified_text)
                    else:
                        results.append(f"é‡å°æŸ¥è©¢ã€Œ{query}ã€æ²’æœ‰æ‰¾åˆ°å…§å®¹ã€‚")
                except Exception as llm_e:
                    self.logger.error(f"LLM åŸ·è¡Œ Google æœå°‹å¤±æ•—: {llm_e}")
                    results.append(f"åŸ·è¡Œæœå°‹æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤: {llm_e}")
                
        except Exception as e:
            self.logger.error(f"Google æœå°‹å¤±æ•—: {e}")
        
        return results
    
    def _execute_citation_tool(self, state: OverallState) -> List[str]:
        """åŸ·è¡Œå¼•ç”¨å·¥å…·ï¼ˆå…§åµŒå¯¦ä½œï¼‰"""
        tool_results = state.tool_results
        
        # ç°¡åŒ–çš„å¼•ç”¨ç”Ÿæˆ
        citations = []
        for i, result in enumerate(tool_results, 1):
            citation = f"[{i}] {result[:50]}..."
            citations.append(citation)
        
        return citations
    
    def _evaluate_results_sufficiency(self, tool_results: List[str], research_topic: str) -> bool:
        """è©•ä¼°çµæžœæ˜¯å¦å……åˆ†"""
        # å¦‚æžœæ²’æœ‰å·¥å…·çµæžœï¼Œè¡¨ç¤ºå¯èƒ½æ²’æœ‰åˆé©çš„å·¥å…·ï¼Œèªç‚ºæ‡‰è©²çµæŸ
        if not tool_results:
            return True  # ä¿®æ”¹ï¼šæ²’æœ‰çµæžœæ™‚èªç‚ºå……åˆ†ï¼Œé¿å…ç„¡é™å¾ªç’°
        
        # å¦‚æžœæœ‰çµæžœä¸”é•·åº¦åˆç†ï¼Œèªç‚ºå……åˆ†
        total_length = sum(len(result) for result in tool_results)
        return total_length > 20  # è‡³å°‘20å­—ç¬¦çš„çµæžœ
    
    def _generate_final_answer(self, messages: List[MsgNode], context: str) -> str:
        """ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆï¼ˆä½¿ç”¨å°ˆç”¨ LLMï¼‰"""
        if not messages:
            return "å—¯...å¥½åƒæ²’æœ‰æ”¶åˆ°ä½ çš„è¨Šæ¯è€¶ï¼Œå¯ä»¥å†è©¦ä¸€æ¬¡å—Žï¼ŸðŸ˜…"
        
        latest_message = messages[-1]
        user_question = latest_message.content
        
        # ä¸ç®¡æœ‰æ²’æœ‰contextï¼Œéƒ½ç”¨LLMç”Ÿæˆæ›´è‡ªç„¶çš„å›žæ‡‰
        try:
            # æ§‹å»ºæ‰€æœ‰æ­·å²å°è©±çš„è¨Šæ¯åˆ—è¡¨ï¼Œé™¤äº†ç•¶å‰ç”¨æˆ¶å•é¡Œ

            if context:
                messages_for_final_answer = [SystemMessage(
                    content=f"""
                    ä½ æ˜¯ä¸€å€‹å‹å–„ã€è°æ˜Žçš„èŠå¤©åŠ©æ‰‹ã€‚è«‹ç”¨è‡ªç„¶ã€äººæ€§åŒ–çš„æ–¹å¼å›žç­”ç”¨æˆ¶çš„å•é¡Œã€‚
                    ä»¥ä¸‹æ˜¯ä½ ä½¿ç”¨å·¥å…·å¾—åˆ°çš„ç›¸é—œè³‡è¨Šï¼Œè«‹ç”¨æ–¼å›žç­”ç”¨æˆ¶çš„å•é¡Œï¼š
                    {context}
                    """
                )]
            else:
                messages_for_final_answer = [SystemMessage(content="ä½ æ˜¯ä¸€å€‹å‹å–„ã€è°æ˜Žçš„èŠå¤©åŠ©æ‰‹ã€‚è«‹ç”¨è‡ªç„¶ã€äººæ€§åŒ–çš„æ–¹å¼å›žç­”ç”¨æˆ¶çš„å•é¡Œã€‚")]
                
            for msg in messages[:-1]: # é™¤äº†æœ€å¾Œä¸€æ¢æ¶ˆæ¯ï¼ˆç•¶å‰ç”¨æˆ¶å•é¡Œï¼‰
                if msg.role == "user":
                    messages_for_final_answer.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    messages_for_final_answer.append(AIMessage(content=msg.content))

            # å°‡ç•¶å‰ç”¨æˆ¶å•é¡Œä½œç‚º HumanMessage åŠ å…¥
            messages_for_final_answer.append(HumanMessage(content=user_question))
            response = self.final_answer_llm.invoke(messages_for_final_answer)
            return response.content.strip()
            
        except Exception as e:
            self.logger.warning(f"ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}")
        
        return self._generate_basic_fallback_answer(messages, context)

    def _generate_basic_fallback_answer(self, messages: List[MsgNode], context: str) -> str:
        return "å‡ºç¾éŒ¯èª¤ï¼Œè«‹å†è©¦ä¸€æ¬¡ ðŸ”„"

    def _extract_sources_from_results(self, tool_results: List[str]) -> List[Dict[str, str]]:
        """å¾žå·¥å…·çµæžœä¸­æå–ä¾†æºä¿¡æ¯"""
        sources = []
        if tool_results:
            for result in tool_results:
                if isinstance(result, str) and "http" in result:
                    # ç°¡åŒ–çš„ä¾†æºæå–é‚è¼¯
                    sources.append({
                        "title": result[:100] + "..." if len(result) > 100 else result,
                        "url": "",
                        "snippet": result
                    })
        return sources



def create_unified_agent(config: Optional[AppConfig] = None) -> UnifiedAgent:
    """å»ºç«‹çµ±ä¸€ Agent å¯¦ä¾‹"""
    return UnifiedAgent(config)


def create_agent_graph(config: Optional[AppConfig] = None) -> StateGraph:
    """å»ºç«‹ä¸¦ç·¨è­¯ Agent åœ–"""
    agent = create_unified_agent(config)
    return agent.build_graph() 