"""
çµ±ä¸€çš„ LangGraph åœ–å¯¦ä½œ

æ ¹æ“šé…ç½®å‹•æ…‹æ§‹å»º LangGraph çš„ StateGraphï¼Œå®šç¾©æ‰€æœ‰ç¯€é»žçš„å¯¦ç¾ï¼Œ
åŒ…æ‹¬å·¥å…·é‚è¼¯çš„ç›´æŽ¥å…§åµŒã€‚åš´æ ¼åƒè€ƒ reference_arch.md ä¸­å®šç¾©çš„ Agent æ ¸å¿ƒæµç¨‹ã€‚
"""

import asyncio
import logging
import json
import copy
import re
import uuid
from typing import Dict, Any, List, Optional, Literal, Union
from datetime import datetime

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langchain_core.runnables import RunnableConfig
from google.genai import Client

from schemas.agent_types import OverallState, MsgNode, ToolPlan, AgentPlan, ToolExecutionState, ReminderDetails, ToolExecutionResult
from utils.config_loader import load_typed_config
from .progress_types import ProgressStage, ToolStatus
from prompt_system.prompts import get_current_date, PromptSystem
from schemas.config_types import AppConfig
from .progress_mixin import ProgressMixin
from .agent_utils import _extract_text_content

# å°Žå…¥ LangChain å·¥å…·
from tools import GoogleSearchTool, set_reminder
from tools.youtube_summary import YouTubeSummaryTool
from utils.youtube_utils import extract_first_youtube_url
from langchain_core.messages import ToolMessage


class UnifiedAgent(ProgressMixin):
    """çµ±ä¸€çš„ Agent å¯¦ä½œï¼Œæ ¹æ“šé…ç½®å‹•æ…‹èª¿æ•´è¡Œç‚º"""
    
    def __init__(self, config: Optional[AppConfig] = None):
        """
        åˆå§‹åŒ–çµ±ä¸€ Agent
        
        Args:
            config: åž‹åˆ¥å®‰å…¨çš„é…ç½®å¯¦ä¾‹ï¼Œå¦‚æžœç‚º None å‰‡è¼‰å…¥é è¨­é…ç½®
        """
        # é¦–å…ˆåˆå§‹åŒ– ProgressMixin
        super().__init__()
        
        self.config = config or load_typed_config()
        self.logger = logging.getLogger(__name__)
        
        # å¾žé…ç½®ç²å– agent è¨­å®š - ä½¿ç”¨åž‹åˆ¥å®‰å…¨å­˜å–
        self.agent_config = self.config.agent
        self.tools_config = self.config.agent.tools
        self.behavior_config = self.config.agent.behavior
        
        # åˆå§‹åŒ–å¤šå€‹ LLM å¯¦ä¾‹
        self.llm_instances = self._initialize_llm_instances()
        
        # åˆå§‹åŒ– Google å®¢æˆ¶ç«¯ï¼ˆå¦‚æžœå·¥å…·å•Ÿç”¨ï¼‰
        self.google_client = None
        if self.config.is_tool_enabled("google_search"):
            api_key = self.config.gemini_api_key
            if api_key:
                self.google_client = Client(api_key=api_key)
        
        # åˆå§‹åŒ–æç¤ºè©žç³»çµ±
        self.prompt_system = PromptSystem(
            persona_cache_enabled=self.config.prompt_system.persona.cache_personas
        )
        
        # åˆå§‹åŒ–é€²åº¦ LLM
        self._progress_llm = self.llm_instances.get("progress_msg")
        
        # åˆå§‹åŒ–ä¸¦ç¶å®š LangChain å·¥å…·
        self._initialize_tools()
    
    def _initialize_llm_instances(self) -> Dict[str, Optional[ChatGoogleGenerativeAI]]:
        """åˆå§‹åŒ–ä¸åŒç”¨é€”çš„ LLM å¯¦ä¾‹"""
        api_key = self.config.gemini_api_key
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is required")
        
        # ä½¿ç”¨åž‹åˆ¥å®‰å…¨çš„é…ç½®å­˜å–
        llm_configs_source = self.config.llm.models
        
        llm_instances = {}
        for purpose, llm_config in llm_configs_source.items():
            try:
                # llm_config ç¾åœ¨ç¸½æ˜¯ LLMModelConfig å¯¦ä¾‹
                model_name = llm_config.model
                temperature = llm_config.temperature
                max_output_tokens = llm_config.max_output_tokens
                
                # æº–å‚™ LLM åƒæ•¸
                llm_params = {
                    "model": model_name,
                    "temperature": temperature,
                    "api_key": api_key
                }
                
                # å¦‚æžœè¨­ç½®äº† max_output_tokens ä¸”ç‚ºæœ‰æ•ˆå€¼ï¼Œå‰‡æ·»åŠ åˆ°åƒæ•¸ä¸­
                if max_output_tokens is not None and max_output_tokens > 0:
                    llm_params["max_output_tokens"] = max_output_tokens
                
                llm_instances[purpose] = ChatGoogleGenerativeAI(**llm_params)
                self.logger.info(f"åˆå§‹åŒ– {purpose} LLM: {model_name} (max_tokens: {max_output_tokens})")
            except Exception as e:
                self.logger.warning(f"åˆå§‹åŒ– {purpose} LLM å¤±æ•—: {e}")
                llm_instances[purpose] = None
        
        self.tool_analysis_llm: ChatGoogleGenerativeAI = llm_instances.get("tool_analysis")
        self.final_answer_llm: ChatGoogleGenerativeAI = llm_instances.get("final_answer")
        
        return llm_instances
    
    def _initialize_tools(self):
        """åˆå§‹åŒ–ä¸¦ç¶å®š LangChain å·¥å…·åˆ° LLM"""
        self.available_tools = []
        self.tool_mapping = {}
        
        # åˆå§‹åŒ– Google æœå°‹å·¥å…·ï¼ˆå¦‚æžœå•Ÿç”¨ï¼‰
        if self.config.is_tool_enabled("google_search") and self.google_client:
            self.google_search_tool = GoogleSearchTool(
                google_client=self.google_client,
                prompt_system_instance=self.prompt_system,
                config=self.config,
                logger=self.logger
            )
            self.available_tools.append(self.google_search_tool)
            self.tool_mapping[self.google_search_tool.name] = self.google_search_tool
            self.logger.info("å·²åˆå§‹åŒ– Google æœå°‹å·¥å…·")
        
        # åˆå§‹åŒ–æé†’å·¥å…·ï¼ˆå¦‚æžœå•Ÿç”¨ï¼‰
        if self.config.is_tool_enabled("reminder"):
            self.available_tools.append(set_reminder)
            self.tool_mapping[set_reminder.name] = set_reminder
            self.logger.info("å·²åˆå§‹åŒ–æé†’å·¥å…·")

        # åˆå§‹åŒ– YouTube æ‘˜è¦å·¥å…·ï¼ˆåƒ…åŠ å…¥ mappingï¼Œä¸ç¶å®šçµ¦ LLMï¼‰
        if self.config.is_tool_enabled("youtube_summary") and self.google_client:
            self.youtube_summary_tool = YouTubeSummaryTool(
                google_client=self.google_client,
                config=self.config,
                logger=self.logger
            )
            # ä¸åŠ å…¥ self.available_tools ä»¥é¿å…æš´éœ²çµ¦ LLM
            self.tool_mapping[self.youtube_summary_tool.name] = self.youtube_summary_tool
            self.logger.info("å·²åˆå§‹åŒ– YouTube æ‘˜è¦å·¥å…·")
        
        # ç¶å®šå·¥å…·åˆ° tool_analysis_llm
        if self.available_tools and self.tool_analysis_llm:
            self.tool_analysis_llm = self.tool_analysis_llm.bind_tools(self.available_tools)
            self.logger.info(f"å·²ç¶å®š {len(self.available_tools)} å€‹å·¥å…·åˆ° LLM")
        else:
            if not self.tool_analysis_llm:
                self.logger.warning("tool_analysis_llm æœªåˆå§‹åŒ–ï¼Œç„¡æ³•ç¶å®šå·¥å…·")
            elif not self.available_tools:
                self.logger.warning("æ²’æœ‰å¯ç”¨çš„å·¥å…·ï¼Œä½¿ç”¨æœªç¶å®šå·¥å…·çš„ LLM")
    
    async def _build_agent_messages_for_progress(self, stage: str, current_state) -> List:
        """Agent åªè™•ç† Agent ç‰¹æœ‰è³‡è¨Š
        
        Args:
            stage: é€²åº¦éšŽæ®µ
            current_state: ç•¶å‰ç‹€æ…‹
            
        Returns:
            List[BaseMessage]: Agent æ§‹å»ºçš„ messages
        """
        from langchain_core.messages import BaseMessage
        
        if not current_state:
            return []
        
        # 1. ç²å–å‰10å‰‡æ¶ˆæ¯
        recent_msg_nodes = current_state.messages[-10:] if current_state.messages else []
        
        # 2. ä½¿ç”¨å›ºå®š persona æ§‹å»º system prompt
        system_prompt = self.prompt_system.get_system_instructions(
            self.config, persona=current_state.current_persona
        )
        
        # 3. æ§‹å»º messages
        messages = self._build_messages_for_llm(recent_msg_nodes, system_prompt)
        
        return messages
    
    def build_graph(self) -> StateGraph:
        """å»ºç«‹ç°¡åŒ–çš„ LangGraph"""
        builder = StateGraph(OverallState)
        
        # æ ¸å¿ƒç¯€é»ž
        builder.add_node("generate_query_or_plan", self.generate_query_or_plan)
        builder.add_node("execute_tools", self.execute_tools_node)
        builder.add_node("reflection", self.reflection)
        builder.add_node("finalize_answer", self.finalize_answer)
        
        # æµç¨‹è¨­ç½®
        builder.add_edge(START, "generate_query_or_plan")
        
        # æ¢ä»¶è·¯ç”±ï¼šæ ¹æ“šè¨ˆåŠƒæ±ºå®šä¸‹ä¸€æ­¥
        builder.add_conditional_edges(
            "generate_query_or_plan",
            self.route_and_dispatch_tools,
            {
                "execute_tools": "execute_tools",
                "direct_answer": "finalize_answer"
            }
        )
        
        # Connect dispatch node to reflection
        builder.add_edge("execute_tools", "reflection")
        
        # åæ€å¾Œçš„è·¯ç”±æ±ºç­–
        builder.add_conditional_edges(
            "reflection",
            self.decide_next_step,
            {
                "continue": "generate_query_or_plan",  # é‡æ–°è¦åŠƒä¸‹ä¸€è¼ª
                "finish": "finalize_answer"
            }
        )
        
        builder.add_edge("finalize_answer", END)
        return builder.compile()

    async def generate_query_or_plan(self, state: OverallState) -> Dict[str, Any]:
        """
        çµ±ä¸€çš„è¨ˆåŠƒç”Ÿæˆç¯€é»žï¼šä½¿ç”¨ LangChain å·¥å…·ç¶å®šçš„ LLM é€²è¡Œåˆ†æž
        
        LLM æœƒè‡ªå‹•æ±ºå®šæ˜¯å¦éœ€è¦èª¿ç”¨å·¥å…·ï¼Œä¸¦ç”Ÿæˆç›¸æ‡‰çš„ tool_calls
        """
        try:
            self.logger.info("generate_query_or_plan: é–‹å§‹åˆ†æžç”¨æˆ¶è«‹æ±‚")
            
            # â˜… æ–°å¢žï¼šç¢ºå®š current_personaï¼ˆåœ¨ç¬¬ä¸€å€‹ç¯€é»žè™•ç†ï¼‰
            if not state.current_persona:
                if self.config.prompt_system.persona.random_selection:
                    state.current_persona = self.prompt_system.get_random_persona_name()
                else:
                    state.current_persona = self.config.prompt_system.persona.default_persona
            
            # é€šçŸ¥é–‹å§‹éšŽæ®µ
            await self._notify_progress(
                stage=ProgressStage.GENERATE_QUERY, 
                message="",
                progress_percentage=30,
                current_state=state
            )
            
            user_content = _extract_text_content(state.messages[-1].content)
            max_tool_rounds = self.behavior_config.max_tool_rounds
            
            if max_tool_rounds == 0:
                # ç´”å°è©±æ¨¡å¼
                return {
                    "agent_plan": AgentPlan(needs_tools=False),
                    "tool_round": 0
                }
            
            # æª¢æ¸¬ YouTube URLï¼Œè‹¥æœ‰å‰‡é å‚™ä¸€å€‹ç¨‹å¼åŒ–å·¥å…·èª¿ç”¨ (ç¨å¾Œèˆ‡ LLM çµæžœåˆä½µ)
            youtube_tool_call = None

            # åœ¨æœ€è¿‘ 10 å‰‡è¨Šæ¯å…§å°‹æ‰¾ç¬¬ä¸€å€‹ YouTube URLï¼ˆç”±è¿‘åˆ°é ï¼‰
            yt_url = None
            recent_msgs = state.messages[-10:]
            for m in reversed(recent_msgs):
                text_part = _extract_text_content(m.content)
                hit = extract_first_youtube_url(text_part)
                if hit:
                    yt_url = hit
                    break

            if yt_url and self.config.is_tool_enabled("youtube_summary"):
                self.logger.info(f"åµæ¸¬åˆ° YouTube URL: {yt_url}ï¼Œå°‡åŠ å…¥ youtube_summary å·¥å…·èª¿ç”¨ (ç¨å¾Œåˆä½µ)")

                youtube_tool_call = {
                    "id": str(uuid.uuid4()),
                    "name": self.youtube_summary_tool.name,
                    "args": {"url": yt_url}
                }
            
            # ä½¿ç”¨ç¶å®šå·¥å…·çš„ LLM é€²è¡Œåˆ†æž
            if not self.tool_analysis_llm:
                # å›žé€€åˆ°ç°¡å–®é‚è¼¯
                needs_tools = self._analyze_tool_necessity_fallback(state.messages)
                agent_plan = AgentPlan(
                    needs_tools=needs_tools,
                    reasoning="LLM æœªå¯ç”¨ï¼Œä½¿ç”¨ç°¡åŒ–é‚è¼¯æ±ºç­–"
                )
            else:
                # system_prompt = ""
                system_prompt = self._build_planning_system_prompt(state.messages_global_metadata)
                messages_for_llm = self._build_messages_for_llm(state.messages, system_prompt)
                self._log_messages(messages_for_llm, "messages_for_llm (planning)")
                ai_message = await self.tool_analysis_llm.ainvoke(messages_for_llm)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·èª¿ç”¨
                if ai_message.tool_calls or youtube_tool_call:
                    # æœ‰å·¥å…·èª¿ç”¨ï¼Œéœ€è¦åŸ·è¡Œå·¥å…·
                    combined_tool_calls = list(ai_message.tool_calls) if ai_message.tool_calls else []
                    if youtube_tool_call:
                        # å°‡ YouTube æ‘˜è¦å·¥å…·æ”¾åœ¨æœ€å‰é¢ï¼Œé¿å…èˆ‡å…¶å®ƒå·¥å…·è¡çª
                        combined_tool_calls.insert(0, youtube_tool_call)

                    logging.debug(f"æœ€çµ‚ tool_calls: {combined_tool_calls}")

                    agent_plan = AgentPlan(
                        needs_tools=True,
                        reasoning="LLM æ±ºå®šèª¿ç”¨å·¥å…·" + ("ï¼Œä¸¦åŠ å…¥ YouTube æ‘˜è¦" if youtube_tool_call else "")
                    )

                    # å°‡ combined_tool_calls å­˜å„²åœ¨ state ä¸­ä¾›å¾ŒçºŒç¯€é»žä½¿ç”¨
                    state.metadata = state.metadata or {}
                    state.metadata["pending_tool_calls"] = combined_tool_calls
                    
                    # ä¿å­˜ AI è¨Šæ¯å’Œå·¥å…·èª¿ç”¨åˆ°å°è©±æ­·å²
                    state.messages.append(MsgNode(
                        role="assistant",
                        content=ai_message.content or "",
                        metadata={"tool_calls": combined_tool_calls}
                    ))
                else:
                    # æ²’æœ‰å·¥å…·èª¿ç”¨ï¼Œç›´æŽ¥å›žç­”
                    agent_plan = AgentPlan(
                        needs_tools=False,
                        reasoning="LLM æ±ºå®šç›´æŽ¥å›žç­”ï¼Œç„¡éœ€å·¥å…·"
                    )
            
            self.logger.info(f"ç”Ÿæˆè¨ˆåŠƒ: éœ€è¦å·¥å…·={agent_plan.needs_tools}")
            
            return {
                "agent_plan": agent_plan,
                "tool_round": state.tool_round + 1,
                "research_topic": user_content,
                "metadata": state.metadata,
                "messages": state.messages,
            }
            
        except Exception as e:
            self.logger.error(f"generate_query_or_plan å¤±æ•—: {e}")
            return {
                "agent_plan": AgentPlan(needs_tools=False),
                "finished": True
            }

    def route_and_dispatch_tools(self, state: OverallState) -> str:
        """
        è¦åŠƒå¾Œçš„è·¯ç”±æ±ºç­–ï¼š
        æª¢æŸ¥æ˜¯å¦éœ€è¦åŸ·è¡Œå·¥å…·
        """
        agent_plan = state.agent_plan
        
        if agent_plan and agent_plan.needs_tools:
            # æª¢æŸ¥æ˜¯å¦æœ‰å¾…åŸ·è¡Œçš„å·¥å…·èª¿ç”¨
            pending_tool_calls = state.metadata.get("pending_tool_calls") if state.metadata else None
            if pending_tool_calls:
                self.logger.info(f"route_and_dispatch_tools: éœ€è¦åŸ·è¡Œ {len(pending_tool_calls)} å€‹å·¥å…·")
                return "execute_tools"
            else:
                self.logger.error("route_and_dispatch_tools: è¨ˆåŠƒéœ€è¦å·¥å…·ä½†æ²’æœ‰å¾…åŸ·è¡Œçš„å·¥å…·èª¿ç”¨")
                return "direct_answer"
        else:
            self.logger.info("route_and_dispatch_tools: ç„¡éœ€å·¥å…·ï¼Œç›´æŽ¥å›žç­”")
            return "direct_answer"

    async def execute_tools_node(self, state: OverallState) -> Dict[str, Any]:
        """åŸ·è¡Œ LangChain å·¥å…·èª¿ç”¨"""
        try:
            # ç²å–å¾…åŸ·è¡Œçš„å·¥å…·èª¿ç”¨
            pending_tool_calls = state.metadata.get("pending_tool_calls") if state.metadata else None
            if not pending_tool_calls:
                self.logger.warning("execute_tools_node: æ²’æœ‰å¾…åŸ·è¡Œçš„å·¥å…·èª¿ç”¨")
                return {"tool_results": []}
            
            self.logger.info(f"execute_tools_node: å¹³è¡ŒåŸ·è¡Œ {len(pending_tool_calls)} å€‹å·¥å…·èª¿ç”¨")

            # Phase3: å…ˆé€šçŸ¥å·¥å…·æ¸…å–® (todo list)
            try:
                await self._notify_progress(
                    stage=ProgressStage.TOOL_LIST,
                    message="ðŸ› ï¸ å·¥å…·é€²åº¦",
                    todo=[tc["name"] for tc in pending_tool_calls],
                    current_state=state
                )
            except Exception as e:
                self.logger.warning(f"ç™¼é€å·¥å…·æ¸…å–®é€²åº¦å¤±æ•—: {e}")

            # é€šçŸ¥å·¥å…·åŸ·è¡ŒéšŽæ®µ
            await self._notify_progress(
                stage=ProgressStage.TOOL_EXECUTION,
                message="",  # ä½¿ç”¨é…ç½®ä¸­çš„è¨Šæ¯
                progress_percentage=50,
                current_state=state
            )
            
            # å‰µå»ºå¹³è¡ŒåŸ·è¡Œçš„ä»»å‹™
            tasks = []
            for tool_call in pending_tool_calls:
                task = self._execute_single_tool_call(tool_call)
                tasks.append(task)
            
            # å¹³è¡ŒåŸ·è¡Œæ‰€æœ‰å·¥å…·èª¿ç”¨
            tool_results_with_messages = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è™•ç†çµæžœ
            new_tool_messages: List[ToolMessage] = []
            tool_results: List[str] = []
            
            for i, result in enumerate(tool_results_with_messages):
                tool_call = pending_tool_calls[i]
                tool_call_id = tool_call["id"]
                tool_name = tool_call["name"]
                state_update_dict = {}
                if isinstance(result, Exception):
                    # è™•ç†ç•°å¸¸
                    self.logger.error(f"å·¥å…· {tool_name} åŸ·è¡Œç•°å¸¸: {result}")
                    error_msg = f"å·¥å…·åŸ·è¡Œç•°å¸¸: {str(result)}"
                    tool_message = ToolMessage(
                        content=error_msg,
                        tool_call_id=tool_call_id
                    )
                    new_tool_messages.append(tool_message)
                    tool_results.append(error_msg)
                else:
                    # æ­£å¸¸çµæžœ
                    tool_message, tool_execution_result = result
                    new_tool_messages.append(tool_message)
                    tool_results.append(tool_execution_result.message)
                    
                    # ç‰¹åˆ¥è™•ç† set_reminder å·¥å…·çš„æˆåŠŸçµæžœ
                    if tool_name == "set_reminder" and tool_execution_result.success:
                        state_update_dict.update(await self._process_reminder_result(state, tool_execution_result))
            
            # å°‡ ToolMessage è½‰æ›ç‚º MsgNode ä¸¦æ·»åŠ åˆ°å°è©±æ­·å²
            for tool_msg in new_tool_messages:
                state.messages.append(MsgNode(
                    role="tool",
                    content=tool_msg.content,
                    metadata={"tool_call_id": tool_msg.tool_call_id}
                ))
            
            # æ¸…é™¤å·²åŸ·è¡Œçš„å·¥å…·èª¿ç”¨
            if state.metadata:
                state.metadata.pop("pending_tool_calls", None)
            
            self.logger.info(f"å¹³è¡ŒåŸ·è¡Œå®Œæˆï¼Œå…±è™•ç† {len(tool_results)} å€‹å·¥å…·çµæžœ")
            
            return {
                "tool_results": tool_results,
                "metadata": state.metadata,
                "messages": state.messages,
            } | state_update_dict # Merge state_update_dict into the return dict
            
        except Exception as e:
            self.logger.error(f"execute_tools_node å¤±æ•—: {e}")
            return {
                "tool_results": [],
                "error": str(e)
            }
    
    async def _execute_single_tool_call(self, tool_call: Dict[str, Any]) -> tuple[ToolMessage, ToolExecutionResult]:
        """åŸ·è¡Œå–®å€‹å·¥å…·èª¿ç”¨ - ç”¨æ–¼å¹³è¡ŒåŸ·è¡Œ"""
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        tool_call_id = tool_call["id"]
        
        self.logger.info(f"åŸ·è¡Œå·¥å…·: {tool_name} with args: {tool_args}")

        # Phase3: å·¥å…·ç‹€æ…‹ - running
        try:
            await self._notify_progress(
                stage=ProgressStage.TOOL_STATUS,
                message="",  # ä½¿ç”¨é…ç½®è¨Šæ¯
                progress_percentage=50,  # ä¿æŒé€²åº¦æ¢
                tool=tool_name,
                status=ToolStatus.RUNNING
            )
        except Exception as e:
            self.logger.warning(f"é€šçŸ¥å·¥å…· {tool_name} åŸ·è¡Œä¸­ ç‹€æ…‹å¤±æ•—: {e}")
        
        try:
            # æŸ¥æ‰¾å°æ‡‰çš„å·¥å…·
            selected_tool = self.tool_mapping.get(tool_name)
            if not selected_tool:
                error_result = ToolExecutionResult(
                    success=False,
                    message=f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å·¥å…· '{tool_name}'"
                )
                tool_message = ToolMessage(
                    content=error_result.message,
                    tool_call_id=tool_call_id
                )
                # Phase3: å·¥å…·ç‹€æ…‹ - error
                try:
                    await self._notify_progress(
                        stage=ProgressStage.TOOL_STATUS,
                        message="",  # ä½¿ç”¨é…ç½®è¨Šæ¯
                        progress_percentage=50,  # ä¿æŒé€²åº¦æ¢
                        tool=tool_name,
                        status=ToolStatus.ERROR
                    )
                except Exception as ne:
                    self.logger.warning(f"é€šçŸ¥å·¥å…· {tool_name} error ç‹€æ…‹å¤±æ•—: {ne}")
                return tool_message, error_result
            
            # åŸ·è¡Œå·¥å…·
            if hasattr(selected_tool, 'ainvoke'):
                raw_result = await selected_tool.ainvoke(tool_args)
            elif hasattr(selected_tool, 'invoke'):
                raw_result = selected_tool.invoke(tool_args)
            else:
                if asyncio.iscoroutinefunction(selected_tool):
                    raw_result = await selected_tool(**tool_args)
                else:
                    raw_result = selected_tool(**tool_args)
            
            # è™•ç†å·¥å…·çµæžœ
            if isinstance(raw_result, ToolExecutionResult):
                tool_execution_result = raw_result
            else:
                # å¦‚æžœæ˜¯å­—ä¸²ï¼Œå˜—è©¦è§£æžç‚º ToolExecutionResult
                try:
                    import json
                    result_data = json.loads(raw_result)
                    tool_execution_result = ToolExecutionResult(**result_data)
                except (json.JSONDecodeError, TypeError, ValueError):
                    # å¦‚æžœè§£æžå¤±æ•—ï¼Œå‰µå»ºä¸€å€‹ç°¡å–®çš„æˆåŠŸçµæžœ
                    tool_execution_result = ToolExecutionResult(
                        success=True,
                        message=str(raw_result)
                    )
            print(tool_execution_result)
            # å‰µå»º ToolMessage
            tool_message = ToolMessage(
                content=tool_execution_result.message,
                tool_call_id=tool_call_id
            )
            
            # Phase3: å·¥å…·ç‹€æ…‹ - completed
            try:
                await self._notify_progress(
                    stage=ProgressStage.TOOL_STATUS,
                    message="",  # ä½¿ç”¨é…ç½®è¨Šæ¯
                    progress_percentage=50,  # ä¿æŒé€²åº¦æ¢
                    tool=tool_name,
                    status=ToolStatus.COMPLETED
                )
            except Exception as ce:
                self.logger.warning(f"é€šçŸ¥å·¥å…· {tool_name} completed ç‹€æ…‹å¤±æ•—: {ce}")

            return tool_message, tool_execution_result
                
        except Exception as e:
            self.logger.error(f"åŸ·è¡Œå·¥å…· {tool_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            error_result = ToolExecutionResult(
                success=False,
                message=f"å·¥å…·åŸ·è¡ŒéŒ¯èª¤: {str(e)}"
            )
            tool_message = ToolMessage(
                content=error_result.message,
                tool_call_id=tool_call_id
            )
            # Phase3: å·¥å…·ç‹€æ…‹ - error
            try:
                await self._notify_progress(
                    stage=ProgressStage.TOOL_STATUS,
                    message="",  # ä½¿ç”¨é…ç½®è¨Šæ¯
                    progress_percentage=50,  # ä¿æŒé€²åº¦æ¢
                    tool=tool_name,
                    status=ToolStatus.ERROR
                )
            except Exception as ne:
                self.logger.warning(f"é€šçŸ¥å·¥å…· {tool_name} error ç‹€æ…‹å¤±æ•—: {ne}")
            return tool_message, error_result

    async def _process_reminder_result(self, state: OverallState, tool_execution_result: ToolExecutionResult):
        """å¾ž ToolExecutionResult è™•ç†æé†’å·¥å…·çš„åŸ·è¡Œçµæžœ"""
        try:
            if tool_execution_result.success and tool_execution_result.data:
                # æå– ReminderDetails
                reminder_data = tool_execution_result.data.get("reminder_details")
                if reminder_data:
                    # ç¢ºä¿åŒ…å« msg_id æ¬„ä½ï¼Œå¦‚æžœæ²’æœ‰å‰‡è¨­ç‚ºç©ºå­—ä¸²
                    logging.debug(f"reminder_data: {reminder_data}")
                    if "msg_id" not in reminder_data:
                        reminder_data["msg_id"] = ""
                    reminder_details = ReminderDetails(**reminder_data)
                    # æ·»åŠ åˆ° state çš„ reminder_requests
                    if not hasattr(state, "reminder_requests") or state.reminder_requests is None:
                        state.reminder_requests = []
                    state.reminder_requests.append(reminder_details)
                    
                    self.logger.info(f"æˆåŠŸè™•ç†æé†’è«‹æ±‚: {reminder_details.message}")
            return {"reminder_requests": state.reminder_requests}
                    
        except (KeyError, TypeError) as e:
            self.logger.error(f"è™•ç†æé†’çµæžœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {}

    def _deduplicate_results(self, results: List[str]) -> List[str]:
        """åŽ»é‡å’ŒæŽ’åºçµæžœ"""
        if not results:
            return []
        
        # ç°¡å–®åŽ»é‡
        seen = set()
        unique_results = []
        for result in results:
            if result and result not in seen:
                seen.add(result)
                unique_results.append(result)
        
        return unique_results

    async def reflection(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šåæ€
        
        è©•ä¼°å·¥å…·çµæžœçš„è³ªé‡å’Œå®Œæ•´æ€§ã€‚
        """
        try:
            if not self.behavior_config.enable_reflection:
                # å¦‚æžœç¦ç”¨åæ€ï¼Œç›´æŽ¥èªç‚ºçµæžœå……åˆ†
                return {"is_sufficient": True}
            
            self.logger.info("reflection: é–‹å§‹åæ€å·¥å…·çµæžœ")
            
            # é€šçŸ¥åæ€éšŽæ®µ
            await self._notify_progress(
                stage=ProgressStage.REFLECTION,
                message="",  # ä½¿ç”¨é…ç½®ä¸­çš„è¨Šæ¯
                progress_percentage=75,
                current_state=state
            )
            
            # tool_results will be accumulated from parallel execute_single_tool calls
            raw_tool_results = state.tool_results or []
            unique_tool_results = self._deduplicate_results(raw_tool_results) # Deduplicate here
            research_topic = state.research_topic
            
            # ä½¿ç”¨å°ˆç”¨çš„åæ€ LLM æˆ–ç°¡åŒ–é‚è¼¯
            is_sufficient = self._evaluate_results_sufficiency(unique_tool_results, research_topic)
            
            self.logger.info(f"reflection: çµæžœå……åˆ†åº¦={is_sufficient}")
            
            return {
                "is_sufficient": is_sufficient,
                "reflection_complete": True,
                "reflection_reasoning": f"åŸºæ–¼ {len(unique_tool_results)} å€‹çµæžœçš„è©•ä¼°",
                "aggregated_tool_results": unique_tool_results # Update for next stages
            }
            
        except Exception as e:
            self.logger.error(f"reflection å¤±æ•—: {e}")
            return {"is_sufficient": True}  # å¤±æ•—æ™‚å‡è¨­å……åˆ†

    def decide_next_step(self, state: OverallState) -> Literal["continue", "finish"]:
        """æ±ºå®šä¸‹ä¸€æ­¥çš„è·¯ç”±å‡½æ•¸"""
        try:
            current_round = state.tool_round
            max_rounds = self.behavior_config.max_tool_rounds
            is_sufficient = state.is_sufficient
            
            # æ±ºç­–é‚è¼¯
            if is_sufficient or current_round >= max_rounds:
                self.logger.info(f"æ±ºå®šå®Œæˆç ”ç©¶ (è¼ªæ¬¡={current_round}/{max_rounds}, å……åˆ†={is_sufficient})")
                return "finish"
            else:
                self.logger.info(f"æ±ºå®šç¹¼çºŒç ”ç©¶ (è¼ªæ¬¡={current_round}/{max_rounds})")
                return "continue"
                
        except Exception as e:
            self.logger.error(f"decide_next_step å¤±æ•—: {e}")
            return "finish"

    def _build_planning_system_prompt(self, messages_global_metadata: str = "") -> str:
        """æ§‹å»ºæœ€çµ‚ç­”æ¡ˆçš„ç³»çµ±æç¤ºè©ž
        
        Args:
            context: å·¥å…·çµæžœä¸Šä¸‹æ–‡
            messages_global_metadata: å…¨åŸŸè¨Šæ¯å…ƒæ•¸æ“š
            
        Returns:
            str: å®Œæ•´çš„ç³»çµ±æç¤ºè©ž
        """
        # ä½¿ç”¨ PromptSystem æ§‹å»ºåŸºç¤Žç³»çµ±æç¤ºè©ž
        
        context_prompt = self.prompt_system.get_tool_prompt(
            "planning_instructions"
        )
        system_prompt = self.prompt_system.get_system_instructions(
            config=self.config,
            messages_global_metadata=context_prompt + "\n\n" + messages_global_metadata
        )
        return system_prompt
            

    def _build_final_system_prompt(self, context: str, messages_global_metadata: str) -> str:
        """æ§‹å»ºæœ€çµ‚ç­”æ¡ˆçš„ç³»çµ±æç¤ºè©ž
        
        Args:
            context: å·¥å…·çµæžœä¸Šä¸‹æ–‡
            messages_global_metadata: å…¨åŸŸè¨Šæ¯å…ƒæ•¸æ“š
            
        Returns:
            str: å®Œæ•´çš„ç³»çµ±æç¤ºè©ž
        """
        try:
            # ä½¿ç”¨ PromptSystem æ§‹å»ºåŸºç¤Žç³»çµ±æç¤ºè©ž
            base_system_prompt = self.prompt_system.get_system_instructions(
                config=self.config,
                messages_global_metadata=messages_global_metadata
            )
            
            # æ·»åŠ ä¸Šä¸‹æ–‡è³‡è¨Šï¼ˆå¦‚æžœæœ‰çš„è©±ï¼‰
            if context:
                try:
                    context_prompt = self.prompt_system.get_final_answer_context(
                        context=context
                    )
                    full_system_prompt = base_system_prompt + "\n\n" + context_prompt
                except Exception as e:
                    self.logger.warning(f"è®€å–æœ€çµ‚ç­”æ¡ˆä¸Šä¸‹æ–‡æç¤ºè©žå¤±æ•—: {e}")
                    context_prompt = f"ä»¥ä¸‹æ˜¯ç›¸é—œè³‡è¨Šï¼š\n{context}\nè«‹åŸºæ–¼ä»¥ä¸Šè³‡è¨Šå›žç­”ã€‚"
                    full_system_prompt = base_system_prompt + "\n\n" + context_prompt
            else:
                full_system_prompt = base_system_prompt
            
            return full_system_prompt
            
        except Exception as e:
            self.logger.error(f"æ§‹å»ºæœ€çµ‚ç³»çµ±æç¤ºè©žå¤±æ•—: {e}")
            # å›žé€€åˆ°ç°¡å–®çš„ç³»çµ±æç¤ºè©ž
            fallback_prompt = f"ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„è³‡è¨Šå›žç­”ç”¨æˆ¶çš„å•é¡Œã€‚ç”±æ–¼æµç¨‹å‡ºç¾éŒ¯èª¤ï¼Œè«‹æé†’ä¸€ä¸‹ç”¨æˆ¶ {e}ã€‚"
            if context:
                fallback_prompt += f"\n\nä»¥ä¸‹æ˜¯ç›¸é—œè³‡è¨Šï¼š\n{context}\nè«‹åŸºæ–¼ä»¥ä¸Šè³‡è¨Šå›žç­”ã€‚"
            return fallback_prompt

    def _build_messages_for_llm(self, messages: List[MsgNode], system_prompt: str) -> List:
        """æ§‹å»ºç”¨æ–¼ LLM çš„è¨Šæ¯åˆ—è¡¨
        
        Args:
            messages: åŽŸå§‹è¨Šæ¯åˆ—è¡¨
            system_prompt: ç³»çµ±æç¤ºè©ž
            
        Returns:
            List: LangChain æ ¼å¼çš„è¨Šæ¯åˆ—è¡¨
        """
        try:
            # æ§‹å»ºè¨Šæ¯åˆ—è¡¨
            messages_for_llm = [SystemMessage(content=system_prompt)]
            
            for msg in messages:
                if msg.role == "user":
                    messages_for_llm.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    # æª¢æŸ¥æ˜¯å¦æœ‰ tool_calls
                    tool_calls = msg.metadata.get("tool_calls") if msg.metadata else None
                    if tool_calls:
                        ai_msg = AIMessage(content=msg.content, tool_calls=tool_calls)
                    else:
                        ai_msg = AIMessage(content=msg.content)
                    messages_for_llm.append(ai_msg)
                elif msg.role == "tool":
                    # ä½¿ç”¨æ­£ç¢ºçš„ ToolMessage æ ¼å¼
                    tool_call_id = msg.metadata.get("tool_call_id") if msg.metadata else None
                    if tool_call_id:
                        messages_for_llm.append(ToolMessage(
                            content=msg.content,
                            tool_call_id=str(tool_call_id)
                        ))
                    else:
                        # å¦‚æžœæ²’æœ‰ tool_call_idï¼Œè¨˜éŒ„éŒ¯èª¤ä½†ä¸ä¸­æ–·
                        self.logger.warning(f"Tool message without tool_call_id: {msg.content}")
                        messages_for_llm.append(HumanMessage(content="Tool Result: " + msg.content))
            
            return messages_for_llm
            
        except Exception as e:
            self.logger.error(f"æ§‹å»º LLM è¨Šæ¯åˆ—è¡¨å¤±æ•—: {e}")
            # å›žé€€åˆ°ç°¡å–®çš„è¨Šæ¯æ ¼å¼
            fallback_content = _extract_text_content(messages[-1].content) if messages else "è«‹å›žç­”ç”¨æˆ¶çš„å•é¡Œã€‚"
            return [
                SystemMessage(content=system_prompt),
                HumanMessage(content=fallback_content)
            ]

    async def finalize_answer(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šç”Ÿæˆæœ€çµ‚ç­”æ¡ˆï¼ˆæ”¯æ´ä¸²æµï¼‰
        
        åŸºæ–¼æ‰€æœ‰å¯ç”¨çš„ä¿¡æ¯ç”Ÿæˆæœ€çµ‚å›žç­”ï¼Œç‰¹åˆ¥è™•ç†æé†’ç›¸é—œçš„å›žè¦†ã€‚
        """
        try:
            self.logger.info("finalize_answer: ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
            
            # é€šçŸ¥ç­”æ¡ˆç”ŸæˆéšŽæ®µ
            # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„æé†’è«‹æ±‚
            if state.reminder_requests:
                self.logger.info("finalize_answer: ç¢ºèªæé†’å®Œæˆ")
                
                # é€šçŸ¥å®Œæˆ
                await self._notify_progress(
                    stage=ProgressStage.COMPLETED,
                    message="âœ… æé†’è¨­å®šå®Œæˆï¼",
                    progress_percentage=90,
                    current_state=state
                )
            
            await self._notify_progress(
                stage=ProgressStage.FINALIZE_ANSWER,
                message="",  # ä½¿ç”¨é…ç½®ä¸­çš„è¨Šæ¯
                progress_percentage=90,
                current_state=state
            )
            
            messages = state.messages
            tool_results = state.aggregated_tool_results or state.tool_results or []
            
            # æ§‹å»ºä¸Šä¸‹æ–‡
            context = ""
            if tool_results:
                context = "\n".join([f"æœå°‹çµæžœ: {result}" for result in tool_results])
            
            # æ§‹å»ºç³»çµ±æç¤ºè©žå’Œè¨Šæ¯åˆ—è¡¨
            system_prompt = self._build_final_system_prompt(context, state.messages_global_metadata)
            messages_for_llm = self._build_messages_for_llm(messages, system_prompt)
            
            self._log_messages(messages_for_llm, "messages_for_llm (finalize_answer)")
            
            # æª¢æŸ¥ä¸²æµé…ç½®

            final_answer = ""
            
            if self.config.streaming.enabled:
                self.logger.info("finalize_answer: å•Ÿç”¨ä¸²æµå›žæ‡‰")
                
                try:
                    # ä½¿ç”¨ LLM ä¸²æµç”Ÿæˆç­”æ¡ˆ
                    final_answer_chunks = []
                    async for chunk in self.final_answer_llm.astream(messages_for_llm):
                        content = chunk.content or ""
                        if content:  # åªè™•ç†æœ‰å…§å®¹çš„ chunk
                            final_answer_chunks.append(content)
                            # é€šçŸ¥ä¸²æµå¡Šï¼Œæœ€å¾Œä¸€å€‹ chunk é€šå¸¸å…§å®¹ç‚ºç©ºï¼Œç”¨ä¾†æ¨™ç¤ºçµæŸ
                            is_final = len(content) == 0
                            await self._notify_streaming_chunk(content, is_final=is_final)
                    
                    # çµ„åˆå®Œæ•´ç­”æ¡ˆ
                    final_answer = "".join(final_answer_chunks)
                    
                    # é€šçŸ¥ä¸²æµå®Œæˆ
                    await self._notify_streaming_complete()
                    
                except Exception as e:
                    self.logger.warning(f"ä¸²æµç”Ÿæˆå¤±æ•—ï¼Œå›žé€€åˆ°åŒæ­¥æ¨¡å¼: {e}")
                    # å›žé€€åˆ°åŒæ­¥æ¨¡å¼
                    final_answer = self.final_answer_llm.invoke(messages_for_llm).content
                    
            else:
                self.logger.info("finalize_answer: æœªå•Ÿç”¨ä¸²æµæˆ–ç„¡è§€å¯Ÿè€…ï¼Œä½¿ç”¨ä¸€èˆ¬å›žæ‡‰")
                # ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼ˆéžä¸²æµï¼‰
                try:
                    # ç›´æŽ¥ä½¿ç”¨ LLM invoke
                    response = self.final_answer_llm.invoke(messages_for_llm)
                    final_answer = response.content
                except Exception as e:
                    self.logger.warning(f"LLM ç­”æ¡ˆç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨åŸºæœ¬å›žè¦†: {e}")
                    final_answer = self._generate_basic_fallback_answer(messages, context)
                
                # é€šçŸ¥å®Œæˆï¼ˆéžä¸²æµæ¨¡å¼ï¼‰
                await self._notify_progress(
                    stage=ProgressStage.COMPLETED,
                    message="âœ… å›žç­”å®Œæˆï¼",
                    progress_percentage=100,
                    current_state=state
                )
            
            if state.reminder_requests:
                final_answer = final_answer + "\n" + "âœ… æé†’è¨­å®šå®Œæˆï¼"
                
            if final_answer == "":
                final_answer = "âœ… å›žç­”å®Œæˆï¼ (Agent ç„¡è¨€äº†)"
                
            self.logger.info("finalize_answer: ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            return {
                "final_answer": final_answer,
                "finished": True,
                "sources": self._extract_sources_from_results(tool_results)
            }
            
        except Exception as e:
            self.logger.error(f"finalize_answer å¤±æ•—: {e}")
            
            # é€šçŸ¥éŒ¯èª¤
            await self._notify_error(e)
            
            return {
                "final_answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›žç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚", 
                "finished": True
            }

    def _analyze_tool_necessity_fallback(self, messages: List[MsgNode]) -> bool:
        """é—œéµå­—æª¢æ¸¬å›žé€€æ–¹æ¡ˆ (ä»åŸºæ–¼æœ€æ–°è¨Šæ¯)"""
        tool_keywords = [
            "æœå°‹", "æŸ¥è©¢", "æœ€æ–°", "ç¾åœ¨", "ä»Šå¤©", "ä»Šå¹´", "æŸ¥æ‰¾",
            "è³‡è¨Š", "è³‡æ–™", "æ–°èž", "æ¶ˆæ¯", "æ›´æ–°", "ç‹€æ³", "search",
            "find", "latest", "current", "recent", "what is", "å‘Šè¨´æˆ‘",
            "ç¶²è·¯æœå°‹", "ç¶²è·¯ç ”ç©¶"
        ]
        
        # å®‰å…¨åœ°æå–æ–‡å­—å…§å®¹
        user_text = _extract_text_content(messages[-1].content)
        content_lower = user_text.lower()
        return any(keyword in content_lower for keyword in tool_keywords)

    def _evaluate_results_sufficiency(self, tool_results: List[str], research_topic: str) -> bool:
        """è©•ä¼°çµæžœæ˜¯å¦å……åˆ†"""
        # é‡æ§‹å¾Œç°¡åŒ–é‚è¼¯ï¼šç¸½æ˜¯èªç‚ºçµæžœå……åˆ†ï¼Œé¿å…ç„¡é™å¾ªç’°
        # å¯¦éš›çš„è¼ªæ¬¡æŽ§åˆ¶ç”± max_tool_rounds ä¾†è™•ç†
        return True

    def _generate_basic_fallback_answer(self, messages: List[MsgNode], context: str) -> str:
        return "å‡ºç¾éŒ¯èª¤ï¼Œè«‹å†è©¦ä¸€æ¬¡ ðŸ”„"

    def _extract_sources_from_results(self, tool_results: List[str]) -> List[Dict[str, str]]:
        """å¾žå·¥å…·çµæžœä¸­æå–ä¾†æºä¿¡æ¯"""
        sources = []
        if tool_results:
            for result in tool_results:
                if isinstance(result, str) and "http" in result:
                    # ç°¡åŒ–çš„ä¾†æºæå–é‚è¼¯
                    sources.append({
                        "title": result[:100] + "..." if len(result) > 100 else result,
                        "url": "",
                        "snippet": result
                    })
        return sources

    def _log_messages(self, messages_for_llm: List[Any], context_message: str = "messages_for_llm"):
        """Helper function to log messages, truncating image data."""
        if not messages_for_llm or len(messages_for_llm) < 2:
            logging.debug(f"{context_message} (raw): {messages_for_llm}")
            return
        try:
            # The first message is always SystemMessage, which we don't usually need to log repeatedly.
            loggable_messages = copy.deepcopy(messages_for_llm)
            for msg in loggable_messages:
                # HumanMessage content can be a list of parts
                if isinstance(msg.content, list):
                    for part in msg.content:
                        if isinstance(part, dict) and part.get("type") == "image_url":
                            image_url_content = part.get("image_url")
                            if isinstance(image_url_content, dict):
                                url = image_url_content.get("url", "")
                                if isinstance(url, str) and url.startswith("data:image"):
                                    image_url_content["url"] = f"{url[:50]}...[TRUNCATED]"
                            elif isinstance(image_url_content, str) and image_url_content.startswith("data:image"):
                                part["image_url"] = f"{image_url_content[:50]}...[TRUNCATED]"

            # Log all messages starting from the second one (skip system prompt)
            logging.debug(f"{context_message}: {loggable_messages[1:]}")
        except Exception as e:
            logging.warning(f"Could not format messages for logging: {e}")
            # Log raw messages if formatting fails, also skipping system prompt
            logging.debug(f"{context_message} (raw): {messages_for_llm[1:]}")


def create_unified_agent(config: Optional[AppConfig] = None) -> UnifiedAgent:
    """å»ºç«‹çµ±ä¸€ Agent å¯¦ä¾‹"""
    return UnifiedAgent(config)


def create_agent_graph(config: Optional[AppConfig] = None) -> StateGraph:
    """å»ºç«‹ä¸¦ç·¨è­¯ Agent åœ–"""
    agent = create_unified_agent(config)
    return agent.build_graph() 