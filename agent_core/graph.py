"""
çµ±ä¸€çš„ LangGraph åœ–å¯¦ä½œ

æ ¹æ“šé…ç½®å‹•æ…‹æ§‹å»º LangGraph çš„ StateGraphï¼Œå®šç¾©æ‰€æœ‰ç¯€é»žçš„å¯¦ç¾ï¼Œ
åŒ…æ‹¬å·¥å…·é‚è¼¯çš„ç›´æŽ¥å…§åµŒã€‚åš´æ ¼åƒè€ƒ reference_arch.md ä¸­å®šç¾©çš„ Agent æ ¸å¿ƒæµç¨‹ã€‚
"""

import asyncio
import logging
import json
from typing import Dict, Any, List, Optional, Literal
from datetime import datetime

from langchain_core.messages import AIMessage, HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langchain_core.runnables import RunnableConfig
from google.genai import Client

from schemas.agent_types import OverallState, MsgNode, DiscordProgressUpdate
from utils.config_loader import load_config
from agents.prompts import web_searcher_instructions, get_current_date
from agents.utils import resolve_urls, get_citations, insert_citation_markers


class UnifiedAgent:
    """çµ±ä¸€çš„ Agent å¯¦ä½œï¼Œæ ¹æ“šé…ç½®å‹•æ…‹èª¿æ•´è¡Œç‚º"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–çµ±ä¸€ Agent
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¦‚æžœç‚º None å‰‡è¼‰å…¥é è¨­é…ç½®
        """
        self.config = config or load_config()
        self.logger = logging.getLogger(__name__)
        
        # å¾žé…ç½®ç²å– agent è¨­å®š
        self.agent_config = self.config.get("agent", {})
        self.tools_config = self.agent_config.get("tools", {})
        self.behavior_config = self.agent_config.get("behavior", {})
        
        # åˆå§‹åŒ–å¤šå€‹ LLM å¯¦ä¾‹
        self.llm_instances = self._initialize_llm_instances()
        
        # åˆå§‹åŒ– Google å®¢æˆ¶ç«¯ï¼ˆå¦‚æžœå·¥å…·å•Ÿç”¨ï¼‰
        self.google_client = None
        if self.tools_config.get("google_search", {}).get("enabled", False):
            api_key = self.config.get("gemini_api_key")
            if api_key:
                self.google_client = Client(api_key=api_key)
    
    def _initialize_llm_instances(self) -> Dict[str, Optional[ChatGoogleGenerativeAI]]:
        """åˆå§‹åŒ–ä¸åŒç”¨é€”çš„ LLM å¯¦ä¾‹"""
        api_key = self.config.get("gemini_api_key")
        if not api_key:
            return {
                "tool_analysis": None,
                "final_answer": None,
                "reflection": None
            }
        
        # LLM é…ç½®
        llm_configs = self.config.get("llm_models", {
            "tool_analysis": {
                "model": "gemini-2.0-flash-exp",
                "temperature": 0.1
            },
            "final_answer": {
                "model": "gemini-2.0-flash-exp", 
                "temperature": 0.7
            },
            "reflection": {
                "model": "gemini-2.0-flash-exp",
                "temperature": 0.3
            }
        })
        
        llm_instances = {}
        for purpose, config in llm_configs.items():
            try:
                llm_instances[purpose] = ChatGoogleGenerativeAI(
                    model=config.get("model", "gemini-2.0-flash-exp"),
                    temperature=config.get("temperature", 0.5),
                    api_key=api_key
                )
                self.logger.info(f"åˆå§‹åŒ– {purpose} LLM: {config.get('model')}")
            except Exception as e:
                self.logger.warning(f"åˆå§‹åŒ– {purpose} LLM å¤±æ•—: {e}")
                llm_instances[purpose] = None
        
        return llm_instances
    
    def build_graph(self) -> StateGraph:
        """å»ºç«‹ä¸¦ç·¨è­¯ LangGraph"""
        builder = StateGraph(OverallState)
        
        # æ·»åŠ æ‰€æœ‰ç¯€é»ž
        builder.add_node("generate_query_or_plan", self.generate_query_or_plan)
        builder.add_node("tool_selection", self.tool_selection)
        builder.add_node("execute_tool", self.execute_tool)
        builder.add_node("reflection", self.reflection)
        builder.add_node("evaluate_research", self.evaluate_research)
        builder.add_node("finalize_answer", self.finalize_answer)
        
        # è¨­ç½®æµç¨‹é‚Šç·£
        builder.add_edge(START, "generate_query_or_plan")
        
        # æ ¹æ“šé…ç½®æ±ºå®šæµç¨‹
        max_tool_rounds = self.behavior_config.get("max_tool_rounds", 0)
        
        if max_tool_rounds == 0:
            # ç´”å°è©±æ¨¡å¼ï¼šç›´æŽ¥åˆ°æœ€çµ‚ç­”æ¡ˆ
            builder.add_edge("generate_query_or_plan", "finalize_answer")
        else:
            # å·¥å…·æ¨¡å¼ï¼šå®Œæ•´æµç¨‹
            builder.add_edge("generate_query_or_plan", "tool_selection")
            builder.add_edge("tool_selection", "execute_tool")
            builder.add_edge("execute_tool", "reflection")
            
            builder.add_conditional_edges(
                "reflection",
                self.decide_next_step,
                {
                    "continue": "tool_selection",
                    "finish": "finalize_answer"
                }
            )
        
        builder.add_edge("finalize_answer", END)
        
        return builder.compile()
    
    def generate_query_or_plan(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šç”ŸæˆæŸ¥è©¢æˆ–åˆæ­¥è¦åŠƒ
        
        åˆ†æžç”¨æˆ¶è«‹æ±‚ä¸¦æ±ºå®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œç”Ÿæˆåˆæ­¥çš„è¡Œå‹•è¨ˆåŠƒã€‚
        """
        try:
            self.logger.info("generate_query_or_plan: é–‹å§‹åˆ†æžç”¨æˆ¶è«‹æ±‚")
            
            # ç²å–æœ€æ–°çš„ç”¨æˆ¶è¨Šæ¯
            if not state.messages:
                return {"finished": True}
            
            latest_message = state.messages[-1] if state.messages else None
            if not latest_message or latest_message.role != "user":
                return {"finished": True}
            
            user_content = latest_message.content
            max_tool_rounds = self.behavior_config.get("max_tool_rounds", 0)
            
            # æ±ºå®šæ˜¯å¦éœ€è¦å·¥å…·
            needs_tools = False
            if max_tool_rounds > 0:
                needs_tools = self._analyze_tool_necessity(state.messages)
            
            # ç”ŸæˆæŸ¥è©¢æˆ–è¨ˆåŠƒ
            queries = []
            if needs_tools and self.google_client:
                queries = self._generate_search_queries(state.messages)
            
            self.logger.info(f"generate_query_or_plan: éœ€è¦å·¥å…·={needs_tools}, æŸ¥è©¢æ•¸é‡={len(queries)}")
            
            return {
                "tool_round": 0,
                "needs_tools": needs_tools,
                "search_queries": queries,
                "research_topic": user_content[:200],  # æˆªå–å‰200å­—ç¬¦ä½œç‚ºç ”ç©¶ä¸»é¡Œ
            }
            
        except Exception as e:
            self.logger.error(f"generate_query_or_plan å¤±æ•—: {e}")
            return {"finished": True}
    
    def tool_selection(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šå·¥å…·é¸æ“‡
        
        æ ¹æ“šç•¶å‰ç‹€æ…‹å’Œé…ç½®æ±ºå®šä½¿ç”¨å“ªäº›å·¥å…·ã€‚
        """
        try:
            self.logger.info("tool_selection: é¸æ“‡é©ç•¶çš„å·¥å…·")
            
            available_tools = []
            
            # æ ¹æ“šå„ªå…ˆç´šæŽ’åºå·¥å…·
            tools_by_priority = sorted(
                self.tools_config.items(),
                key=lambda x: x[1].get("priority", 999)
            )
            
            for tool_name, tool_config in tools_by_priority:
                if tool_config.get("enabled", False):
                    # æª¢æŸ¥å·¥å…·æ˜¯å¦çœŸçš„å¯ç”¨
                    if tool_name == "google_search" and self.google_client:
                        available_tools.append(tool_name)
                    elif tool_name != "google_search":
                        available_tools.append(tool_name)
            
            # æ±ºå®šä½¿ç”¨çš„å·¥å…·
            selected_tool = None
            if state.needs_tools and available_tools:
                # ç°¡å–®ç­–ç•¥ï¼šé¸æ“‡ç¬¬ä¸€å€‹å¯ç”¨çš„å·¥å…·
                selected_tool = available_tools[0]
            
            self.logger.info(f"tool_selection: å¯ç”¨å·¥å…·={available_tools}, é¸æ“‡å·¥å…·={selected_tool}")
            
            return {
                "available_tools": available_tools,
                "selected_tool": selected_tool
            }
            
        except Exception as e:
            self.logger.error(f"tool_selection å¤±æ•—: {e}")
            return {"selected_tool": None}
    
    def execute_tool(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šåŸ·è¡Œå·¥å…·
        
        åŸ·è¡Œé¸å®šçš„å·¥å…·ï¼Œå…§åµŒå·¥å…·é‚è¼¯å¦‚ Google Searchã€‚
        """
        try:
            selected_tool = state.selected_tool
            
            if not selected_tool:
                self.logger.info("execute_tool: æ²’æœ‰é¸æ“‡å·¥å…·ï¼Œè·³éŽ")
                return {
                    "tool_results": [], 
                    "tool_round": state.tool_round + 1
                }
            
            self.logger.info(f"execute_tool: åŸ·è¡Œå·¥å…· {selected_tool}")
            
            tool_results = []
            
            if selected_tool == "google_search":
                # å…§åµŒ Google Search é‚è¼¯
                tool_results = self._execute_google_search(state)
            elif selected_tool == "citation":
                # å…§åµŒå¼•ç”¨è™•ç†é‚è¼¯
                tool_results = self._execute_citation_tool(state)
            
            # æ›´æ–°å·¥å…·ä½¿ç”¨è¼ªæ¬¡
            current_round = state.tool_round + 1
            
            self.logger.info(f"execute_tool: å®Œæˆï¼Œçµæžœæ•¸é‡={len(tool_results)}, è¼ªæ¬¡={current_round}")
            self.logger.info(f"execute_tool: çµæžœ={tool_results}")
            
            
            return {
                "tool_results": tool_results,
                "tool_round": current_round
            }
            
        except Exception as e:
            self.logger.error(f"execute_tool å¤±æ•—: {e}")
            return {"tool_results": [], "tool_round": state.tool_round + 1}
    
    def reflection(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šåæ€
        
        è©•ä¼°å·¥å…·çµæžœçš„è³ªé‡å’Œå®Œæ•´æ€§ã€‚
        """
        try:
            if not self.behavior_config.get("enable_reflection", True):
                # å¦‚æžœç¦ç”¨åæ€ï¼Œç›´æŽ¥èªç‚ºçµæžœå……åˆ†
                return {"is_sufficient": True}
            
            self.logger.info("reflection: é–‹å§‹åæ€å·¥å…·çµæžœ")
            
            tool_results = state.tool_results
            research_topic = state.research_topic
            
            # ä½¿ç”¨å°ˆç”¨çš„åæ€ LLM æˆ–ç°¡åŒ–é‚è¼¯
            is_sufficient = self._evaluate_results_sufficiency(tool_results, research_topic)
            
            self.logger.info(f"reflection: çµæžœå……åˆ†åº¦={is_sufficient}")
            
            return {
                "is_sufficient": is_sufficient,
                "reflection_complete": True
            }
            
        except Exception as e:
            self.logger.error(f"reflection å¤±æ•—: {e}")
            return {"is_sufficient": True}  # å¤±æ•—æ™‚å‡è¨­å……åˆ†
    
    def evaluate_research(self, state: OverallState) -> str:
        """
        LangGraph è·¯ç”±ç¯€é»žï¼šè©•ä¼°ç ”ç©¶é€²åº¦
        
        æ±ºå®šæ˜¯å¦ç¹¼çºŒç ”ç©¶æˆ–é€²å…¥æœ€çµ‚ç­”æ¡ˆç”Ÿæˆã€‚
        """
        try:
            current_round = state.tool_round
            max_rounds = self.behavior_config.get("max_tool_rounds", 1)
            is_sufficient = state.is_sufficient
            
            # æ±ºç­–é‚è¼¯
            if is_sufficient or current_round >= max_rounds:
                self.logger.info(f"evaluate_research: å®Œæˆç ”ç©¶ (è¼ªæ¬¡={current_round}, å……åˆ†={is_sufficient})")
                return "finish"
            else:
                self.logger.info(f"evaluate_research: ç¹¼çºŒç ”ç©¶ (è¼ªæ¬¡={current_round}/{max_rounds})")
                return "continue"
                
        except Exception as e:
            self.logger.error(f"evaluate_research å¤±æ•—: {e}")
            return "finish"
    
    def decide_next_step(self, state: OverallState) -> Literal["continue", "finish"]:
        """æ±ºå®šä¸‹ä¸€æ­¥çš„è·¯ç”±å‡½æ•¸"""
        return self.evaluate_research(state)
    
    def finalize_answer(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
        
        æ•´åˆæ‰€æœ‰ä¿¡æ¯ä¸¦ç”Ÿæˆæœ€çµ‚å›žè¦†ã€‚
        """
        try:
            self.logger.info("finalize_answer: ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
            
            # ç²å–åŸºç¤Žä¿¡æ¯
            messages = state.messages
            tool_results = state.tool_results
            research_topic = state.research_topic
            
            if not messages:
                return {"finished": True}
            
            # æº–å‚™ç­”æ¡ˆç”Ÿæˆçš„ä¸Šä¸‹æ–‡
            context_parts = []
            
            # æ·»åŠ å·¥å…·çµæžœä½œç‚ºä¸Šä¸‹æ–‡
            if tool_results:
                context_parts.append("ç ”ç©¶çµæžœ:")
                for i, result in enumerate(tool_results[:5], 1):  # é™åˆ¶æœ€å¤š5å€‹çµæžœ
                    context_parts.append(f"{i}. {result}")
            
            context = "\n".join(context_parts) if context_parts else ""
            
            # ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
            final_answer = self._generate_final_answer(messages, context)
            
            self.logger.info("finalize_answer: ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            
            return {
                "final_answer": final_answer,
                "finished": True
            }
            
        except Exception as e:
            self.logger.error(f"finalize_answer å¤±æ•—: {e}")
            return {
                "final_answer": "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚",
                "finished": True
            }
    
    # å…§éƒ¨è¼”åŠ©æ–¹æ³•
    
    def _analyze_tool_necessity(self, messages: List[MsgNode]) -> bool:
        """åˆ†æžæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼ˆä½¿ç”¨å°ˆç”¨ LLM æ™ºèƒ½åˆ¤æ–·ï¼‰"""
        tool_analysis_llm = self.llm_instances.get("tool_analysis")
        if not tool_analysis_llm:
            return self._analyze_tool_necessity_fallback(messages)
        
        try:
            # æ§‹å»ºåŒ…å«æ‰€æœ‰æ­·å²å°è©±çš„æç¤º
            conversation_history = ""
            for msg in messages:
                if msg.role == "user":
                    conversation_history += f"ç”¨æˆ¶: {msg.content}\n"
                elif msg.role == "assistant":
                    conversation_history += f"åˆè¯: {msg.content}\n"

            analysis_prompt = f"""
è«‹åˆ†æžä»¥ä¸‹å°è©±æ­·å²ï¼Œåˆ¤æ–·æœ€æ–°ä¸€æ¢ç”¨æˆ¶è«‹æ±‚æ˜¯å¦éœ€è¦ä½¿ç”¨æœå°‹å·¥å…·ä¾†ç²å–æœ€æ–°è³‡è¨Šï¼š

å°è©±æ­·å²ï¼š
{conversation_history}

åˆ¤æ–·æ¨™æº–ï¼š
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚éœ€è¦æœ€æ–°è³‡è¨Šã€å³æ™‚æ•¸æ“šã€æ–°èžäº‹ä»¶
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚éœ€è¦æŸ¥æ‰¾ç‰¹å®šäº‹å¯¦ã€æ•¸æ“šã€çµ±è¨ˆ
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚æ¶‰åŠç•¶å‰ç‹€æ³ã€æœ€æ–°ç™¼å±•
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚éœ€è¦é©—è­‰æˆ–å¼•ç”¨å¤–éƒ¨è³‡æº
- æœ€æ–°ç”¨æˆ¶è«‹æ±‚æ˜¯ä¹‹å‰å•é¡Œçš„å»¶ä¼¸æˆ–è£œå……ï¼Œä¸”éœ€è¦æœå°‹ä¾†å›žç­”ã€‚

è«‹åªå›žç­”ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ï¼Œä¸éœ€è¦è§£é‡‹ã€‚
"""
            
            response = tool_analysis_llm.invoke(analysis_prompt)
            result = response.content.strip().lower()
            
            needs_tools = "æ˜¯" in result or "yes" in result or "éœ€è¦" in result
            
            self.logger.info(f"LLM å·¥å…·éœ€æ±‚åˆ¤æ–· (åŸºæ–¼æ­·å²å°è©±): '{messages[-1].content[:50]}...' -> {needs_tools}")
            return needs_tools
            
        except Exception as e:
            self.logger.warning(f"LLM å·¥å…·éœ€æ±‚åˆ¤æ–·å¤±æ•—ï¼Œå›žé€€åˆ°é—œéµå­—æª¢æ¸¬: {e}")
            return self._analyze_tool_necessity_fallback(messages)

    def _analyze_tool_necessity_fallback(self, messages: List[MsgNode]) -> bool:
        """é—œéµå­—æª¢æ¸¬å›žé€€æ–¹æ¡ˆ (ä»åŸºæ–¼æœ€æ–°è¨Šæ¯)"""
        tool_keywords = [
            "æœå°‹", "æŸ¥è©¢", "æœ€æ–°", "ç¾åœ¨", "ä»Šå¤©", "ä»Šå¹´", "æŸ¥æ‰¾",
            "è³‡è¨Š", "è³‡æ–™", "æ–°èž", "æ¶ˆæ¯", "æ›´æ–°", "ç‹€æ³", "search",
            "find", "latest", "current", "recent", "what is", "å‘Šè¨´æˆ‘"
        ]
        
        content_lower = messages[-1].content.lower()
        return any(keyword in content_lower for keyword in tool_keywords)
    
    def _generate_search_queries(self, messages: List[MsgNode]) -> List[str]:
        """ç”Ÿæˆæœå°‹æŸ¥è©¢ï¼ˆåŸºæ–¼å®Œæ•´å°è©±æ­·å²ï¼‰"""
        tool_analysis_llm = self.llm_instances.get("tool_analysis") # ä»ä½¿ç”¨æ­¤ LLM
        if not tool_analysis_llm:
            # å›žé€€åˆ°ç°¡åŒ–å¯¦ä½œï¼šç›´æŽ¥ä½¿ç”¨æœ€æ–°ç”¨æˆ¶è¼¸å…¥ä½œç‚ºæŸ¥è©¢
            return [messages[-1].content[:100]]
        
        try:
            # æ§‹å»ºåŒ…å«æ‰€æœ‰æ­·å²å°è©±çš„æç¤º
            conversation_history = ""
            for msg in messages:
                if msg.role == "user":
                    conversation_history += f"ç”¨æˆ¶: {msg.content}\n"
                elif msg.role == "assistant":
                    conversation_history += f"åˆè¯: {msg.content}\n"

            query_generation_prompt = f"""
æ ¹æ“šä»¥ä¸‹å°è©±æ­·å²ï¼Œç‚ºæœ€æ–°ä¸€æ¢ç”¨æˆ¶è«‹æ±‚ç”Ÿæˆ 1-2 å€‹ç²¾ç¢ºçš„ç¶²è·¯æœå°‹æŸ¥è©¢ã€‚è«‹ç¢ºä¿æŸ¥è©¢èƒ½æ¶µè“‹ç”¨æˆ¶è«‹æ±‚çš„æœ€æ–°è³‡è¨Šéœ€æ±‚ã€‚

å°è©±æ­·å²ï¼š
{conversation_history}

è¼¸å‡ºæ ¼å¼ï¼š
è«‹ç›´æŽ¥æä¾› JSON æ ¼å¼çš„æœå°‹æŸ¥è©¢åˆ—è¡¨ã€‚å¦‚æžœæ²’æœ‰éœ€è¦æœå°‹çš„æŸ¥è©¢ï¼Œè«‹å›žå‚³ä¸€å€‹ç©ºçš„ JSON åˆ—è¡¨ `[]`ã€‚

ç¯„ä¾‹ï¼š
[ "æŸ¥è©¢ä¸€", "æŸ¥è©¢äºŒ" ]
æˆ–è€…ï¼Œå¦‚æžœä¸éœ€è¦æŸ¥è©¢ï¼š
[]
"""
            
            response = tool_analysis_llm.invoke(query_generation_prompt)
            raw_content = response.content.strip()
            
            parsed_json = None
            if "```json" in raw_content:
                # Try to extract JSON from markdown code block
                start_marker = "```json"
                end_marker = "```"
                start_index = raw_content.find(start_marker)
                end_index = raw_content.rfind(end_marker)

                if start_index != -1 and end_index != -1 and end_index > start_index:
                    json_str = raw_content[start_index + len(start_marker):end_index].strip()
                    try:
                        if json_str:
                            parsed_json = json.loads(json_str)
                        else:
                            parsed_json = []
                    except json.JSONDecodeError:
                        self.logger.warning(f"ç„¡æ³•è§£æž Markdown ä¸­çš„ JSONã€‚å›žæ‡‰: {raw_content}")
            
            if parsed_json is None and raw_content.startswith("["):
                # Try to parse as direct JSON array
                try:
                    parsed_json = json.loads(raw_content)
                except json.JSONDecodeError:
                    self.logger.warning(f"ç„¡æ³•è§£æžç‚º JSON åˆ—è¡¨ã€‚å›žæ‡‰: {raw_content}")

            if parsed_json is None or not isinstance(parsed_json, list):
                self.logger.warning(f"LLM ç”Ÿæˆçš„æœå°‹æŸ¥è©¢å…§å®¹ç‚ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¢ºï¼Œå›žé€€åˆ°ç°¡åŒ–æ¨¡å¼ã€‚å›žæ‡‰: {raw_content}")
                return [messages[-1].content[:100]]
            
            queries = parsed_json

            self.logger.info(f"LLM ç”Ÿæˆæœå°‹æŸ¥è©¢: {queries}")
            return queries
            
        except Exception as e:
            self.logger.warning(f"LLM ç”Ÿæˆæœå°‹æŸ¥è©¢å¤±æ•—ï¼Œå›žé€€åˆ°ç°¡åŒ–æ¨¡å¼: {e}")
            return [messages[-1].content[:100]]
    
    def _execute_google_search(self, state: OverallState) -> List[str]:
        """åŸ·è¡Œ Google æœå°‹ï¼ˆå…§åµŒå¯¦ä½œï¼‰"""
        if not self.google_client:
            return ["Google å®¢æˆ¶ç«¯æœªé…ç½®ï¼Œç„¡æ³•åŸ·è¡Œ Google æœå°‹ã€‚"]
        
        search_queries = state.search_queries
        self.logger.debug(f"DEBUG: _execute_google_search: search_queries type: {type(search_queries)}, value: {search_queries}")
        results = []
        
        try:
            for query in search_queries[:2]:  # é™åˆ¶æœ€å¤š2å€‹æŸ¥è©¢
                current_date = get_current_date()
                
                # æº–å‚™å‚³éžçµ¦ Gemini æ¨¡åž‹çš„æç¤º
                formatted_prompt = web_searcher_instructions.format(
                    research_topic=query,
                    current_date=current_date
                )
                
                # ä½¿ç”¨å°ˆç”¨çš„å·¥å…·åˆ†æž LLM é€²è¡Œæœå°‹
                tool_llm = self.llm_instances.get("tool_analysis")
                if not tool_llm:
                    results.append("æœå°‹ LLM æœªé…ç½®ï¼Œç„¡æ³•åŸ·è¡Œ Google æœå°‹ã€‚")
                    continue

                try:
                    tool_llm_config = self.config.get("llm_models", {}).get("tool_analysis", {})
                    model_name = tool_llm_config.get("model", "gemini-2.0-flash-exp")
                    # èª¿ç”¨ Gemini API ä¸¦å•Ÿç”¨ google_search å·¥å…·
                    response = self.google_client.models.generate_content(
                        model=model_name,
                        contents=formatted_prompt,
                        config={
                            "tools": [{"google_search": {}}],
                            "temperature": 0
                        }
                    )
                    if response.text:
                        # ç”Ÿæˆä¸€å€‹å”¯ä¸€çš„ ID çµ¦æ¯å€‹æŸ¥è©¢çµæžœï¼Œç”¨æ–¼ resolve_urls
                        query_id = state.tool_round * 100 + search_queries.index(query) # ç¢ºä¿å”¯ä¸€æ€§
                        
                        # åˆå§‹åŒ–å®‰å…¨é è¨­å€¼
                        grounding_chunks = []
                        resolved_urls = {} # ç¢ºä¿åˆå§‹åŒ–ç‚ºç©ºå­—å…¸
                        citations = []
                        modified_text = response.text

                        # å˜—è©¦æå– grounding_chunks
                        if response.candidates and len(response.candidates) > 0:
                            candidate_0 = response.candidates[0]
                            if candidate_0 and hasattr(candidate_0, 'grounding_metadata') and candidate_0.grounding_metadata:
                                if hasattr(candidate_0.grounding_metadata, 'grounding_chunks') and candidate_0.grounding_metadata.grounding_chunks:
                                    grounding_chunks = candidate_0.grounding_metadata.grounding_chunks

                        # è™•ç† URL è§£æžå’Œå¼•ç”¨
                        try:
                            resolved_urls = resolve_urls(grounding_chunks, query_id)
                            # ç¢ºèªé€™è£¡æ²’æœ‰å°åˆ—è¡¨é¡žåž‹çš„æª¢æŸ¥ï¼Œå› ç‚º resolved_urls æ‡‰ç‚ºå­—å…¸
                        except Exception as url_e:
                            self.logger.warning(f"è§£æž URL å¤±æ•—: {url_e}")
                            resolved_urls = {} # ç¢ºä¿é‡è¨­ç‚ºç©ºå­—å…¸

                        try:
                            citations = get_citations(response, resolved_urls)
                            # ç¢ºèªé€™è£¡ä¸å†æœ‰å°åˆ—è¡¨é¡žåž‹çš„æª¢æŸ¥ï¼Œå› ç‚º get_citations å…§éƒ¨æœƒè™•ç†
                        except Exception as cite_e:
                            self.logger.warning(f"ç²å–å¼•ç”¨å¤±æ•—: {cite_e}")
                            citations = []

                        # æ’å…¥å¼•ç”¨æ¨™è¨˜
                        modified_text = insert_citation_markers(response.text, citations)
                        
                        results.append(modified_text)
                    else:
                        results.append(f"é‡å°æŸ¥è©¢ã€Œ{query}ã€æ²’æœ‰æ‰¾åˆ°å…§å®¹ã€‚")
                except Exception as llm_e:
                    self.logger.error(f"LLM åŸ·è¡Œ Google æœå°‹å¤±æ•—: {llm_e}")
                    results.append(f"åŸ·è¡Œæœå°‹æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤: {llm_e}")
                
        except Exception as e:
            self.logger.error(f"Google æœå°‹å¤±æ•—: {e}")
        
        return results
    
    def _execute_citation_tool(self, state: OverallState) -> List[str]:
        """åŸ·è¡Œå¼•ç”¨å·¥å…·ï¼ˆå…§åµŒå¯¦ä½œï¼‰"""
        tool_results = state.tool_results
        
        # ç°¡åŒ–çš„å¼•ç”¨ç”Ÿæˆ
        citations = []
        for i, result in enumerate(tool_results, 1):
            citation = f"[{i}] {result[:50]}..."
            citations.append(citation)
        
        return citations
    
    def _evaluate_results_sufficiency(self, tool_results: List[str], research_topic: str) -> bool:
        """è©•ä¼°çµæžœæ˜¯å¦å……åˆ†"""
        # å¦‚æžœæ²’æœ‰å·¥å…·çµæžœï¼Œè¡¨ç¤ºå¯èƒ½æ²’æœ‰åˆé©çš„å·¥å…·ï¼Œèªç‚ºæ‡‰è©²çµæŸ
        if not tool_results:
            return True  # ä¿®æ”¹ï¼šæ²’æœ‰çµæžœæ™‚èªç‚ºå……åˆ†ï¼Œé¿å…ç„¡é™å¾ªç’°
        
        # å¦‚æžœæœ‰çµæžœä¸”é•·åº¦åˆç†ï¼Œèªç‚ºå……åˆ†
        total_length = sum(len(result) for result in tool_results)
        return total_length > 20  # è‡³å°‘20å­—ç¬¦çš„çµæžœ
    
    def _generate_final_answer(self, messages: List[MsgNode], context: str) -> str:
        """ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆï¼ˆä½¿ç”¨å°ˆç”¨ LLMï¼‰"""
        if not messages:
            return "å—¯...å¥½åƒæ²’æœ‰æ”¶åˆ°ä½ çš„è¨Šæ¯è€¶ï¼Œå¯ä»¥å†è©¦ä¸€æ¬¡å—Žï¼ŸðŸ˜…"
        
        latest_message = messages[-1]
        user_question = latest_message.content
        
        # å˜—è©¦ä½¿ç”¨å°ˆç”¨çš„å›žç­”ç”Ÿæˆ LLM
        final_answer_llm = self.llm_instances.get("final_answer")
        
        # ä¸ç®¡æœ‰æ²’æœ‰contextï¼Œéƒ½ç”¨LLMç”Ÿæˆæ›´è‡ªç„¶çš„å›žæ‡‰
        if final_answer_llm:
            try:
                if context:
                    # æœ‰æœå°‹çµæžœçš„æƒ…æ³
                    answer_prompt = f"""
ä½ æ˜¯ä¸€å€‹å‹å–„ã€è°æ˜Žçš„èŠå¤©åŠ©æ‰‹ã€‚è«‹ç”¨è‡ªç„¶ã€äººæ€§åŒ–çš„æ–¹å¼å›žç­”ç”¨æˆ¶çš„å•é¡Œã€‚

ç”¨æˆ¶å•é¡Œï¼š{user_question}

ç›¸é—œè³‡è¨Šï¼š
{context}

å›žç­”è¦æ±‚ï¼š
- ç”¨è¼•é¬†ã€å‹å¥½çš„èªžèª¿
- åƒæœ‹å‹é–“èŠå¤©ä¸€æ¨£è‡ªç„¶
- å¯ä»¥é©ç•¶ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ
- å›žç­”è¦å¯¦ç”¨ä¸”å®¹æ˜“ç†è§£
- è¡¨ç¾å‡ºå°ç”¨æˆ¶å•é¡Œçš„é—œå¿ƒå’Œç†è§£

è«‹æä¾›ä¸€å€‹æº«æš–ã€æœ‰å¹«åŠ©çš„å›žç­”ï¼š
"""
                else:
                    # ä¸€èˆ¬èŠå¤©çš„æƒ…æ³
                    answer_prompt = f"""
                ä½ æ˜¯ä¸€å€‹å‹å–„ã€è°æ˜Žçš„èŠå¤©åŠ©æ‰‹ã€‚è«‹ç”¨è‡ªç„¶ã€äººæ€§åŒ–çš„æ–¹å¼å›žç­”ç”¨æˆ¶çš„å•é¡Œæˆ–èˆ‡ç”¨æˆ¶å°è©±ã€‚

                ç”¨æˆ¶èªªï¼š{user_question}

                å›žç­”è¦æ±‚ï¼š
                - ç”¨è¼•é¬†ã€å‹å¥½çš„èªžèª¿ï¼Œåƒæœ‹å‹é–“èŠå¤©
                - å¯ä»¥é©ç•¶ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ
                - ç›´æŽ¥å›žç­”å•é¡Œæˆ–å›žæ‡‰ç”¨æˆ¶çš„è©±é¡Œ
                - è¡¨ç¾å‡ºèˆˆè¶£å’Œé—œå¿ƒ
                - å¦‚æžœæ˜¯å•é¡Œï¼Œç›¡åŠ›å›žç­”ï¼›å¦‚æžœæ˜¯é–’èŠï¼Œå‹å–„å›žæ‡‰
                - å¯ä»¥é©ç•¶æå‡ºç›¸é—œçš„å¾ŒçºŒå•é¡Œä¾†å»¶çºŒå°è©±

                è«‹æä¾›ä¸€å€‹æº«æš–ã€è‡ªç„¶çš„å›žæ‡‰ï¼š
                """
                
                response = final_answer_llm.invoke(answer_prompt)
                return response.content.strip()
                
            except Exception as e:
                self.logger.warning(f"ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}")
        
        # å›žé€€åˆ°ç°¡åŒ–é‚è¼¯ - ä¹Ÿè¦äººæ€§åŒ–
        if context:
            return f"é—œæ–¼ä½ å•çš„ã€Œ{user_question}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€äº›æœ‰ç”¨çš„è³‡è¨Šå‘¢ï¼âœ¨\n\n{context}\n\nå¸Œæœ›é€™äº›å°ä½ æœ‰å¹«åŠ©ï¼é‚„æœ‰ä»€éº¼æƒ³äº†è§£çš„å—Žï¼ŸðŸ˜Š"
        else:
            return f"å—¨ï¼é—œæ–¼ã€Œ{user_question}ã€ï¼Œæˆ‘å¾ˆæ¨‚æ„å’Œä½ èŠèŠï½žé›–ç„¶æˆ‘ç¾åœ¨æ²’æœ‰é¡å¤–çš„æœå°‹è³‡è¨Šï¼Œä½†æˆ‘æœƒç›¡æˆ‘æ‰€çŸ¥ä¾†å›žç­”ä½ ï¼ðŸ˜Š æœ‰ä»€éº¼ç‰¹åˆ¥æƒ³èŠçš„å—Žï¼Ÿ"


# ä¾¿åˆ©å‡½æ•¸

def create_unified_agent(config: Optional[Dict[str, Any]] = None) -> UnifiedAgent:
    """å»ºç«‹çµ±ä¸€ Agent å¯¦ä¾‹"""
    return UnifiedAgent(config)


def create_agent_graph(config: Optional[Dict[str, Any]] = None) -> StateGraph:
    """å»ºç«‹ä¸¦ç·¨è­¯ Agent åœ–"""
    agent = create_unified_agent(config)
    return agent.build_graph() 