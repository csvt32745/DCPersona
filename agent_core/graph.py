"""
çµ±ä¸€çš„ LangGraph åœ–å¯¦ä½œ

æ ¹æ“šé…ç½®å‹•æ…‹æ§‹å»º LangGraph çš„ StateGraphï¼Œå®šç¾©æ‰€æœ‰ç¯€é»žçš„å¯¦ç¾ï¼Œ
åŒ…æ‹¬å·¥å…·é‚è¼¯çš„ç›´æŽ¥å…§åµŒã€‚åš´æ ¼åƒè€ƒ reference_arch.md ä¸­å®šç¾©çš„ Agent æ ¸å¿ƒæµç¨‹ã€‚
"""

import asyncio
import logging
import json
from typing import Dict, Any, List, Optional, Literal
from datetime import datetime

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langchain_core.runnables import RunnableConfig
from google.genai import Client

from schemas.agent_types import OverallState, MsgNode, ToolPlan, AgentPlan, ToolExecutionState
from utils.config_loader import load_typed_config
from prompt_system.prompts import get_current_date, PromptSystem
from .progress_mixin import ProgressMixin
from schemas.config_types import AppConfig


class UnifiedAgent(ProgressMixin):
    """çµ±ä¸€çš„ Agent å¯¦ä½œï¼Œæ ¹æ“šé…ç½®å‹•æ…‹èª¿æ•´è¡Œç‚º"""
    
    def __init__(self, config: Optional[AppConfig] = None):
        """
        åˆå§‹åŒ–çµ±ä¸€ Agent
        
        Args:
            config: åž‹åˆ¥å®‰å…¨çš„é…ç½®å¯¦ä¾‹ï¼Œå¦‚æžœç‚º None å‰‡è¼‰å…¥é è¨­é…ç½®
        """
        # é¦–å…ˆåˆå§‹åŒ– ProgressMixin
        super().__init__()
        
        self.config = config or load_typed_config()
        self.logger = logging.getLogger(__name__)
        
        # å¾žé…ç½®ç²å– agent è¨­å®š - ä½¿ç”¨åž‹åˆ¥å®‰å…¨å­˜å–
        self.agent_config = self.config.agent
        self.tools_config = self.config.agent.tools
        self.behavior_config = self.config.agent.behavior
        
        # åˆå§‹åŒ–å¤šå€‹ LLM å¯¦ä¾‹
        self.llm_instances = self._initialize_llm_instances()
        
        # åˆå§‹åŒ– Google å®¢æˆ¶ç«¯ï¼ˆå¦‚æžœå·¥å…·å•Ÿç”¨ï¼‰
        self.google_client = None
        if self.config.is_tool_enabled("google_search"):
            api_key = self.config.gemini_api_key
            if api_key:
                self.google_client = Client(api_key=api_key)
        
        # åˆå§‹åŒ–æç¤ºè©žç³»çµ±
        self.prompt_system = PromptSystem(
            persona_cache_enabled=self.config.prompt_system.persona.cache_personas
        )
    
    def _initialize_llm_instances(self) -> Dict[str, Optional[ChatGoogleGenerativeAI]]:
        """åˆå§‹åŒ–ä¸åŒç”¨é€”çš„ LLM å¯¦ä¾‹"""
        api_key = self.config.gemini_api_key
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is required")
        
        # ä½¿ç”¨åž‹åˆ¥å®‰å…¨çš„é…ç½®å­˜å–
        llm_configs_source = self.config.llm.models
        
        llm_instances = {}
        for purpose, llm_config in llm_configs_source.items():
            try:
                # llm_config ç¾åœ¨ç¸½æ˜¯ LLMModelConfig å¯¦ä¾‹
                model_name = llm_config.model
                temperature = llm_config.temperature
                
                llm_instances[purpose] = ChatGoogleGenerativeAI(
                    model=model_name,
                    temperature=temperature,
                    api_key=api_key
                )
                self.logger.info(f"åˆå§‹åŒ– {purpose} LLM: {model_name}")
            except Exception as e:
                self.logger.warning(f"åˆå§‹åŒ– {purpose} LLM å¤±æ•—: {e}")
                llm_instances[purpose] = None
        
        self.tool_analysis_llm: ChatGoogleGenerativeAI = llm_instances.get("tool_analysis")
        self.final_answer_llm: ChatGoogleGenerativeAI = llm_instances.get("final_answer")
        
        return llm_instances
    
    def build_graph(self) -> StateGraph:
        """å»ºç«‹ç°¡åŒ–çš„ LangGraph"""
        builder = StateGraph(OverallState)
        
        # æ ¸å¿ƒç¯€é»ž
        builder.add_node("generate_query_or_plan", self.generate_query_or_plan)
        builder.add_node("execute_single_tool", self.execute_single_tool)
        builder.add_node("reflection", self.reflection)
        builder.add_node("finalize_answer", self.finalize_answer)
        
        # æµç¨‹è¨­ç½®
        builder.add_edge(START, "generate_query_or_plan")
        
        # æ¢ä»¶è·¯ç”±ï¼šæ ¹æ“šè¨ˆåŠƒæ±ºå®šä¸‹ä¸€æ­¥
        builder.add_conditional_edges(
            "generate_query_or_plan",
            self.route_and_dispatch_tools,
            {
                "execute_tools": "execute_single_tool",
                "direct_answer": "finalize_answer"
            }
        )
        
        # Connect dispatch node to reflection
        builder.add_edge("execute_single_tool", "reflection")
        
        # åæ€å¾Œçš„è·¯ç”±æ±ºç­–
        builder.add_conditional_edges(
            "reflection",
            self.decide_next_step,
            {
                "continue": "generate_query_or_plan",  # é‡æ–°è¦åŠƒä¸‹ä¸€è¼ª
                "finish": "finalize_answer"
            }
        )
        
        builder.add_edge("finalize_answer", END)
        return builder.compile()

    def route_after_planning(self, state: OverallState) -> str:
        """è¦åŠƒå¾Œçš„è·¯ç”±æ±ºç­–"""
        agent_plan = state.agent_plan
        if agent_plan and agent_plan.needs_tools and agent_plan.tool_plans:
            return "use_tools"
        else:
            return "direct_answer"

    async def generate_query_or_plan(self, state: OverallState) -> Dict[str, Any]:
        """
        çµ±ä¸€çš„è¨ˆåŠƒç”Ÿæˆç¯€é»žï¼šåŒæ™‚æ±ºå®šå·¥å…·ä½¿ç”¨å’Œç”ŸæˆæŸ¥è©¢
        
        åƒè€ƒ Gemini å¯¦ä½œï¼Œä½¿ç”¨ structured output ä¸€æ¬¡æ€§æ±ºå®šï¼š
        1. æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
        2. éœ€è¦ä½¿ç”¨å“ªäº›å·¥å…·
        3. æ¯å€‹å·¥å…·çš„å…·é«”æŸ¥è©¢åƒæ•¸
        """
        try:
            self.logger.info("generate_query_or_plan: é–‹å§‹ç”ŸæˆåŸ·è¡Œè¨ˆåŠƒ")
            
            # é€šçŸ¥é–‹å§‹éšŽæ®µ
            await self._notify_progress(
                stage="generate_query", 
                message="ðŸ¤” æ­£åœ¨åˆ†æžæ‚¨çš„å•é¡Œä¸¦åˆ¶å®šæœå°‹è¨ˆåŠƒ..."
            )
            
            user_content = state.messages[-1].content
            max_tool_rounds = self.behavior_config.max_tool_rounds
            
            if max_tool_rounds == 0:
                # ç´”å°è©±æ¨¡å¼
                return {
                    "agent_plan": AgentPlan(needs_tools=False),
                    "tool_round": 0
                }
            
            # ä½¿ç”¨ structured LLM ç”Ÿæˆå®Œæ•´è¨ˆåŠƒ
            if not self.tool_analysis_llm:
                # å›žé€€åˆ°ç°¡å–®é‚è¼¯
                needs_tools = self._analyze_tool_necessity_fallback(state.messages)
                queries = [user_content] if needs_tools else []
                tool_plans = []
                if needs_tools and self.google_client:
                    tool_plans = [ToolPlan(tool_name="google_search", queries=queries)]
                
                agent_plan = AgentPlan(
                    needs_tools=needs_tools,
                    tool_plans=tool_plans,
                    reasoning="ä½¿ç”¨ç°¡åŒ–é‚è¼¯æ±ºç­–"
                )
            else:
                # ä½¿ç”¨ structured output ç”Ÿæˆè¨ˆåŠƒ
                agent_plan = await self._generate_structured_plan(state.messages, state.messages_global_metadata)
            
            self.logger.info(f"ç”Ÿæˆè¨ˆåŠƒ: éœ€è¦å·¥å…·={agent_plan.needs_tools}, å·¥å…·æ•¸é‡={len(agent_plan.tool_plans)}")
            
            return {
                "agent_plan": agent_plan,
                "tool_round": 0,
                "research_topic": user_content
            }
            
        except Exception as e:
            self.logger.error(f"generate_query_or_plan å¤±æ•—: {e}")
            return {
                "agent_plan": AgentPlan(needs_tools=False),
                "finished": True
            }

    async def _generate_structured_plan(self, messages: List[MsgNode], messages_global_metadata: str = "") -> AgentPlan:
        """ä½¿ç”¨ structured output ç”ŸæˆåŸ·è¡Œè¨ˆåŠƒ"""
        try:
            # æ§‹å»ºè¨ˆåŠƒç”Ÿæˆæç¤ºè©ž
            plan_prompt = self._build_planning_prompt(messages, messages_global_metadata)
            
            # ç”±æ–¼ LangChain çš„ structured output å¯èƒ½ä¸æ”¯æ´è¤‡é›œçš„åµŒå¥—çµæ§‹ï¼Œ
            # æˆ‘å€‘å…ˆç”¨æ™®é€š LLM ç”Ÿæˆ JSONï¼Œç„¶å¾Œæ‰‹å‹•è§£æž
            response = await asyncio.to_thread(self.tool_analysis_llm.invoke, plan_prompt)
            
            # è§£æž JSON å›žæ‡‰
            plan_data = self._parse_plan_response(response.content)
            
            # æ§‹å»º AgentPlan
            tool_plans = []
            if plan_data.get("needs_tools", False):
                for tool_data in plan_data.get("tool_plans", []):
                    tool_plan = ToolPlan(
                        tool_name=tool_data.get("tool_name", "google_search"),
                        queries=tool_data.get("queries", []),
                        priority=tool_data.get("priority", 1)
                    )
                    tool_plans.append(tool_plan)
            
            return AgentPlan(
                needs_tools=plan_data.get("needs_tools", False),
                tool_plans=tool_plans,
                reasoning=plan_data.get("reasoning", "")
            )
            
        except Exception as e:
            self.logger.warning(f"structured plan ç”Ÿæˆå¤±æ•—ï¼Œå›žé€€åˆ°ç°¡åŒ–é‚è¼¯: {e}")
            # å›žé€€é‚è¼¯
            needs_tools = self._analyze_tool_necessity_fallback(messages)
            queries = [messages[-1].content[:100]] if needs_tools else []
            tool_plans = []
            if needs_tools and self.google_client:
                tool_plans = [ToolPlan(tool_name="google_search", queries=queries)]
            
            return AgentPlan(
                needs_tools=needs_tools,
                tool_plans=tool_plans,
                reasoning="å›žé€€åˆ°ç°¡åŒ–é‚è¼¯"
            )

    def _build_planning_prompt(self, messages: List[MsgNode], messages_global_metadata: str = "") -> List:
        """æ§‹å»ºè¨ˆåŠƒç”Ÿæˆæç¤ºè©žï¼ˆä½¿ç”¨çµ±ä¸€ PromptSystem å’Œå·¥å…·æç¤ºè©žæª”æ¡ˆï¼‰ï¼Œæ•´åˆå…¨åŸŸ metadata"""
        try:
            # ä½¿ç”¨ PromptSystem æ§‹å»ºåŸºç¤Ž system prompt
            base_system_prompt = self.prompt_system.get_system_instructions(
                config=self.config,  # ä½¿ç”¨ typed config
                available_tools=self.config.get_enabled_tools(),
                messages_global_metadata=messages_global_metadata
            )
            
            # å¾žæª”æ¡ˆè®€å–è¨ˆåŠƒç”Ÿæˆç‰¹å®šçš„æŒ‡ä»¤
            current_date = get_current_date(self.config.system.timezone)
            planning_instructions = self.prompt_system.get_planning_instructions(
                current_date=current_date
            )
            
            # çµ„åˆå®Œæ•´çš„ system prompt
            full_system_prompt = base_system_prompt + "\n\n" + planning_instructions
            messages_for_llm = [SystemMessage(content=full_system_prompt)]
            
            # æ·»åŠ å°è©±æ­·å²
            for msg in messages:
                if msg.role == "user":
                    messages_for_llm.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    messages_for_llm.append(AIMessage(content=msg.content))
            
            return messages_for_llm
            
        except Exception as e:
            self.logger.error(f"æ§‹å»ºè¨ˆåŠƒæç¤ºè©žå¤±æ•—: {e}")
            # å›žé€€åˆ°ç°¡åŒ–ç‰ˆæœ¬
            fallback_prompt = "ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ã€‚è«‹åˆ†æžç”¨æˆ¶çš„å•é¡Œä¸¦æ±ºå®šæ˜¯å¦éœ€è¦æœå°‹è³‡è¨Šã€‚"
            return [SystemMessage(content=fallback_prompt)]

    def _parse_plan_response(self, response_content: str) -> Dict[str, Any]:
        """è§£æžè¨ˆåŠƒå›žæ‡‰çš„ JSON"""
        try:
            # å˜—è©¦æå– JSON
            if "```json" in response_content:
                start_marker = "```json"
                end_marker = "```"
                start_index = response_content.find(start_marker)
                end_index = response_content.rfind(end_marker)
                
                if start_index != -1 and end_index != -1 and end_index > start_index:
                    json_str = response_content[start_index + len(start_marker):end_index].strip()
                    return json.loads(json_str)
            
            # å˜—è©¦ç›´æŽ¥è§£æž
            if response_content.strip().startswith("{"):
                return json.loads(response_content.strip())
            
            # å¦‚æžœéƒ½å¤±æ•—ï¼Œè¿”å›žé è¨­å€¼
            return {"needs_tools": False, "tool_plans": [], "reasoning": "JSON è§£æžå¤±æ•—"}
            
        except json.JSONDecodeError as e:
            self.logger.warning(f"JSON è§£æžå¤±æ•—: {e}")
            return {"needs_tools": False, "tool_plans": [], "reasoning": "JSON è§£æžå¤±æ•—"}

    def route_and_dispatch_tools(self, state: OverallState) -> Any:
        """
        è¦åŠƒå¾Œçš„è·¯ç”±æ±ºç­–ï¼š
        å¦‚æžœéœ€è¦å·¥å…·ï¼Œå‰‡è¿”å›ž Send ç‰©ä»¶åˆ—è¡¨ä»¥é€²è¡Œä¸¦è¡ŒåŸ·è¡Œï¼›
        å¦å‰‡ï¼Œè¿”å›ž "direct_answer" ä»¥ç›´æŽ¥è·³åˆ°æœ€çµ‚ç­”æ¡ˆã€‚
        """
        agent_plan = state.agent_plan
        
        if agent_plan and agent_plan.needs_tools and agent_plan.tool_plans:
            # ç‚ºæ¯å€‹å·¥å…·è¨ˆåŠƒå‰µå»ºä¸¦è¡ŒåŸ·è¡Œä»»å‹™
            sends = []
            for idx, tool_plan in enumerate(agent_plan.tool_plans):
                for query_idx, query in enumerate(tool_plan.queries):
                    sends.append(Send(
                        "execute_single_tool", 
                        {
                            "tool_name": tool_plan.tool_name,
                            "query": query,
                            "task_id": f"{idx}_{query_idx}",
                            "priority": tool_plan.priority
                        }
                    ))
            self.logger.info(f"route_and_dispatch_tools: æ­£åœ¨èª¿åº¦ {len(sends)} å€‹ä¸¦è¡Œå·¥å…·åŸ·è¡Œä»»å‹™")
            return sends # è¿”å›ž Send ç‰©ä»¶åˆ—è¡¨ä»¥é€²è¡Œä¸¦è¡ŒåŸ·è¡Œ
        else:
            self.logger.info("route_and_dispatch_tools: ç„¡éœ€å·¥å…·ï¼Œç›´æŽ¥å›žç­”")
            return "direct_answer" # è¿”å›žå­—ç¬¦ä¸²ä»¥ç›´æŽ¥è·¯ç”±

    async def execute_single_tool(self, state: ToolExecutionState) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹å·¥å…·ä»»å‹™ï¼ˆä¸¦è¡Œç¯€é»žï¼‰"""
        tool_name = state.get("tool_name")
        query = state.get("query")
        task_id = state.get("task_id")
        
        try:
            self.logger.info(f"åŸ·è¡Œå–®å€‹å·¥å…·: {tool_name}({query})")
            
            if tool_name == "google_search":
                await self._notify_progress(
                    stage="Tool Execution",
                    message=f"ðŸ¤” æ­£åœ¨åŸ·è¡Œ {tool_name} å·¥å…·ï¼ŒæŸ¥è©¢é—œéµå­—: {query}",
                    progress_percentage=50
                )
                result = await self._execute_google_search_single(query, task_id)
            else:
                await self._notify_progress(
                    stage="Tool Execution",
                    message=f"ðŸ¤” æ­£åœ¨åŸ·è¡Œ {tool_name} å·¥å…·...",
                    progress_percentage=50
                )
                
                result = f"æœªçŸ¥å·¥å…·: {tool_name}"
            
            return {
                "tool_results": [result]
                # "task_id": task_id,
                # "tool_name": tool_name
            }
            
        except Exception as e:
            self.logger.error(f"å·¥å…·åŸ·è¡Œå¤±æ•— {tool_name}({query}): {e}")
            return {
                "tool_results": [],
                # "task_id": task_id,
                "error": str(e)
            }

    async def _execute_google_search_single(self, query: str, task_id: str) -> str:
        """åŸ·è¡Œå–®å€‹ Google æœå°‹"""
        if not self.google_client:
            return f"Google å®¢æˆ¶ç«¯æœªé…ç½®ï¼Œç„¡æ³•åŸ·è¡Œæœå°‹: {query}"
        
        try:
            current_date = get_current_date(timezone_str=self.config.system.timezone)
            
            # æº–å‚™å‚³éžçµ¦ Gemini æ¨¡åž‹çš„æç¤º
            formatted_prompt = self.prompt_system.get_web_searcher_instructions(
                research_topic=query,
                current_date=current_date
            )
            
            model_name = self.tool_analysis_llm.model
            # èª¿ç”¨ Gemini API ä¸¦å•Ÿç”¨ google_search å·¥å…·
            response = self.google_client.models.generate_content(
                model=model_name,
                contents=formatted_prompt,
                config={
                    "tools": [{"google_search": {}}],
                    "temperature": 0
                }
            )
            
            if response.text:
                # è™•ç† grounding å’Œå¼•ç”¨
                # grounding_chunks = []
                # resolved_urls = {}
                
                # # å˜—è©¦æå– grounding_chunks
                # if response.candidates and len(response.candidates) > 0:
                #     candidate_0 = response.candidates[0]
                #     if candidate_0 and hasattr(candidate_0, 'grounding_metadata') and candidate_0.grounding_metadata:
                #         if hasattr(candidate_0.grounding_metadata, 'grounding_chunks') and candidate_0.grounding_metadata.grounding_chunks:
                #             grounding_chunks = candidate_0.grounding_metadata.grounding_chunks

                # è™•ç† URL è§£æžå’Œå¼•ç”¨
                # try:
                #     resolved_urls = resolve_urls(grounding_chunks, task_id)
                #     return response.text
                # except Exception as e:
                #     self.logger.warning(f"è™•ç†å¼•ç”¨å¤±æ•—: {e}")
                return response.text
            else:
                return f"é‡å°æŸ¥è©¢ã€Œ{query}ã€æ²’æœ‰æ‰¾åˆ°å…§å®¹ã€‚"
                
        except Exception as e:
            self.logger.error(f"Google æœå°‹å¤±æ•—: {e}")
            return f"æœå°‹åŸ·è¡Œå¤±æ•—: {str(e)}"

    def _deduplicate_results(self, results: List[str]) -> List[str]:
        """åŽ»é‡å’ŒæŽ’åºçµæžœ"""
        if not results:
            return []
        
        # ç°¡å–®åŽ»é‡
        seen = set()
        unique_results = []
        for result in results:
            if result and result not in seen:
                seen.add(result)
                unique_results.append(result)
        
        return unique_results

    async def reflection(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šåæ€
        
        è©•ä¼°å·¥å…·çµæžœçš„è³ªé‡å’Œå®Œæ•´æ€§ã€‚
        """
        try:
            if not self.behavior_config.enable_reflection:
                # å¦‚æžœç¦ç”¨åæ€ï¼Œç›´æŽ¥èªç‚ºçµæžœå……åˆ†
                return {"is_sufficient": True}
            
            self.logger.info("reflection: é–‹å§‹åæ€å·¥å…·çµæžœ")
            
            # é€šçŸ¥åæ€éšŽæ®µ
            await self._notify_progress(
                stage="reflection",
                message="ðŸ¤” æ­£åœ¨è©•ä¼°æœå°‹çµæžœçš„å“è³ª...",
                progress_percentage=75
            )
            
            # tool_results will be accumulated from parallel execute_single_tool calls
            raw_tool_results = state.tool_results or []
            unique_tool_results = self._deduplicate_results(raw_tool_results) # Deduplicate here
            research_topic = state.research_topic
            
            # ä½¿ç”¨å°ˆç”¨çš„åæ€ LLM æˆ–ç°¡åŒ–é‚è¼¯
            is_sufficient = self._evaluate_results_sufficiency(unique_tool_results, research_topic)
            
            self.logger.info(f"reflection: çµæžœå……åˆ†åº¦={is_sufficient}")
            
            return {
                "is_sufficient": is_sufficient,
                "reflection_complete": True,
                "reflection_reasoning": f"åŸºæ–¼ {len(unique_tool_results)} å€‹çµæžœçš„è©•ä¼°",
                "aggregated_tool_results": unique_tool_results # Update for next stages
            }
            
        except Exception as e:
            self.logger.error(f"reflection å¤±æ•—: {e}")
            return {"is_sufficient": True}  # å¤±æ•—æ™‚å‡è¨­å……åˆ†

    def decide_next_step(self, state: OverallState) -> Literal["continue", "finish"]:
        """æ±ºå®šä¸‹ä¸€æ­¥çš„è·¯ç”±å‡½æ•¸"""
        try:
            current_round = state.tool_round
            max_rounds = self.behavior_config.max_tool_rounds
            is_sufficient = state.is_sufficient
            
            # æ±ºç­–é‚è¼¯
            if is_sufficient or current_round >= max_rounds:
                self.logger.info(f"æ±ºå®šå®Œæˆç ”ç©¶ (è¼ªæ¬¡={current_round}, å……åˆ†={is_sufficient})")
                return "finish"
            else:
                self.logger.info(f"æ±ºå®šç¹¼çºŒç ”ç©¶ (è¼ªæ¬¡={current_round}/{max_rounds})")
                return "continue"
                
        except Exception as e:
            self.logger.error(f"decide_next_step å¤±æ•—: {e}")
            return "finish"

    async def finalize_answer(self, state: OverallState) -> Dict[str, Any]:
        """
        LangGraph ç¯€é»žï¼šç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
        
        åŸºæ–¼æ‰€æœ‰å¯ç”¨çš„ä¿¡æ¯ç”Ÿæˆæœ€çµ‚å›žç­”ã€‚
        """
        try:
            self.logger.info("finalize_answer: ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
            
            # é€šçŸ¥ç­”æ¡ˆç”ŸæˆéšŽæ®µ
            await self._notify_progress(
                stage="finalize_answer",
                message="âœï¸ æ­£åœ¨æ•´ç†ç­”æ¡ˆ...",
                progress_percentage=90
            )
            
            messages = state.messages
            tool_results = state.aggregated_tool_results or state.tool_results or []
            
            # æ§‹å»ºä¸Šä¸‹æ–‡
            context = ""
            if tool_results:
                context = "\n".join([f"æœå°‹çµæžœ: {result}" for result in tool_results])
            
            # ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
            try:
                final_answer = self._generate_final_answer(messages, context, state.messages_global_metadata)
            except Exception as e:
                self.logger.warning(f"LLM ç­”æ¡ˆç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨åŸºæœ¬å›žè¦†: {e}")
                final_answer = self._generate_basic_fallback_answer(messages, context)
            
            self.logger.info("finalize_answer: ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            
            # é€šçŸ¥å®Œæˆ
            await self._notify_progress(
                stage="completed",
                message="âœ… å›žç­”å®Œæˆï¼",
                progress_percentage=100
            )
            
            return {
                "final_answer": final_answer,
                "finished": True,
                "sources": self._extract_sources_from_results(tool_results)
            }
            
        except Exception as e:
            self.logger.error(f"finalize_answer å¤±æ•—: {e}")
            
            # é€šçŸ¥éŒ¯èª¤
            await self._notify_error(e)
            
            return {
                "final_answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›žç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚", 
                "finished": True
            }

    # ä¿ç•™åŽŸæœ‰çš„è¼”åŠ©æ–¹æ³•ä»¥ç¢ºä¿å‘å¾Œç›¸å®¹æ€§
    def _analyze_tool_necessity_fallback(self, messages: List[MsgNode]) -> bool:
        """é—œéµå­—æª¢æ¸¬å›žé€€æ–¹æ¡ˆ (ä»åŸºæ–¼æœ€æ–°è¨Šæ¯)"""
        tool_keywords = [
            "æœå°‹", "æŸ¥è©¢", "æœ€æ–°", "ç¾åœ¨", "ä»Šå¤©", "ä»Šå¹´", "æŸ¥æ‰¾",
            "è³‡è¨Š", "è³‡æ–™", "æ–°èž", "æ¶ˆæ¯", "æ›´æ–°", "ç‹€æ³", "search",
            "find", "latest", "current", "recent", "what is", "å‘Šè¨´æˆ‘",
            "ç¶²è·¯æœå°‹", "ç¶²è·¯ç ”ç©¶"
        ]
        
        content_lower = messages[-1].content.lower()
        return any(keyword in content_lower for keyword in tool_keywords)

    def _evaluate_results_sufficiency(self, tool_results: List[str], research_topic: str) -> bool:
        """è©•ä¼°çµæžœæ˜¯å¦å……åˆ†"""
        # é‡æ§‹å¾Œç°¡åŒ–é‚è¼¯ï¼šç¸½æ˜¯èªç‚ºçµæžœå……åˆ†ï¼Œé¿å…ç„¡é™å¾ªç’°
        # å¯¦éš›çš„è¼ªæ¬¡æŽ§åˆ¶ç”± max_tool_rounds ä¾†è™•ç†
        return True

    def _generate_final_answer(self, messages: List[MsgNode], context: str, messages_global_metadata: str = "") -> str:
        """ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆï¼ˆä½¿ç”¨çµ±ä¸€ PromptSystem å’Œå·¥å…·æç¤ºè©žæª”æ¡ˆï¼‰ï¼Œæ•´åˆå…¨åŸŸ metadata"""
        if not messages:
            return "å—¯...å¥½åƒæ²’æœ‰æ”¶åˆ°ä½ çš„è¨Šæ¯è€¶ï¼Œå¯ä»¥å†è©¦ä¸€æ¬¡å—Žï¼ŸðŸ˜…"
        
        latest_message = messages[-1]
        user_question = latest_message.content
        
        # ä½¿ç”¨ PromptSystem æ§‹å»ºç³»çµ±æç¤ºè©ž
        try:
            # æ§‹å»ºåŸºç¤Žç³»çµ±æç¤ºè©ž
            base_system_prompt = self.prompt_system.get_system_instructions(
                config=self.config,  # ä½¿ç”¨ typed config
                available_tools=[],  # æœ€çµ‚ç­”æ¡ˆç”ŸæˆéšŽæ®µä¸éœ€è¦å·¥å…·æè¿°
                messages_global_metadata=messages_global_metadata
            )
            
            # æ·»åŠ ä¸Šä¸‹æ–‡è³‡è¨Šï¼ˆå¦‚æžœæœ‰çš„è©±ï¼‰
            if context:
                try:
                    context_prompt = self.prompt_system.get_final_answer_context(
                        context=context
                    )
                    full_system_prompt = base_system_prompt + "\n\n" + context_prompt
                except Exception as e:
                    self.logger.warning(f"è®€å–æœ€çµ‚ç­”æ¡ˆä¸Šä¸‹æ–‡æç¤ºè©žå¤±æ•—: {e}")
                    # å›žé€€åˆ°ç¡¬ç·¨ç¢¼ç‰ˆæœ¬
                    context_prompt = f"ä»¥ä¸‹æ˜¯ç›¸é—œè³‡è¨Šï¼š\n{context}\nè«‹åŸºæ–¼ä»¥ä¸Šè³‡è¨Šå›žç­”ã€‚"
                    full_system_prompt = base_system_prompt + "\n\n" + context_prompt
            else:
                full_system_prompt = base_system_prompt
            
            messages_for_final_answer = [SystemMessage(content=full_system_prompt)]
                
            for msg in messages[:-1]: # é™¤äº†æœ€å¾Œä¸€æ¢æ¶ˆæ¯ï¼ˆç•¶å‰ç”¨æˆ¶å•é¡Œï¼‰
                if msg.role == "user":
                    messages_for_final_answer.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    messages_for_final_answer.append(AIMessage(content=msg.content))

            # å°‡ç•¶å‰ç”¨æˆ¶å•é¡Œä½œç‚º HumanMessage åŠ å…¥
            messages_for_final_answer.append(HumanMessage(content=user_question))
            response = self.final_answer_llm.invoke(messages_for_final_answer)
            return response.content.strip()
            
        except Exception as e:
            self.logger.warning(f"ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}")
        
        return self._generate_basic_fallback_answer(messages, context)

    def _generate_basic_fallback_answer(self, messages: List[MsgNode], context: str) -> str:
        return "å‡ºç¾éŒ¯èª¤ï¼Œè«‹å†è©¦ä¸€æ¬¡ ðŸ”„"

    def _extract_sources_from_results(self, tool_results: List[str]) -> List[Dict[str, str]]:
        """å¾žå·¥å…·çµæžœä¸­æå–ä¾†æºä¿¡æ¯"""
        sources = []
        if tool_results:
            for result in tool_results:
                if isinstance(result, str) and "http" in result:
                    # ç°¡åŒ–çš„ä¾†æºæå–é‚è¼¯
                    sources.append({
                        "title": result[:100] + "..." if len(result) > 100 else result,
                        "url": "",
                        "snippet": result
                    })
        return sources


def create_unified_agent(config: Optional[AppConfig] = None) -> UnifiedAgent:
    """å»ºç«‹çµ±ä¸€ Agent å¯¦ä¾‹"""
    return UnifiedAgent(config)


def create_agent_graph(config: Optional[AppConfig] = None) -> StateGraph:
    """å»ºç«‹ä¸¦ç·¨è­¯ Agent åœ–"""
    agent = create_unified_agent(config)
    return agent.build_graph() 