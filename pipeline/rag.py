"""
RAG æµç¨‹ - æ•´åˆ LangGraph æ™ºèƒ½ç ”ç©¶

æ›´æ–°å¾Œçš„ RAG æµç¨‹æ”¯æ´æ™ºèƒ½æ¨¡å¼åˆ‡æ›ï¼š
- ç°¡å–®å•ç­”ä½¿ç”¨å‚³çµ±æµç¨‹
- è¤‡é›œç ”ç©¶è‡ªå‹•å•Ÿå‹• LangGraph
- æ”¯æ´ç”¨æˆ¶å¼·åˆ¶æŒ‡å®šæ¨¡å¼
"""

import logging
import asyncio
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
import discord
from openai import AsyncOpenAI

from agents.research_agent import ResearchAgent, create_research_agent_from_config, should_use_research_mode
from agents.state import DiscordContext, ResearchConfig
from agents.configuration import AgentConfiguration
from agents.utils import analyze_message_complexity, assess_message_complexity_with_llm, generate_session_id
from agents.tools_and_schemas import ErrorHandler, DiscordTools
from core.session_manager import get_session_manager


class SmartRAGPipeline:
    """æ™ºèƒ½ RAG æµç¨‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._research_agent: Optional[ResearchAgent] = None
        self._agent_config: Optional[AgentConfiguration] = None
        self._llm_client: Optional[AsyncOpenAI] = None
        self._cfg: Optional[Dict[str, Any]] = None
    
    async def initialize(self, llmcord_config: Dict[str, Any]):
        """
        åˆå§‹åŒ– RAG æµç¨‹
        
        Args:
            llmcord_config: llmcord é…ç½®
        """
        try:
            # å„²å­˜é…ç½®
            self._cfg = llmcord_config
            
            # å‰µå»º LLM å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼è¤‡é›œåº¦è©•ä¼°ï¼‰
            self._create_llm_client(llmcord_config)
            
            # å‰µå»º agent é…ç½®
            self._agent_config = AgentConfiguration.from_llmcord_config(llmcord_config)
            
            # é©—è­‰ API é‡‘é‘°
            is_valid, message = self._agent_config.validate_api_keys()
            if not is_valid:
                self.logger.warning(f"LangGraph åŠŸèƒ½å¯èƒ½å—é™: {message}")
            
            # å‰µå»ºç ”ç©¶ä»£ç†
            self._research_agent = ResearchAgent(self._agent_config)
            
            self.logger.info("æ™ºèƒ½ RAG æµç¨‹å·²åˆå§‹åŒ–")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ– RAG æµç¨‹å¤±æ•—: {str(e)}")
            self._research_agent = None
    
    def _create_llm_client(self, cfg: Dict[str, Any]):
        """
        å‰µå»º LLM å®¢æˆ¶ç«¯ç”¨æ–¼è¤‡é›œåº¦è©•ä¼°
        
        Args:
            cfg: llmcord é…ç½®
        """
        try:
            provider_slash_model = cfg.get("model", "openai/gpt-3.5-turbo")
            provider, model = provider_slash_model.split("/", 1)
            base_url = cfg.get("providers", {}).get(provider, {}).get("base_url", "https://api.openai.com/v1")
            api_key = cfg.get("providers", {}).get(provider, {}).get("api_key", "sk-no-key-required")
            
            self._llm_client = AsyncOpenAI(base_url=base_url, api_key=api_key)
            self._model_name = model  # å„²å­˜æ¨¡å‹åç¨±ç”¨æ–¼è¤‡é›œåº¦è©•ä¼°
            self.logger.info(f"LLM å®¢æˆ¶ç«¯å·²å‰µå»º: {provider}/{model}")
            
        except Exception as e:
            self.logger.warning(f"å‰µå»º LLM å®¢æˆ¶ç«¯å¤±æ•—: {str(e)}ï¼Œå°‡å›é€€åˆ°è¦å‰‡å‹è¤‡é›œåº¦è©•ä¼°")
            self._llm_client = None
            self._model_name = None
    
    async def process_message(
        self,
        message: discord.Message,
        message_data: Dict[str, Any],
        cfg: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è™•ç†è¨Šæ¯ä¸¦æ±ºå®šä½¿ç”¨å“ªç¨®æµç¨‹
        
        Args:
            message: Discord è¨Šæ¯
            message_data: è¨Šæ¯è³‡æ–™
            cfg: é…ç½®
            
        Returns:
            Dict: è™•ç†çµæœ
        """
        try:
            # æå–ä½¿ç”¨è€…è¨Šæ¯å…§å®¹
            user_content = self._extract_user_message(message_data)
            if not user_content:
                return await self._fallback_response(message, "ç„¡æ³•è­˜åˆ¥ä½¿ç”¨è€…è¨Šæ¯")
            
            # æª¢æŸ¥æ˜¯å¦å¼·åˆ¶ä½¿ç”¨ç ”ç©¶æ¨¡å¼
            force_research = self._check_force_research_mode(user_content)
            
            # ä½¿ç”¨æ–°çš„ LLM è¤‡é›œåº¦è©•ä¼°ï¼ˆå„ªå…ˆï¼‰ï¼Œå›é€€åˆ°è¦å‰‡è©•ä¼°
            try:
                if self._llm_client:
                    # ä½¿ç”¨ LLM é€²è¡Œè¤‡é›œåº¦è©•ä¼°
                    complexity_analysis = await assess_message_complexity_with_llm(
                        message_content=user_content,
                        llm_client=self._llm_client,
                        fallback_to_rules=True,
                        model_name=getattr(self, '_model_name', 'gpt-3.5-turbo')
                    )
                    
                    self.logger.info(f"LLM è¤‡é›œåº¦è©•ä¼°å®Œæˆ - æ±ºå®š: {complexity_analysis['final_decision']}, "
                                   f"æ–¹æ³•: {complexity_analysis['method']}, "
                                   f"ä¿¡å¿ƒ: {complexity_analysis['confidence']:.2f}")
                else:
                    # å›é€€åˆ°è¦å‰‡å‹è©•ä¼°
                    rule_result = analyze_message_complexity(user_content)
                    complexity_analysis = {
                        "final_decision": "RESEARCH" if rule_result["use_research"] else "SIMPLE",
                        "use_research": rule_result["use_research"],
                        "confidence": rule_result["complexity_score"],
                        "method": "rule_based_fallback"
                    }
                    
                    self.logger.info(f"è¦å‰‡è¤‡é›œåº¦è©•ä¼° - è¤‡é›œåº¦: {rule_result['complexity_score']:.2f}, "
                                   f"ä½¿ç”¨ç ”ç©¶: {rule_result['use_research']}")
                    
            except Exception as e:
                self.logger.warning(f"è¤‡é›œåº¦è©•ä¼°å¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡å‹è©•ä¼°: {str(e)}")
                rule_result = analyze_message_complexity(user_content)
                complexity_analysis = {
                    "final_decision": "RESEARCH" if rule_result["use_research"] else "SIMPLE",
                    "use_research": rule_result["use_research"],
                    "confidence": rule_result["complexity_score"],
                    "method": "rule_based_error_fallback"
                }
            
            # æ±ºå®šä½¿ç”¨å“ªç¨®æ¨¡å¼
            use_research_mode = (
                force_research or
                (self._research_agent is not None and
                 (complexity_analysis["use_research"] or
                  (self._agent_config and self._agent_config.should_use_research_mode(user_content))))
            )
            
            self.logger.info(f"æœ€çµ‚æ±ºå®š - ä½¿ç”¨ç ”ç©¶æ¨¡å¼: {use_research_mode} "
                           f"(å¼·åˆ¶: {force_research}, è¤‡é›œåº¦å»ºè­°: {complexity_analysis['use_research']})")
            
            if use_research_mode and self._research_agent:
                return await self._process_with_langgraph(message, user_content, message_data)
            else:
                return await self._process_with_traditional_rag(message, user_content, message_data)
                
        except Exception as e:
            self.logger.error(f"è™•ç†è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            return await self._fallback_response(message, str(e))
    
    async def _process_with_langgraph(
        self,
        message: discord.Message,
        user_content: str,
        message_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ LangGraph é€²è¡Œæ·±åº¦ç ”ç©¶"""
        try:
            # å‰µå»º Discord ä¸Šä¸‹æ–‡
            discord_ctx = DiscordContext(
                message=message,
                user_id=message.author.id,
                channel_id=message.channel.id,
                guild_id=message.guild.id if message.guild else None,
                session_id=generate_session_id(message.author.id, message.channel.id),
                is_dm=message.guild is None
            )
            
            # ç²å–æœƒè©±ç®¡ç†å™¨
            session_manager = get_session_manager()
            session_data = session_manager.get_or_create_session(message)
            
            # å‰µå»ºé€²åº¦å›èª¿
            progress_callback = await self._research_agent.create_progress_callback(message)
            
            # åŸ·è¡Œç ”ç©¶æµç¨‹
            research_result = await self._research_agent.process_research_request(
                discord_ctx=discord_ctx,
                user_message=user_content,
                progress_callback=progress_callback
            )
            
            if research_result["success"]:
                # æ›´æ–°æœƒè©±ç‹€æ…‹
                session_manager.update_session_state(
                    discord_ctx.session_id,
                    research_result["result"]
                )
                
                # æå–æœ€çµ‚å›æ‡‰
                final_message = research_result["result"]["messages"][-1].content
                sources = research_result["result"].get("sources_gathered", [])
                
                return {
                    "augmented_content": final_message,
                    "sources": sources,
                    "research_mode": True,
                    "execution_time": research_result.get("execution_time"),
                    "session_id": discord_ctx.session_id,
                }
            else:
                # ç ”ç©¶å¤±æ•—ï¼Œä½¿ç”¨é™ç´šæ©Ÿåˆ¶
                error_obj = research_result.get("error", {})
                error_message = "Unknown error"
                user_friendly_message = "ç ”ç©¶éç¨‹ç™¼ç”ŸéŒ¯èª¤"
                try:
                    if hasattr(error_obj, "error_message"):
                        error_message = getattr(error_obj, "error_message", "Unknown error")
                    elif isinstance(error_obj, dict):
                        error_message = error_obj.get("error_message", "Unknown error")
                    else:
                        error_message = str(error_obj)
                    if hasattr(error_obj, "user_friendly_message"):
                        user_friendly_message = getattr(error_obj, "user_friendly_message", user_friendly_message)
                    elif isinstance(error_obj, dict):
                        user_friendly_message = error_obj.get("user_friendly_message", user_friendly_message)
                except Exception as e:
                    self.logger.warning(f"è™•ç† error çµæ§‹æ™‚ç™¼ç”Ÿä¾‹å¤–: {e}")
                self.logger.warning(f"LangGraph ç ”ç©¶å¤±æ•—: {error_message}")
                return await self._fallback_response(
                    message,
                    user_friendly_message
                )
                
        except Exception as e:
            self.logger.error(f"LangGraph è™•ç†å¤±æ•—: {str(e)}", exc_info=True)
            return await self._fallback_response(message, "æ·±åº¦ç ”ç©¶åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨")
    
    async def _process_with_traditional_rag(
        self,
        message: discord.Message,
        user_content: str,
        message_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨å‚³çµ± RAG æµç¨‹"""
        self.logger.info("ä½¿ç”¨å‚³çµ± RAG æµç¨‹è™•ç†è¨Šæ¯")
        
        # é€™è£¡ä¿æŒåŸæœ‰çš„ç°¡å–® RAG é‚è¼¯
        # æœªä¾†å¯ä»¥æ•´åˆå‘é‡è³‡æ–™åº«ã€çŸ¥è­˜åœ–è­œç­‰
        return {
            "augmented_content": None,
            "sources": [],
            "research_mode": False,
            "note": "ä½¿ç”¨å‚³çµ±æµç¨‹è™•ç†"
        }
    
    async def _fallback_response(self, message: discord.Message, error_msg: str) -> Dict[str, Any]:
        """é™ç´šå›æ‡‰æ©Ÿåˆ¶"""
        try:
            # ä½¿ç”¨ä¸‰è§’åˆè¯çš„é™ç´šå›æ‡‰
            fallback_messages = [
                "æŠ±æ­‰ï¼Œæˆ‘åœ¨è™•ç†å¦³çš„å•é¡Œæ™‚é‡åˆ°äº†ä¸€äº›å›°é›£ ğŸ˜…",
                "æŠ€è¡“ä¸Šå‡ºäº†é»å°ç‹€æ³... ä¸éæˆ‘æœƒç›¡åŠ›å›ç­”å¦³çš„ ğŸ’­",
                "ç³»çµ±æœ‰é»ä¸ç©©å®šå‘¢... è®“æˆ‘ç”¨å…¶ä»–æ–¹å¼ä¾†å›ç­”å¦³ âœ¨",
                "ç¶²è·¯æœå°‹åŠŸèƒ½æš«æ™‚æœ‰å•é¡Œï¼Œä½†æˆ‘é‚„æ˜¯æƒ³å¹«åŠ©å¦³ ğŸŒŸ"
            ]
            
            import random
            fallback_content = random.choice(fallback_messages)
            
            return {
                "augmented_content": fallback_content,
                "sources": [],
                "research_mode": False,
                "is_fallback": True,
                "error": error_msg
            }
            
        except Exception as e:
            self.logger.error(f"é™ç´šå›æ‡‰ä¹Ÿå¤±æ•—äº†: {str(e)}")
            return {
                "augmented_content": "æŠ±æ­‰ï¼Œç›®å‰é‡åˆ°æŠ€è¡“å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ ğŸ˜”",
                "sources": [],
                "research_mode": False,
                "is_fallback": True,
                "error": str(e)
            }
    
    def _extract_user_message(self, message_data: Dict[str, Any]) -> Optional[str]:
        """å¾è¨Šæ¯è³‡æ–™ä¸­æå–ä½¿ç”¨è€…å…§å®¹"""
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ message_data çµæ§‹ä¾†æå–
        # å‡è¨­çµæ§‹é¡ä¼¼æ–¼ {"content": "...", "messages": [...]}
        if "content" in message_data:
            return message_data["content"]
        
        if "messages" in message_data and message_data["messages"]:
            # å–æœ€å¾Œä¸€æ¢ä½¿ç”¨è€…è¨Šæ¯
            for msg in reversed(message_data["messages"]):
                if msg.get("role") == "user":
                    return msg.get("content", "")
        
        return None
    
    def _check_force_research_mode(self, content: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦å¼·åˆ¶ä½¿ç”¨ç ”ç©¶æ¨¡å¼"""
        force_keywords = ["!research", "!ç ”ç©¶", "!èª¿æŸ¥", "!æ·±å…¥"]
        content_lower = content.lower()
        
        return any(keyword in content_lower for keyword in force_keywords)
    
    def is_research_available(self) -> bool:
        """æª¢æŸ¥ç ”ç©¶åŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return self._research_agent is not None and self._agent_config is not None
    
    def get_research_stats(self) -> Dict[str, Any]:
        """ç²å–ç ”ç©¶åŠŸèƒ½çµ±è¨ˆ"""
        session_manager = get_session_manager()
        session_stats = session_manager.get_session_stats()
        
        return {
            "research_available": self.is_research_available(),
            "agent_config": self._agent_config.model_dump() if self._agent_config else None,
            "session_stats": session_stats,
        }


# å…¨åŸŸ RAG æµç¨‹å¯¦ä¾‹
_rag_pipeline: Optional[SmartRAGPipeline] = None


async def get_rag_pipeline() -> SmartRAGPipeline:
    """ç²å–å…¨åŸŸ RAG æµç¨‹å¯¦ä¾‹"""
    global _rag_pipeline
    if _rag_pipeline is None:
        _rag_pipeline = SmartRAGPipeline()
    return _rag_pipeline


async def init_rag_pipeline(llmcord_config: Dict[str, Any]) -> SmartRAGPipeline:
    """
    åˆå§‹åŒ–å…¨åŸŸ RAG æµç¨‹
    
    Args:
        llmcord_config: llmcord é…ç½®
        
    Returns:
        SmartRAGPipeline: RAG æµç¨‹å¯¦ä¾‹
    """
    global _rag_pipeline
    _rag_pipeline = SmartRAGPipeline()
    await _rag_pipeline.initialize(llmcord_config)
    return _rag_pipeline


# å‘å¾Œç›¸å®¹çš„å‡½å¼ï¼ˆä¿æŒåŸæœ‰ APIï¼‰
async def retrieve_augmented_context(message_data: Dict[str, Any], message: discord.Message = None, cfg: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    å‘å¾Œç›¸å®¹çš„ RAG ä¸Šä¸‹æ–‡æª¢ç´¢å‡½å¼
    
    Args:
        message_data: è¨Šæ¯è³‡æ–™
        message: Discord è¨Šæ¯ï¼ˆæ–°å¢åƒæ•¸ï¼‰
        cfg: é…ç½®ï¼ˆæ–°å¢åƒæ•¸ï¼‰
        
    Returns:
        Dict: å¢å¼·çš„ä¸Šä¸‹æ–‡è³‡è¨Š
    """
    try:
        # å¦‚æœæ²’æœ‰æä¾›å¿…è¦åƒæ•¸ï¼Œè¿”å›ç©ºçµæœ
        if not message or not cfg:
            logging.info("å‘å¾Œç›¸å®¹æ¨¡å¼ï¼šç¼ºå°‘å¿…è¦åƒæ•¸ï¼Œè¿”å›ç©ºçµæœ")
            return {
                "augmented_content": None,
                "sources": []
            }
        
        # ç²å– RAG æµç¨‹å¯¦ä¾‹
        rag_pipeline = await get_rag_pipeline()
        
        # å¦‚æœæœªåˆå§‹åŒ–ï¼Œå…ˆåˆå§‹åŒ–
        if not rag_pipeline.is_research_available():
            await rag_pipeline.initialize(cfg)
        
        # è™•ç†è¨Šæ¯
        result = await rag_pipeline.process_message(message, message_data, cfg)
        
        # è½‰æ›ç‚ºå‘å¾Œç›¸å®¹çš„æ ¼å¼
        return {
            "augmented_content": result.get("augmented_content"),
            "sources": result.get("sources", []),
            "research_mode_used": result.get("research_mode", False),
            "execution_time": result.get("execution_time"),
        }
        
    except Exception as e:
        logging.error(f"RAG ä¸Šä¸‹æ–‡æª¢ç´¢å¤±æ•—: {str(e)}")
        return {
            "augmented_content": None,
            "sources": [],
            "error": str(e)
        }


# æ–°çš„ä¸»è¦å…¥å£å‡½å¼
async def process_smart_rag(
    message: discord.Message,
    message_data: Dict[str, Any],
    cfg: Dict[str, Any]
) -> Dict[str, Any]:
    """
    æ™ºèƒ½ RAG è™•ç†ä¸»å…¥å£
    
    Args:
        message: Discord è¨Šæ¯
        message_data: è¨Šæ¯è³‡æ–™
        cfg: é…ç½®
        
    Returns:
        Dict: è™•ç†çµæœ
    """
    rag_pipeline = await get_rag_pipeline()
    
    # å¦‚æœæœªåˆå§‹åŒ–ï¼Œå…ˆåˆå§‹åŒ–
    if not rag_pipeline.is_research_available():
        await rag_pipeline.initialize(cfg)
    
    return await rag_pipeline.process_message(message, message_data, cfg)
